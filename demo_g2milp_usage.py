"""
G2MILPäºŒåˆ†å›¾æ•°æ®è¡¨ç¤ºæ–¹æ³•ä½¿ç”¨æ¼”ç¤º
G2MILP Bipartite Graph Data Representation Usage Demo

æœ¬æ¼”ç¤ºè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ï¼š
1. ä½¿ç”¨ç°æœ‰çš„MILPç”Ÿæˆå™¨åˆ›å»ºä¼˜åŒ–é—®é¢˜å®ä¾‹
2. å°†MILPå®ä¾‹è½¬æ¢ä¸ºG2MILPæ¡†æ¶çš„äºŒåˆ†å›¾è¡¨ç¤º
3. åˆ†æäºŒåˆ†å›¾çš„ç»“æ„å’Œç‰¹å¾
4. å¯¼å‡ºä¸ºä¸åŒçš„å›¾ç¥ç»ç½‘ç»œæ¡†æ¶æ ¼å¼
5. è¿›è¡Œæ‰¹é‡å¤„ç†å’Œç»Ÿè®¡åˆ†æ

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç«¯åˆ°ç«¯æ¼”ç¤ºï¼Œå±•ç¤ºäº†G2MILPåœ¨ç”µåŠ›ç³»ç»Ÿä¼˜åŒ–é—®é¢˜ä¸­çš„åº”ç”¨ã€‚
"""

import sys
import logging
from pathlib import Path
from datetime import datetime
import numpy as np
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    from src.datasets.loader import load_system_data
    from src.models.biased_milp_generator import (
        BiasedMILPGenerator, 
        PerturbationConfig,
        create_scenario_perturbation_configs
    )
    from src.models.g2milp_bipartite import create_g2milp_generator
except ImportError as e:
    print(f"æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class G2MILPDemo:
    """G2MILPæ¼”ç¤ºç±»"""
    
    def __init__(self, data_dir: str = "data", output_dir: str = "output/g2milp_demo"):
        """
        åˆå§‹åŒ–æ¼”ç¤ºç¯å¢ƒ
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"G2MILPæ¼”ç¤ºåˆå§‹åŒ– - æ•°æ®ç›®å½•: {self.data_dir}, è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def step_1_load_data(self):
        """æ­¥éª¤1: åŠ è½½ç”µåŠ›ç³»ç»Ÿæ•°æ®"""
        logger.info("="*60)
        logger.info("æ­¥éª¤ 1: åŠ è½½ç”µåŠ›ç³»ç»Ÿæ•°æ®")
        logger.info("="*60)
        
        if not self.data_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
        
        # åŠ è½½ç³»ç»Ÿæ•°æ®
        self.system_data = load_system_data(str(self.data_dir))
        
        logger.info("âœ… ç”µåŠ›ç³»ç»Ÿæ•°æ®åŠ è½½æˆåŠŸ")
        logger.info(f"  ğŸ“Š å‘ç”µæœºæ•°é‡: {len(self.system_data.generators)}")
        logger.info(f"  ğŸ“Š è´Ÿè·èŠ‚ç‚¹æ•°: {len(self.system_data.loads)}")
        logger.info(f"  ğŸ“Š æ”¯è·¯æ•°é‡: {len(self.system_data.branches)}")
        
        return self.system_data
    
    def step_2_create_milp_instance(self):
        """æ­¥éª¤2: åˆ›å»ºMILPå®ä¾‹"""
        logger.info("="*60)
        logger.info("æ­¥éª¤ 2: åˆ›å»ºMILPå®ä¾‹")
        logger.info("="*60)
        
        # åˆ›å»ºMILPç”Ÿæˆå™¨
        self.milp_generator = BiasedMILPGenerator(
            base_system_data=self.system_data,
            output_dir=str(self.output_dir / "milp_instances")
        )
        
        # é…ç½®æ•°æ®æ‰°åŠ¨å‚æ•°
        perturbation_config = PerturbationConfig(
            load_perturbation_type="gaussian",
            load_noise_std=0.1,                    # è´Ÿè·10%æ ‡å‡†å·®æ‰°åŠ¨
            generator_perturbation_type="gaussian",
            generator_noise_std=0.05,              # å‘ç”µæœº5%æ ‡å‡†å·®æ‰°åŠ¨
            pv_noise_std=0.15,                     # å…‰ä¼15%æ ‡å‡†å·®æ‰°åŠ¨
            perturbation_intensity=1.0,            # 100%æ‰°åŠ¨å¼ºåº¦
            random_seed=42                         # å¯é‡ç°æ€§
        )
        
        # ç”ŸæˆMILPå®ä¾‹
        self.milp_instance = self.milp_generator.generate_single_instance(
            perturbation_config=perturbation_config,
            instance_id="g2milp_demo_instance",
            n_periods=21,                          # 21ä¸ªæ—¶é—´æ®µï¼ˆ3:00-23:00ï¼‰
            start_hour=3,
            save_to_file=True
        )
        
        logger.info("âœ… MILPå®ä¾‹åˆ›å»ºæˆåŠŸ")
        logger.info(f"  ğŸ“Š å®ä¾‹ID: {self.milp_instance.instance_id}")
        logger.info(f"  ğŸ“Š å˜é‡æ•°é‡: {self.milp_instance.statistics.n_variables}")
        logger.info(f"  ğŸ“Š çº¦æŸæ•°é‡: {self.milp_instance.statistics.n_constraints}")
        logger.info(f"  ğŸ“Š äºŒè¿›åˆ¶å˜é‡: {self.milp_instance.statistics.n_binary_vars}")
        logger.info(f"  ğŸ“Š è¿ç»­å˜é‡: {self.milp_instance.statistics.n_continuous_vars}")
        
        return self.milp_instance
    
    def step_3_generate_bipartite_graph(self):
        """æ­¥éª¤3: ç”ŸæˆG2MILPäºŒåˆ†å›¾è¡¨ç¤º"""
        logger.info("="*60)
        logger.info("æ­¥éª¤ 3: ç”ŸæˆG2MILPäºŒåˆ†å›¾è¡¨ç¤º")
        logger.info("="*60)
        
        # ç”ŸæˆäºŒåˆ†å›¾è¡¨ç¤º
        success = self.milp_instance.generate_bipartite_graph(
            include_power_system_semantics=True
        )
        
        if not success:
            raise RuntimeError("äºŒåˆ†å›¾ç”Ÿæˆå¤±è´¥")
        
        bg = self.milp_instance.bipartite_graph
        
        logger.info("âœ… G2MILPäºŒåˆ†å›¾ç”ŸæˆæˆåŠŸ")
        logger.info(f"  ğŸ“Š çº¦æŸèŠ‚ç‚¹æ•°: {bg.n_constraint_nodes}")
        logger.info(f"  ğŸ“Š å˜é‡èŠ‚ç‚¹æ•°: {bg.n_variable_nodes}")
        logger.info(f"  ğŸ“Š è¾¹æ•°é‡: {bg.n_edges}")
        logger.info(f"  ğŸ“Š äºŒåˆ†å›¾å¯†åº¦: {bg.graph_statistics.get('bipartite_density', 0):.6f}")
        logger.info(f"  ğŸ“Š å¹³å‡çº¦æŸåº¦æ•°: {bg.graph_statistics.get('avg_constraint_degree', 0):.2f}")
        logger.info(f"  ğŸ“Š å¹³å‡å˜é‡åº¦æ•°: {bg.graph_statistics.get('avg_variable_degree', 0):.2f}")
        
        # ä¿å­˜äºŒåˆ†å›¾
        graph_path = self.output_dir / f"{self.milp_instance.instance_id}_bipartite.pkl"
        self.milp_instance.save_bipartite_graph(str(graph_path))
        logger.info(f"  ğŸ’¾ äºŒåˆ†å›¾å·²ä¿å­˜: {graph_path}")
        
        return bg
    
    def step_4_analyze_features(self):
        """æ­¥éª¤4: åˆ†æäºŒåˆ†å›¾ç‰¹å¾"""
        logger.info("="*60)
        logger.info("æ­¥éª¤ 4: åˆ†æäºŒåˆ†å›¾ç‰¹å¾")
        logger.info("="*60)
        
        bg = self.milp_instance.bipartite_graph
        
        # åˆ†æå˜é‡èŠ‚ç‚¹9ç»´ç‰¹å¾å‘é‡
        logger.info("ğŸ“Š å˜é‡èŠ‚ç‚¹9ç»´ç‰¹å¾å‘é‡åˆ†æ:")
        variable_features = bg.variable_feature_matrix
        feature_names = [
            "å˜é‡ç±»å‹", "ç›®æ ‡å‡½æ•°ç³»æ•°", "ä¸‹ç•Œ", "ä¸Šç•Œ", "å˜é‡åº¦æ•°",
            "ç³»æ•°å‡å€¼", "ç³»æ•°æ ‡å‡†å·®", "ç³»æ•°æœ€å¤§å€¼", "ç´¢å¼•å½’ä¸€åŒ–"
        ]
        
        feature_analysis = {}
        for i, name in enumerate(feature_names):
            values = variable_features[:, i]
            stats = {
                'mean': float(np.mean(values)),
                'std': float(np.std(values)),
                'min': float(np.min(values)),
                'max': float(np.max(values))
            }
            feature_analysis[name] = stats
            logger.info(f"  {name}: å‡å€¼={stats['mean']:.3f}, æ ‡å‡†å·®={stats['std']:.3f}, "
                       f"èŒƒå›´=[{stats['min']:.3f}, {stats['max']:.3f}]")
        
        # åˆ†æçº¦æŸèŠ‚ç‚¹ç‰¹å¾
        logger.info("ğŸ“Š çº¦æŸèŠ‚ç‚¹ç‰¹å¾åˆ†æ:")
        constraint_features = bg.constraint_feature_matrix
        
        # çº¦æŸç±»å‹åˆ†å¸ƒ
        constraint_types = constraint_features[:, 0].astype(int)
        type_counts = np.bincount(constraint_types)
        logger.info(f"  çº¦æŸç±»å‹åˆ†å¸ƒ: {dict(enumerate(type_counts))}")
        
        # è¡Œå¯†åº¦ç»Ÿè®¡
        row_densities = constraint_features[:, 4]
        logger.info(f"  è¡Œå¯†åº¦: å‡å€¼={np.mean(row_densities):.4f}, "
                   f"æ ‡å‡†å·®={np.std(row_densities):.4f}")
        
        # çº¦æŸåº¦æ•°ç»Ÿè®¡
        constraint_degrees = constraint_features[:, 11]
        logger.info(f"  çº¦æŸåº¦æ•°: å‡å€¼={np.mean(constraint_degrees):.2f}, "
                   f"æœ€å¤§={np.max(constraint_degrees):.0f}, "
                   f"æœ€å°={np.min(constraint_degrees):.0f}")
        
        # åˆ†æè¾¹ç‰¹å¾
        logger.info("ğŸ“Š è¾¹ç‰¹å¾åˆ†æ:")
        if bg.n_edges > 0:
            edge_features = bg.edge_feature_matrix
            
            # ç³»æ•°åˆ†å¸ƒ
            coefficients = edge_features[:, 0]  # åŸå§‹ç³»æ•°
            abs_coefficients = edge_features[:, 1]  # ç»å¯¹å€¼
            
            logger.info(f"  ç³»æ•°åˆ†å¸ƒ: å‡å€¼={np.mean(coefficients):.3f}, "
                       f"æ ‡å‡†å·®={np.std(coefficients):.3f}")
            logger.info(f"  ç³»æ•°ç»å¯¹å€¼: å‡å€¼={np.mean(abs_coefficients):.3f}, "
                       f"æœ€å¤§={np.max(abs_coefficients):.3f}, "
                       f"æœ€å°={np.min(abs_coefficients):.6f}")
        
        # ä¿å­˜ç‰¹å¾åˆ†æç»“æœ
        analysis_results = {
            'instance_id': self.milp_instance.instance_id,
            'generation_time': datetime.now().isoformat(),
            'graph_basic_stats': {
                'n_constraint_nodes': bg.n_constraint_nodes,
                'n_variable_nodes': bg.n_variable_nodes,
                'n_edges': bg.n_edges,
                'bipartite_density': bg.graph_statistics.get('bipartite_density', 0)
            },
            'variable_features_analysis': feature_analysis,
            'constraint_features_analysis': {
                'type_distribution': {str(k): int(v) for k, v in enumerate(type_counts)},
                'row_density_stats': {
                    'mean': float(np.mean(row_densities)),
                    'std': float(np.std(row_densities)),
                    'min': float(np.min(row_densities)),
                    'max': float(np.max(row_densities))
                },
                'degree_stats': {
                    'mean': float(np.mean(constraint_degrees)),
                    'std': float(np.std(constraint_degrees)),
                    'min': float(np.min(constraint_degrees)),
                    'max': float(np.max(constraint_degrees))
                }
            }
        }
        
        if bg.n_edges > 0:
            analysis_results['edge_features_analysis'] = {
                'coefficient_stats': {
                    'mean': float(np.mean(coefficients)),
                    'std': float(np.std(coefficients)),
                    'min': float(np.min(coefficients)),
                    'max': float(np.max(coefficients))
                },
                'abs_coefficient_stats': {
                    'mean': float(np.mean(abs_coefficients)),
                    'std': float(np.std(abs_coefficients)),
                    'min': float(np.min(abs_coefficients)),
                    'max': float(np.max(abs_coefficients))
                }
            }
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_path = self.output_dir / "feature_analysis.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ç‰¹å¾åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜: {analysis_path}")
        
        return analysis_results
    
    def step_5_format_export(self):
        """æ­¥éª¤5: æ ¼å¼è½¬æ¢å’Œå¯¼å‡º"""
        logger.info("="*60)
        logger.info("æ­¥éª¤ 5: æ ¼å¼è½¬æ¢å’Œå¯¼å‡º")
        logger.info("="*60)
        
        bg = self.milp_instance.bipartite_graph
        
        # 1. å¯¼å‡ºåŸå§‹ç‰¹å¾çŸ©é˜µ
        features_path = self.output_dir / "g2milp_features.npz"
        np.savez_compressed(
            features_path,
            constraint_features=bg.constraint_feature_matrix,
            variable_features=bg.variable_feature_matrix,
            edge_features=bg.edge_feature_matrix,
            edges=np.array(bg.edges),
            constraint_matrix=bg.constraint_matrix.toarray() if hasattr(bg.constraint_matrix, 'toarray') else bg.constraint_matrix,
            objective_coeffs=bg.objective_coeffs,
            rhs_values=bg.rhs_values
        )
        logger.info(f"âœ… ç‰¹å¾çŸ©é˜µå·²å¯¼å‡º: {features_path}")
        
        # 2. å°è¯•å¯¼å‡ºPyTorch Geometricæ ¼å¼
        try:
            pyg_data = self.milp_instance.export_pytorch_geometric()
            if pyg_data is not None:
                logger.info("âœ… PyTorch Geometricæ ¼å¼å¯¼å‡ºæˆåŠŸ")
                logger.info(f"  ğŸ“Š èŠ‚ç‚¹ç±»å‹: {pyg_data.node_types}")
                logger.info(f"  ğŸ“Š è¾¹ç±»å‹: {pyg_data.edge_types}")
                logger.info(f"  ğŸ“Š çº¦æŸç‰¹å¾å½¢çŠ¶: {pyg_data['constraint'].x.shape}")
                logger.info(f"  ğŸ“Š å˜é‡ç‰¹å¾å½¢çŠ¶: {pyg_data['variable'].x.shape}")
                
                # ä¿å­˜PyTorch Geometricæ•°æ®
                import torch
                pyg_path = self.output_dir / "g2milp_pyg_data.pt"
                torch.save(pyg_data, pyg_path)
                logger.info(f"  ğŸ’¾ PyTorch Geometricæ•°æ®å·²ä¿å­˜: {pyg_path}")
            else:
                logger.warning("âš ï¸ PyTorch Geometricå¯¼å‡ºå¤±è´¥")
        except Exception as e:
            logger.warning(f"âš ï¸ PyTorch Geometricä¸å¯ç”¨: {e}")
        
        # 3. å°è¯•å¯¼å‡ºDGLæ ¼å¼
        try:
            dgl_graph = self.milp_instance.export_dgl_graph()
            if dgl_graph is not None:
                logger.info("âœ… DGLæ ¼å¼å¯¼å‡ºæˆåŠŸ")
                logger.info(f"  ğŸ“Š èŠ‚ç‚¹ç±»å‹: {dgl_graph.ntypes}")
                logger.info(f"  ğŸ“Š è¾¹ç±»å‹: {dgl_graph.etypes}")
                logger.info(f"  ğŸ“Š çº¦æŸèŠ‚ç‚¹æ•°: {dgl_graph.num_nodes('constraint')}")
                logger.info(f"  ğŸ“Š å˜é‡èŠ‚ç‚¹æ•°: {dgl_graph.num_nodes('variable')}")
                
                # ä¿å­˜DGLæ•°æ®
                import dgl
                dgl_path = self.output_dir / "g2milp_dgl_graph.pkl"
                dgl.save_graphs(str(dgl_path), [dgl_graph])
                logger.info(f"  ğŸ’¾ DGLå›¾æ•°æ®å·²ä¿å­˜: {dgl_path}")
            else:
                logger.warning("âš ï¸ DGLå¯¼å‡ºå¤±è´¥")
        except Exception as e:
            logger.warning(f"âš ï¸ DGLä¸å¯ç”¨: {e}")
        
        logger.info("âœ… æ ¼å¼è½¬æ¢å’Œå¯¼å‡ºå®Œæˆ")
    
    def step_6_batch_processing_demo(self):
        """æ­¥éª¤6: æ‰¹é‡å¤„ç†æ¼”ç¤º"""
        logger.info("="*60)
        logger.info("æ­¥éª¤ 6: æ‰¹é‡å¤„ç†æ¼”ç¤º")
        logger.info("="*60)
        
        # åˆ›å»ºå¤šç§æ‰°åŠ¨åœºæ™¯
        scenario_configs = create_scenario_perturbation_configs()
        
        # é€‰æ‹©å‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„åœºæ™¯è¿›è¡Œæ¼”ç¤º
        selected_scenarios = {
            "è´Ÿè·é«˜å³°": scenario_configs["load_peak"],
            "å…‰ä¼ä¸ç¨³å®š": scenario_configs["pv_unstable"],
            "äº¤é€šæ‹¥å µ": scenario_configs["traffic_jam"]
        }
        
        logger.info(f"ç”Ÿæˆ {len(selected_scenarios)} ä¸ªåœºæ™¯çš„MILPå®ä¾‹...")
        
        # ç”Ÿæˆåœºæ™¯å®ä¾‹
        scenario_instances = self.milp_generator.generate_scenario_instances(
            scenario_configs=selected_scenarios,
            n_periods=21,
            start_hour=3,
            save_to_file=True
        )
        
        logger.info(f"âœ… åœºæ™¯å®ä¾‹ç”Ÿæˆå®Œæˆï¼Œå…± {len(scenario_instances)} ä¸ª")
        
        # æ‰¹é‡ç”ŸæˆäºŒåˆ†å›¾
        instances_list = list(scenario_instances.values())
        updated_instances = self.milp_generator.generate_bipartite_graphs_for_instances(
            instances=instances_list,
            include_power_system_semantics=True,
            save_graphs=True,
            graph_output_dir=str(self.output_dir / "scenario_graphs")
        )
        
        # åˆ†æäºŒåˆ†å›¾ç»Ÿè®¡
        analysis = self.milp_generator.analyze_bipartite_graph_statistics(updated_instances)
        
        logger.info("âœ… æ‰¹é‡äºŒåˆ†å›¾ç»Ÿè®¡åˆ†æ:")
        logger.info(f"  ğŸ“Š å®ä¾‹æ€»æ•°: {analysis['total_instances']}")
        logger.info(f"  ğŸ“Š æœ‰æ•ˆå›¾æ•°: {analysis['valid_bipartite_graphs']}")
        logger.info(f"  ğŸ“Š è¦†ç›–ç‡: {analysis['coverage_rate']:.2%}")
        logger.info(f"  ğŸ“Š å¹³å‡çº¦æŸèŠ‚ç‚¹: {analysis['constraint_nodes_stats']['mean']:.1f}")
        logger.info(f"  ğŸ“Š å¹³å‡å˜é‡èŠ‚ç‚¹: {analysis['variable_nodes_stats']['mean']:.1f}")
        logger.info(f"  ğŸ“Š å¹³å‡è¾¹æ•°: {analysis['edges_stats']['mean']:.1f}")
        logger.info(f"  ğŸ“Š å¹³å‡å›¾å¯†åº¦: {analysis['density_stats']['mean']:.6f}")
        
        # ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ
        batch_results = {
            'scenarios': list(selected_scenarios.keys()),
            'analysis': analysis,
            'individual_stats': {}
        }
        
        for scenario_name, instance in scenario_instances.items():
            if instance.bipartite_graph is not None:
                batch_results['individual_stats'][scenario_name] = {
                    'n_constraint_nodes': instance.bipartite_graph.n_constraint_nodes,
                    'n_variable_nodes': instance.bipartite_graph.n_variable_nodes,
                    'n_edges': instance.bipartite_graph.n_edges,
                    'bipartite_density': instance.bipartite_graph.graph_statistics.get('bipartite_density', 0)
                }
        
        batch_path = self.output_dir / "batch_processing_results.json"
        with open(batch_path, 'w', encoding='utf-8') as f:
            json.dump(batch_results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"âœ… æ‰¹é‡å¤„ç†ç»“æœå·²ä¿å­˜: {batch_path}")
        
        return batch_results
    
    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        logger.info("ğŸš€ å¼€å§‹G2MILPäºŒåˆ†å›¾æ•°æ®è¡¨ç¤ºæ–¹æ³•å®Œæ•´æ¼”ç¤º")
        logger.info("="*80)
        
        try:
            # æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
            self.step_1_load_data()
            self.step_2_create_milp_instance()
            self.step_3_generate_bipartite_graph()
            feature_analysis = self.step_4_analyze_features()
            self.step_5_format_export()
            batch_results = self.step_6_batch_processing_demo()
            
            # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            summary_report = {
                'demo_completion_time': datetime.now().isoformat(),
                'output_directory': str(self.output_dir),
                'main_instance': {
                    'instance_id': self.milp_instance.instance_id,
                    'milp_stats': {
                        'n_variables': self.milp_instance.statistics.n_variables,
                        'n_constraints': self.milp_instance.statistics.n_constraints,
                        'n_binary_vars': self.milp_instance.statistics.n_binary_vars,
                        'n_continuous_vars': self.milp_instance.statistics.n_continuous_vars
                    },
                    'bipartite_graph_stats': {
                        'n_constraint_nodes': self.milp_instance.bipartite_graph.n_constraint_nodes,
                        'n_variable_nodes': self.milp_instance.bipartite_graph.n_variable_nodes,
                        'n_edges': self.milp_instance.bipartite_graph.n_edges,
                        'bipartite_density': self.milp_instance.bipartite_graph.graph_statistics.get('bipartite_density', 0)
                    }
                },
                'batch_processing': batch_results,
                'files_generated': {
                    'milp_instances': 'milp_instances/',
                    'bipartite_graphs': 'scenario_graphs/',
                    'feature_analysis': 'feature_analysis.json',
                    'numpy_features': 'g2milp_features.npz',
                    'batch_results': 'batch_processing_results.json'
                }
            }
            
            summary_path = self.output_dir / "demo_summary_report.json"
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary_report, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info("="*80)
            logger.info("ğŸ‰ G2MILPæ¼”ç¤ºå®Œæˆï¼")
            logger.info("="*80)
            logger.info("ğŸ“Š æ¼”ç¤ºæ€»ç»“:")
            logger.info(f"  â€¢ ä¸»å®ä¾‹: {self.milp_instance.instance_id}")
            logger.info(f"  â€¢ MILPå˜é‡æ•°: {self.milp_instance.statistics.n_variables}")
            logger.info(f"  â€¢ MILPçº¦æŸæ•°: {self.milp_instance.statistics.n_constraints}")
            logger.info(f"  â€¢ äºŒåˆ†å›¾çº¦æŸèŠ‚ç‚¹: {self.milp_instance.bipartite_graph.n_constraint_nodes}")
            logger.info(f"  â€¢ äºŒåˆ†å›¾å˜é‡èŠ‚ç‚¹: {self.milp_instance.bipartite_graph.n_variable_nodes}")
            logger.info(f"  â€¢ äºŒåˆ†å›¾è¾¹æ•°: {self.milp_instance.bipartite_graph.n_edges}")
            logger.info(f"  â€¢ æ‰¹é‡å¤„ç†åœºæ™¯æ•°: {len(batch_results['scenarios'])}")
            logger.info("="*80)
            logger.info(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {self.output_dir}")
            logger.info(f"ğŸ“„ æ€»ç»“æŠ¥å‘Š: {summary_path}")
            logger.info("="*80)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¼”ç¤ºæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ G2MILPäºŒåˆ†å›¾æ•°æ®è¡¨ç¤ºæ–¹æ³•æ¼”ç¤º")
    print("="*80)
    print("æœ¬æ¼”ç¤ºå°†å±•ç¤ºå¦‚ä½•:")
    print("â€¢ å°†ç”µåŠ›ç³»ç»ŸMILPä¼˜åŒ–é—®é¢˜è½¬æ¢ä¸ºG2MILPäºŒåˆ†å›¾è¡¨ç¤º")
    print("â€¢ åˆ†æçº¦æŸèŠ‚ç‚¹ã€å˜é‡èŠ‚ç‚¹å’Œè¾¹çš„ç‰¹å¾")
    print("â€¢ å¯¼å‡ºä¸ºPyTorch Geometricå’ŒDGLæ ¼å¼")
    print("â€¢ è¿›è¡Œæ‰¹é‡å¤„ç†å’Œç»Ÿè®¡åˆ†æ")
    print("="*80)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_dir = Path("data")
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print("è¯·ç¡®ä¿data/ç›®å½•åŒ…å«å¿…è¦çš„ç”µåŠ›ç³»ç»Ÿæ•°æ®æ–‡ä»¶")
        return
    
    try:
        # åˆ›å»ºå¹¶è¿è¡Œæ¼”ç¤º
        demo = G2MILPDemo(
            data_dir=str(data_dir),
            output_dir="output/g2milp_demo"
        )
        
        success = demo.run_complete_demo()
        
        if success:
            print("\nâœ… æ¼”ç¤ºæˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“ æŸ¥çœ‹ç»“æœ: {demo.output_dir}")
        else:
            print("\nâŒ æ¼”ç¤ºæ‰§è¡Œå¤±è´¥")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()