"""
G2MILP Early Stopping æ¨¡å—
G2MILP Early Stopping Module

å®ç°é«˜çº§Early Stoppingæœºåˆ¶ï¼Œæ”¯æŒï¼š
1. å¤šæŒ‡æ ‡ç›‘æ§å’Œç»¼åˆè¯„ä¼°
2. è‡ªé€‚åº”åœæ­¢æ¡ä»¶
3. æ€§èƒ½è¶‹åŠ¿åˆ†æ
4. æ™ºèƒ½é‡å¯ç­–ç•¥
"""

import torch
import numpy as np
import logging
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass
from collections import deque
from enum import Enum
import time
from pathlib import Path
import json

logger = logging.getLogger(__name__)


class EarlyStoppingStrategy(Enum):
    """Early Stoppingç­–ç•¥æšä¸¾"""
    SIMPLE = "simple"                    # ç®€å•çš„lossç›‘æ§
    MULTI_METRIC = "multi_metric"        # å¤šæŒ‡æ ‡ç»¼åˆè¯„ä¼°
    ADAPTIVE = "adaptive"                # è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
    TREND_ANALYSIS = "trend_analysis"    # è¶‹åŠ¿åˆ†æ
    COMBINED = "combined"                # ç»„åˆç­–ç•¥


@dataclass
class EarlyStoppingConfig:
    """Early Stoppingé…ç½®"""
    # åŸºç¡€å‚æ•°
    strategy: Union[EarlyStoppingStrategy, str] = EarlyStoppingStrategy.COMBINED
    patience: int = 500                  # ç­‰å¾…epochæ•°
    min_delta: float = 1e-6             # æœ€å°æ”¹å–„é˜ˆå€¼
    monitor_metric: str = "val_loss"     # ä¸»è¦ç›‘æ§æŒ‡æ ‡
    
    # å¤šæŒ‡æ ‡ç›‘æ§
    additional_metrics: List[str] = None  # é¢å¤–ç›‘æ§æŒ‡æ ‡
    metric_weights: Dict[str, float] = None  # æŒ‡æ ‡æƒé‡
    
    # è‡ªé€‚åº”å‚æ•°
    adaptive_patience: bool = True       # æ˜¯å¦è‡ªé€‚åº”è°ƒæ•´patience
    patience_factor: float = 1.5         # patienceå¢é•¿å› å­
    max_patience: int = 1000             # æœ€å¤§patience
    min_patience: int = 100              # æœ€å°patience
    
    # è¶‹åŠ¿åˆ†æ
    trend_window: int = 20               # è¶‹åŠ¿åˆ†æçª—å£
    trend_threshold: float = 0.01        # è¶‹åŠ¿é˜ˆå€¼
    slope_threshold: float = 1e-6        # æ–œç‡é˜ˆå€¼
    
    # è´¨é‡è¯„ä¼°é›†æˆ
    enable_quality_monitoring: bool = True  # å¯ç”¨è´¨é‡ç›‘æ§
    quality_threshold: float = 0.7          # è´¨é‡é˜ˆå€¼
    quality_patience: int = 10               # è´¨é‡patience
    quality_improvement_threshold: float = 0.05  # è´¨é‡æ”¹å–„é˜ˆå€¼
    
    # æ€§èƒ½é˜ˆå€¼
    performance_thresholds: Dict[str, float] = None  # æ€§èƒ½é˜ˆå€¼å­—å…¸
    
    # æ™ºèƒ½é‡å¯
    enable_smart_restart: bool = False   # å¯ç”¨æ™ºèƒ½é‡å¯
    restart_threshold: float = 0.1       # é‡å¯é˜ˆå€¼
    max_restarts: int = 2                # æœ€å¤§é‡å¯æ¬¡æ•°
    
    # ä¿å­˜å’Œæ¢å¤
    save_best_model: bool = True         # ä¿å­˜æœ€ä½³æ¨¡å‹
    restore_best_weights: bool = True    # æ¢å¤æœ€ä½³æƒé‡
    
    # æ—¥å¿—å’Œç›‘æ§
    verbose: bool = True                 # è¯¦ç»†æ—¥å¿—
    log_frequency: int = 10              # æ—¥å¿—é¢‘ç‡
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if self.additional_metrics is None:
            self.additional_metrics = ["train_loss", "kl_weight", "grad_norm"]
        
        if self.metric_weights is None:
            self.metric_weights = {
                "val_loss": 1.0,
                "train_loss": 0.3,
                "kl_weight": 0.1,
                "grad_norm": 0.1
            }
        
        if self.performance_thresholds is None:
            self.performance_thresholds = {
                "val_loss": 0.01,      # éªŒè¯æŸå¤±ç›®æ ‡
                "train_loss": 0.01,    # è®­ç»ƒæŸå¤±ç›®æ ‡
                "quality_score": 0.8,  # è´¨é‡å¾—åˆ†ç›®æ ‡
                "convergence_ratio": 0.95  # æ”¶æ•›æ¯”ä¾‹ç›®æ ‡
            }


class EarlyStoppingMonitor:
    """
    Enhanced Early Stoppingç›‘æ§å™¨
    
    æ”¯æŒå¤šç§åœæ­¢ç­–ç•¥å’Œæ™ºèƒ½ç›‘æ§
    """
    
    def __init__(self, config: EarlyStoppingConfig = None):
        self.config = config or EarlyStoppingConfig()
        
        # ç¡®ä¿ç­–ç•¥æ˜¯æšä¸¾ç±»å‹
        if isinstance(self.config.strategy, str):
            self.config.strategy = EarlyStoppingStrategy(self.config.strategy)
        
        # ç›‘æ§çŠ¶æ€
        self.best_score = float('inf')
        self.best_epoch = 0
        self.patience_counter = 0
        self.current_patience = self.config.patience
        
        # å†å²è®°å½•
        self.history = {
            'epochs': [],
            'scores': [],
            'improvements': [],
            'decisions': []
        }
        
        # å¤šæŒ‡æ ‡ç›‘æ§
        self.metric_history = {metric: deque(maxlen=self.config.trend_window) 
                             for metric in [self.config.monitor_metric] + self.config.additional_metrics}
        
        # è¶‹åŠ¿åˆ†æ
        self.trend_analyzer = TrendAnalyzer(self.config.trend_window)
        
        # è´¨é‡ç›‘æ§
        self.quality_monitor = QualityMonitor(self.config) if self.config.enable_quality_monitoring else None
        
        # æ™ºèƒ½é‡å¯
        self.restart_counter = 0
        self.restart_history = []
        
        # æ¨¡å‹æƒé‡ä¿å­˜
        self.best_weights = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_epochs': 0,
            'improvements': 0,
            'stagnant_periods': 0,
            'restarts': 0,
            'early_stops': 0
        }
        
        logger.info(f"Early Stoppingç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ç­–ç•¥: {self.config.strategy.value}")
        logger.info(f"ç›‘æ§æŒ‡æ ‡: {self.config.monitor_metric}")
        logger.info(f"åˆå§‹patience: {self.current_patience}")
    
    def update(self, 
               epoch: int, 
               metrics: Dict[str, float], 
               model_state: Optional[Dict] = None) -> Dict[str, Any]:
        """
        æ›´æ–°ç›‘æ§çŠ¶æ€
        
        Args:
            epoch: å½“å‰epoch
            metrics: æŒ‡æ ‡å­—å…¸
            model_state: æ¨¡å‹çŠ¶æ€ï¼ˆç”¨äºä¿å­˜æœ€ä½³æƒé‡ï¼‰
            
        Returns:
            ç›‘æ§ç»“æœå­—å…¸
        """
        self.stats['total_epochs'] = epoch
        
        # è·å–ä¸»è¦ç›‘æ§æŒ‡æ ‡
        primary_score = metrics.get(self.config.monitor_metric, float('inf'))
        
        # æ›´æ–°å†å²è®°å½•
        self.history['epochs'].append(epoch)
        self.history['scores'].append(primary_score)
        
        # æ›´æ–°å¤šæŒ‡æ ‡å†å²
        for metric_name in self.metric_history:
            if metric_name in metrics:
                self.metric_history[metric_name].append(metrics[metric_name])
        
        # è¶‹åŠ¿åˆ†æ
        self.trend_analyzer.update(primary_score)
        
        # è´¨é‡ç›‘æ§
        quality_result = {}
        if self.quality_monitor:
            quality_result = self.quality_monitor.update(epoch, metrics)
        
        # åˆ¤æ–­æ”¹å–„
        improvement = self._check_improvement(primary_score)
        self.history['improvements'].append(improvement)
        
        # å†³ç­–é€»è¾‘
        decision = self._make_decision(epoch, primary_score, metrics, improvement, quality_result)
        self.history['decisions'].append(decision)
        
        # æ›´æ–°æœ€ä½³çŠ¶æ€
        if improvement:
            self.best_score = primary_score
            self.best_epoch = epoch
            self.patience_counter = 0
            self.stats['improvements'] += 1
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡
            if self.config.save_best_model and model_state:
                self.best_weights = model_state.copy()
        else:
            self.patience_counter += 1
        
        # è‡ªé€‚åº”patienceè°ƒæ•´
        if self.config.adaptive_patience:
            self._adjust_patience(improvement, quality_result)
        
        # è¿”å›ç›‘æ§ç»“æœ
        result = {
            'should_stop': decision['should_stop'],
            'should_restart': decision['should_restart'],
            'improvement': improvement,
            'best_score': self.best_score,
            'best_epoch': self.best_epoch,
            'patience_counter': self.patience_counter,
            'current_patience': self.current_patience,
            'trend_analysis': self.trend_analyzer.get_analysis(),
            'quality_analysis': quality_result,
            'decision_reason': decision['reason'],
            'statistics': self.stats.copy()
        }
        
        # æ—¥å¿—è¾“å‡º
        if self.config.verbose and epoch % self.config.log_frequency == 0:
            self._log_status(epoch, result)
        
        return result
    
    def _check_improvement(self, score: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„"""
        if self.config.monitor_metric.endswith('_loss'):
            # æŸå¤±å‡½æ•°ï¼šè¶Šå°è¶Šå¥½
            return score < (self.best_score - self.config.min_delta)
        else:
            # å…¶ä»–æŒ‡æ ‡ï¼šè¶Šå¤§è¶Šå¥½
            return score > (self.best_score + self.config.min_delta)
    
    def _make_decision(self, 
                      epoch: int, 
                      primary_score: float, 
                      metrics: Dict[str, float], 
                      improvement: bool,
                      quality_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ¶å®šåœæ­¢å†³ç­–
        
        Returns:
            å†³ç­–ç»“æœå­—å…¸
        """
        decision = {
            'should_stop': False,
            'should_restart': False,
            'reason': '',
            'confidence': 0.0
        }
        
        # æ ¹æ®ç­–ç•¥è¿›è¡Œå†³ç­–
        if self.config.strategy == EarlyStoppingStrategy.SIMPLE:
            decision = self._simple_decision()
        elif self.config.strategy == EarlyStoppingStrategy.MULTI_METRIC:
            decision = self._multi_metric_decision(metrics)
        elif self.config.strategy == EarlyStoppingStrategy.ADAPTIVE:
            decision = self._adaptive_decision(metrics, quality_result)
        elif self.config.strategy == EarlyStoppingStrategy.TREND_ANALYSIS:
            decision = self._trend_analysis_decision()
        elif self.config.strategy == EarlyStoppingStrategy.COMBINED:
            decision = self._combined_decision(epoch, metrics, quality_result)
        
        return decision
    
    def _simple_decision(self) -> Dict[str, Any]:
        """ç®€å•å†³ç­–ï¼šåŸºäºpatience"""
        should_stop = self.patience_counter >= self.current_patience
        return {
            'should_stop': should_stop,
            'should_restart': False,
            'reason': f'Simple patience exceeded ({self.patience_counter}/{self.current_patience})',
            'confidence': 1.0 if should_stop else 0.0
        }
    
    def _multi_metric_decision(self, metrics: Dict[str, float]) -> Dict[str, Any]:
        """å¤šæŒ‡æ ‡ç»¼åˆå†³ç­–"""
        # è®¡ç®—åŠ æƒç»¼åˆå¾—åˆ†
        weighted_score = 0.0
        total_weight = 0.0
        
        for metric_name, weight in self.config.metric_weights.items():
            if metric_name in metrics:
                # å½’ä¸€åŒ–å¤„ç†
                metric_value = metrics[metric_name]
                if metric_name in self.metric_history and len(self.metric_history[metric_name]) > 1:
                    history_values = list(self.metric_history[metric_name])
                    if max(history_values) > min(history_values):
                        normalized_value = (metric_value - min(history_values)) / (max(history_values) - min(history_values))
                        weighted_score += weight * normalized_value
                        total_weight += weight
        
        if total_weight > 0:
            weighted_score /= total_weight
        
        # å†³ç­–é€»è¾‘
        stagnation_threshold = 0.8  # åœæ»é˜ˆå€¼
        should_stop = (weighted_score > stagnation_threshold and 
                      self.patience_counter >= self.current_patience * 0.5)
        
        return {
            'should_stop': should_stop,
            'should_restart': False,
            'reason': f'Multi-metric stagnation detected (score: {weighted_score:.3f})',
            'confidence': weighted_score
        }
    
    def _adaptive_decision(self, metrics: Dict[str, float], quality_result: Dict[str, Any]) -> Dict[str, Any]:
        """è‡ªé€‚åº”å†³ç­–"""
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ€§èƒ½é˜ˆå€¼
        performance_achieved = True
        for metric_name, threshold in self.config.performance_thresholds.items():
            if metric_name in metrics:
                if metrics[metric_name] > threshold:  # å‡è®¾æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯è¶Šå°è¶Šå¥½
                    performance_achieved = False
                    break
        
        # è´¨é‡æ£€æŸ¥
        quality_achieved = False
        if quality_result and 'quality_score' in quality_result:
            quality_achieved = quality_result['quality_score'] >= self.config.quality_threshold
        
        # å†³ç­–
        if performance_achieved and quality_achieved:
            return {
                'should_stop': True,
                'should_restart': False,
                'reason': 'Performance and quality targets achieved',
                'confidence': 1.0
            }
        elif self.patience_counter >= self.current_patience:
            return {
                'should_stop': True,
                'should_restart': False,
                'reason': 'Adaptive patience exceeded',
                'confidence': 0.8
            }
        else:
            return {
                'should_stop': False,
                'should_restart': False,
                'reason': 'Continuing training',
                'confidence': 0.0
            }
    
    def _trend_analysis_decision(self) -> Dict[str, Any]:
        """è¶‹åŠ¿åˆ†æå†³ç­–"""
        analysis = self.trend_analyzer.get_analysis()
        
        # æ£€æŸ¥æ˜¯å¦å¤„äºå¹³ç¨³æœŸ
        if (analysis['slope'] < self.config.slope_threshold and 
            analysis['stability'] > 0.9 and 
            self.patience_counter >= self.current_patience * 0.7):
            
            return {
                'should_stop': True,
                'should_restart': False,
                'reason': f'Trend analysis: flat region detected (slope: {analysis["slope"]:.2e})',
                'confidence': analysis['stability']
            }
        
        return {
            'should_stop': False,
            'should_restart': False,
            'reason': 'Trend analysis: still improving',
            'confidence': 0.0
        }
    
    def _combined_decision(self, epoch: int, metrics: Dict[str, float], quality_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç»„åˆå†³ç­–ç­–ç•¥"""
        # æ”¶é›†å„ç­–ç•¥çš„å†³ç­–
        simple_decision = self._simple_decision()
        multi_metric_decision = self._multi_metric_decision(metrics)
        adaptive_decision = self._adaptive_decision(metrics, quality_result)
        trend_decision = self._trend_analysis_decision()
        
        # æƒé‡æŠ•ç¥¨
        stop_votes = []
        if simple_decision['should_stop']:
            stop_votes.append(('simple', simple_decision['confidence']))
        if multi_metric_decision['should_stop']:
            stop_votes.append(('multi_metric', multi_metric_decision['confidence']))
        if adaptive_decision['should_stop']:
            stop_votes.append(('adaptive', adaptive_decision['confidence']))
        if trend_decision['should_stop']:
            stop_votes.append(('trend', trend_decision['confidence']))
        
        # ç»¼åˆå†³ç­–
        if len(stop_votes) >= 2:  # è‡³å°‘ä¸¤ä¸ªç­–ç•¥åŒæ„åœæ­¢
            total_confidence = sum(vote[1] for vote in stop_votes) / len(stop_votes)
            reasons = [vote[0] for vote in stop_votes]
            
            return {
                'should_stop': True,
                'should_restart': False,
                'reason': f'Combined decision: {", ".join(reasons)} (confidence: {total_confidence:.3f})',
                'confidence': total_confidence
            }
        elif len(stop_votes) == 1 and stop_votes[0][1] > 0.9:  # å•ä¸€ç­–ç•¥ä½†ç½®ä¿¡åº¦å¾ˆé«˜
            return {
                'should_stop': True,
                'should_restart': False,
                'reason': f'High confidence decision: {stop_votes[0][0]} (confidence: {stop_votes[0][1]:.3f})',
                'confidence': stop_votes[0][1]
            }
        else:
            return {
                'should_stop': False,
                'should_restart': False,
                'reason': 'Combined decision: continue training',
                'confidence': 0.0
            }
    
    def _adjust_patience(self, improvement: bool, quality_result: Dict[str, Any]):
        """è‡ªé€‚åº”è°ƒæ•´patience"""
        if improvement:
            # æœ‰æ”¹å–„æ—¶ï¼Œå¯ä»¥é€‚å½“é™ä½patienceï¼ˆæ›´ä¸¥æ ¼ï¼‰
            self.current_patience = max(
                self.config.min_patience,
                int(self.current_patience / self.config.patience_factor)
            )
        else:
            # é•¿æ—¶é—´æ— æ”¹å–„æ—¶ï¼Œå¢åŠ patienceï¼ˆæ›´å®½æ¾ï¼‰
            if self.patience_counter >= self.current_patience * 0.8:
                self.current_patience = min(
                    self.config.max_patience,
                    int(self.current_patience * self.config.patience_factor)
                )
    
    def _log_status(self, epoch: int, result: Dict[str, Any]):
        """è®°å½•çŠ¶æ€æ—¥å¿—"""
        logger.info(f"ğŸ“Š Early Stopping Status (Epoch {epoch}):")
        logger.info(f"  â”œâ”€ Best Score: {self.best_score:.6f} (Epoch {self.best_epoch})")
        logger.info(f"  â”œâ”€ Patience: {self.patience_counter}/{self.current_patience}")
        logger.info(f"  â”œâ”€ Improvement: {'âœ“' if result['improvement'] else 'âœ—'}")
        logger.info(f"  â”œâ”€ Decision: {result['decision_reason']}")
        
        if result['trend_analysis']:
            trend = result['trend_analysis']
            logger.info(f"  â”œâ”€ Trend: Slope={trend['slope']:.2e}, Stability={trend['stability']:.3f}")
        
        if result['quality_analysis']:
            quality = result['quality_analysis']
            logger.info(f"  â””â”€ Quality: {quality.get('quality_score', 'N/A')}")
        
        if result['should_stop']:
            logger.warning(f"ğŸ›‘ Early Stopping Triggered: {result['decision_reason']}")
        elif result['should_restart']:
            logger.warning(f"ğŸ”„ Smart Restart Triggered: {result['decision_reason']}")
    
    def get_best_weights(self) -> Optional[Dict]:
        """è·å–æœ€ä½³æ¨¡å‹æƒé‡"""
        return self.best_weights
    
    def should_restore_weights(self) -> bool:
        """æ˜¯å¦åº”è¯¥æ¢å¤æœ€ä½³æƒé‡"""
        return self.config.restore_best_weights and self.best_weights is not None
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§æ‘˜è¦"""
        return {
            'config': self.config,
            'best_score': self.best_score,
            'best_epoch': self.best_epoch,
            'total_epochs': self.stats['total_epochs'],
            'improvements': self.stats['improvements'],
            'final_patience': self.current_patience,
            'history': self.history,
            'statistics': self.stats
        }
    
    def save_state(self, save_path: str):
        """ä¿å­˜ç›‘æ§çŠ¶æ€"""
        state = {
            'config': self.config.__dict__,
            'monitor_state': {
                'best_score': self.best_score,
                'best_epoch': self.best_epoch,
                'patience_counter': self.patience_counter,
                'current_patience': self.current_patience,
                'restart_counter': self.restart_counter
            },
            'history': self.history,
            'statistics': self.stats
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"Early StoppingçŠ¶æ€å·²ä¿å­˜: {save_path}")


class TrendAnalyzer:
    """è¶‹åŠ¿åˆ†æå™¨"""
    
    def __init__(self, window_size: int = 20):
        self.window_size = window_size
        self.values = deque(maxlen=window_size)
        self.timestamps = deque(maxlen=window_size)
    
    def update(self, value: float):
        """æ›´æ–°æ•°å€¼"""
        self.values.append(value)
        self.timestamps.append(time.time())
    
    def get_analysis(self) -> Dict[str, float]:
        """è·å–è¶‹åŠ¿åˆ†æ"""
        if len(self.values) < 3:
            return {'slope': 0.0, 'stability': 0.0, 'trend': 'insufficient_data'}
        
        # è®¡ç®—çº¿æ€§å›å½’æ–œç‡
        values_array = np.array(self.values)
        x = np.arange(len(values_array))
        
        try:
            slope, intercept = np.polyfit(x, values_array, 1)
        except:
            slope = 0.0
            intercept = 0.0
        
        # è®¡ç®—ç¨³å®šæ€§ï¼ˆå˜å¼‚ç³»æ•°çš„å€’æ•°ï¼‰
        if np.std(values_array) > 0:
            stability = 1.0 / (np.std(values_array) / np.mean(values_array))
        else:
            stability = 1.0
        
        # è¶‹åŠ¿åˆ¤æ–­
        if abs(slope) < 1e-6:
            trend = 'flat'
        elif slope < 0:
            trend = 'decreasing'
        else:
            trend = 'increasing'
        
        return {
            'slope': slope,
            'stability': min(stability, 1.0),
            'trend': trend,
            'mean': np.mean(values_array),
            'std': np.std(values_array),
            'recent_value': self.values[-1] if self.values else 0.0
        }


class QualityMonitor:
    """è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self, config: EarlyStoppingConfig):
        self.config = config
        self.quality_history = deque(maxlen=50)
        self.quality_counter = 0
        self.last_quality_improvement = 0
    
    def update(self, epoch: int, metrics: Dict[str, float]) -> Dict[str, Any]:
        """æ›´æ–°è´¨é‡ç›‘æ§"""
        result = {
            'quality_score': 0.0,
            'quality_trend': 'unknown',
            'quality_patience': self.quality_counter,
            'should_stop_for_quality': False
        }
        
        # æå–è´¨é‡ç›¸å…³æŒ‡æ ‡
        quality_indicators = ['generation_quality', 'overall_quality_score', 'validity_score']
        quality_score = 0.0
        
        for indicator in quality_indicators:
            if indicator in metrics:
                quality_score = max(quality_score, metrics[indicator])
                break
        
        if quality_score > 0:
            self.quality_history.append(quality_score)
            result['quality_score'] = quality_score
            
            # æ£€æŸ¥è´¨é‡æ”¹å–„
            if (len(self.quality_history) >= 2 and 
                quality_score > max(self.quality_history) - self.config.quality_improvement_threshold):
                self.last_quality_improvement = epoch
                self.quality_counter = 0
            else:
                self.quality_counter += 1
            
            # è´¨é‡è¶‹åŠ¿
            if len(self.quality_history) >= 3:
                recent_avg = np.mean(list(self.quality_history)[-3:])
                earlier_avg = np.mean(list(self.quality_history)[:-3])
                
                if recent_avg > earlier_avg + 0.01:
                    result['quality_trend'] = 'improving'
                elif recent_avg < earlier_avg - 0.01:
                    result['quality_trend'] = 'degrading'
                else:
                    result['quality_trend'] = 'stable'
            
            # è´¨é‡åœæ­¢æ¡ä»¶
            if (quality_score >= self.config.quality_threshold and 
                self.quality_counter >= self.config.quality_patience):
                result['should_stop_for_quality'] = True
        
        return result


def create_early_stopping_monitor(strategy: str = "combined", 
                                 patience: int = 500, 
                                 monitor_metric: str = "val_loss",
                                 **kwargs) -> EarlyStoppingMonitor:
    """
    åˆ›å»ºEarly Stoppingç›‘æ§å™¨çš„å·¥å‚å‡½æ•°
    
    Args:
        strategy: åœæ­¢ç­–ç•¥
        patience: ç­‰å¾…epochæ•°
        monitor_metric: ç›‘æ§æŒ‡æ ‡
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        EarlyStoppingMonitorå®ä¾‹
    """
    # åˆ›å»ºé…ç½®æ—¶ç›´æ¥ä¼ å…¥å­—ç¬¦ä¸²ï¼Œåœ¨Monitoråˆå§‹åŒ–æ—¶è½¬æ¢
    config = EarlyStoppingConfig(
        strategy=strategy,  # ä¼ å…¥å­—ç¬¦ä¸²ï¼Œç¨åè½¬æ¢
        patience=patience,
        monitor_metric=monitor_metric,
        **kwargs
    )
    
    return EarlyStoppingMonitor(config)


if __name__ == "__main__":
    # æµ‹è¯•Early Stoppingç›‘æ§å™¨
    print("G2MILP Early Stoppingæ¨¡å—æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = create_early_stopping_monitor(
        strategy="combined",
        patience=100,
        monitor_metric="val_loss",
        verbose=True
    )
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    print("\næ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹:")
    for epoch in range(150):
        # æ¨¡æ‹ŸæŒ‡æ ‡
        metrics = {
            'val_loss': 1.0 - epoch * 0.001 + np.random.normal(0, 0.01),
            'train_loss': 0.8 - epoch * 0.0008 + np.random.normal(0, 0.005),
            'grad_norm': 0.1 * np.exp(-epoch * 0.01) + np.random.normal(0, 0.001),
            'generation_quality': min(0.9, epoch * 0.006) + np.random.normal(0, 0.02)
        }
        
        # æ›´æ–°ç›‘æ§
        result = monitor.update(epoch, metrics)
        
        # æ£€æŸ¥åœæ­¢æ¡ä»¶
        if result['should_stop']:
            print(f"âœ“ è®­ç»ƒåœ¨epoch {epoch}åœæ­¢")
            print(f"  åŸå› : {result['decision_reason']}")
            break
    
    # è¾“å‡ºæ‘˜è¦
    summary = monitor.get_summary()
    print(f"\nè®­ç»ƒæ‘˜è¦:")
    print(f"- æœ€ä½³å¾—åˆ†: {summary['best_score']:.6f}")
    print(f"- æœ€ä½³epoch: {summary['best_epoch']}")
    print(f"- æ€»æ”¹å–„æ¬¡æ•°: {summary['improvements']}")
    print(f"- æœ€ç»ˆpatience: {summary['final_patience']}")
    
    print("Early Stoppingæ¨¡å—æµ‹è¯•å®Œæˆ!")