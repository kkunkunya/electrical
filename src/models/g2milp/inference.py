"""
G2MILPæ¨ç†æ¨¡å—
G2MILP Inference Module

å®ç°G2MILPçš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºäºè®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæ–°çš„MILPå®ä¾‹
2. å¯æ§çš„ç”Ÿæˆå‚æ•°ï¼ˆÎ·ã€æ¸©åº¦ç­‰ï¼‰
3. ç”Ÿæˆç»“æœçš„åˆ†æå’Œå¯è§†åŒ–
4. ä¸åŸå§‹å®ä¾‹çš„å¯¹æ¯”åˆ†æ
"""

import torch
import torch.nn.functional as F
from torch_geometric.data import HeteroData
import numpy as np
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
import json
import pickle
import time
from pathlib import Path
from datetime import datetime
import copy
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.sparse import csr_matrix

from .generator import G2MILPGenerator
from ..g2milp_bipartite import BipartiteGraphRepresentation

logger = logging.getLogger(__name__)


@dataclass
class InferenceConfig:
    """æ¨ç†é…ç½®"""
    # ç”Ÿæˆå‚æ•°
    eta: float = 0.1                    # é®ç›–æ¯”ä¾‹ï¼Œæ§åˆ¶ç›¸ä¼¼åº¦vsåˆ›æ–°æ€§
    num_iterations: Optional[int] = None # è¿­ä»£æ¬¡æ•°ï¼ŒNoneåˆ™æ ¹æ®Î·è®¡ç®—
    temperature: float = 1.0             # é‡‡æ ·æ¸©åº¦
    sample_from_prior: bool = True       # æ˜¯å¦ä»å…ˆéªŒåˆ†å¸ƒé‡‡æ ·
    
    # ç”Ÿæˆç­–ç•¥
    constraint_selection_strategy: str = "random"  # random, degree_based, importance_based
    diversity_boost: bool = False        # æ˜¯å¦ä½¿ç”¨å¤šæ ·æ€§å¢å¼º
    num_diverse_samples: int = 5        # å¤šæ ·æ€§é‡‡æ ·æ•°é‡
    num_test_instances: int = 5         # æµ‹è¯•å®ä¾‹æ•°é‡
    
    # åå¤„ç†
    apply_constraints_validation: bool = True   # æ˜¯å¦éªŒè¯çº¦æŸæœ‰æ•ˆæ€§
    normalize_weights: bool = True              # æ˜¯å¦å½’ä¸€åŒ–æƒé‡
    round_integer_variables: bool = True        # æ˜¯å¦èˆå…¥æ•´æ•°å˜é‡
    
    # åˆ†æå’Œæ¯”è¾ƒ
    compute_similarity_metrics: bool = True     # è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡
    generate_comparison_report: bool = True     # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    save_intermediate_states: bool = False      # ä¿å­˜ä¸­é—´çŠ¶æ€
    
    # è¾“å‡ºè®¾ç½®
    output_dir: str = "output/demo4_g2milp/inference"
    experiment_name: str = f"inference_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    save_generated_instances: bool = True
    
    # è®¾å¤‡
    device: str = "cuda" if torch.cuda.is_available() else "cpu"


class G2MILPInference:
    """
    G2MILPæ¨ç†å™¨
    
    ä½¿ç”¨è®­ç»ƒå¥½çš„G2MILPæ¨¡å‹ç”Ÿæˆæ–°çš„MILPå®ä¾‹
    """
    
    def __init__(self, 
                 model: G2MILPGenerator,
                 config: InferenceConfig = None):
        self.model = model
        self.config = config or InferenceConfig()
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device(self.config.device)
        self.model = self.model.to(self.device)
        self.model.eval()  # è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(self.config.output_dir) / self.config.experiment_name
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨ç†å†å²
        self.inference_history = []
        
        logger.info(f"G2MILPæ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"è®¾å¤‡: {self.device}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def generate_single_instance(self, 
                                source_data: HeteroData,
                                save_intermediate: bool = None,
                                dynamic_config: Dict = None) -> Dict[str, Any]:
        """
        ç”Ÿæˆå•ä¸ªMILPå®ä¾‹ï¼ˆå¢å¼ºç‰ˆæ”¯æŒåŠ¨æ€é…ç½®ï¼‰
        
        Args:
            source_data: æºæ•°æ®ï¼ˆæœ‰åå·®çš„äºŒåˆ†å›¾ï¼‰
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´çŠ¶æ€
            dynamic_config: åŠ¨æ€ç”Ÿæˆé…ç½®ï¼ˆç”¨äºå¤šæ ·æ€§å¢å¼ºï¼‰
            
        Returns:
            ç”Ÿæˆç»“æœå­—å…¸
        """
        if save_intermediate is None:
            save_intermediate = self.config.save_intermediate_states
        
        logger.info("å¼€å§‹ç”Ÿæˆå•ä¸ªMILPå®ä¾‹")
        
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        source_data = source_data.to(self.device)
        
        # åº”ç”¨åŠ¨æ€é…ç½®ï¼ˆç”¨äºå¤šæ ·æ€§å¢å¼ºï¼‰
        if dynamic_config is not None:
            effective_eta = dynamic_config.get('eta', self.config.eta)
            effective_temperature = dynamic_config.get('temperature', getattr(self.model.config, 'temperature', 1.0))
            constraint_selection_strategy = dynamic_config.get('constraint_selection_strategy', 'random')
            logger.info(f"ä½¿ç”¨åŠ¨æ€é…ç½®: Î·={effective_eta:.3f}, temp={effective_temperature:.3f}, strategy={constraint_selection_strategy}")
        else:
            effective_eta = self.config.eta
            effective_temperature = getattr(self.model.config, 'temperature', 1.0)
            constraint_selection_strategy = 'random'
        
        # ç¡®å®šè¿­ä»£æ¬¡æ•°
        n_constraints = source_data['constraint'].x.size(0)
        if self.config.num_iterations is None:
            num_iterations = max(1, int(effective_eta * n_constraints))
        else:
            num_iterations = self.config.num_iterations
        
        logger.info(f"è¿­ä»£æ¬¡æ•°: {num_iterations}, Î· = {effective_eta:.3f}")
        
        # åˆå§‹åŒ–ç”Ÿæˆè¿‡ç¨‹
        current_data = copy.deepcopy(source_data)
        generation_steps = []
        similarity_evolution = []
        
        with torch.no_grad():
            for iteration in range(num_iterations):
                logger.debug(f"æ¨ç†è¿­ä»£ {iteration + 1}/{num_iterations}")
                
                # è®°å½•è¿­ä»£å‰çŠ¶æ€
                if save_intermediate:
                    pre_iteration_state = self._extract_graph_state(current_data)
                
                # å•æ¬¡ç”Ÿæˆè¿­ä»£
                current_data, iteration_info = self.model.generate_single_iteration(
                    current_data
                )
                
                # ç¡®ä¿æ•°æ®è®¾å¤‡ä¸€è‡´æ€§
                current_data = self._ensure_device_consistency(current_data)
                
                # è®¡ç®—ä¸åŸå§‹å®ä¾‹çš„ç›¸ä¼¼åº¦
                if self.config.compute_similarity_metrics:
                    similarity = self._compute_similarity(source_data, current_data)
                    similarity_evolution.append(similarity)
                    iteration_info['similarity'] = similarity
                
                # è®°å½•ç”Ÿæˆæ­¥éª¤
                iteration_info['iteration'] = iteration
                generation_steps.append(iteration_info)
                
                # ä¿å­˜ä¸­é—´çŠ¶æ€
                if save_intermediate:
                    post_iteration_state = self._extract_graph_state(current_data)
                    self._save_intermediate_state(
                        iteration, pre_iteration_state, post_iteration_state, iteration_info
                    )
                
                logger.debug(f"å®Œæˆè¿­ä»£ {iteration + 1}")
        
        # åå¤„ç†
        if self.config.apply_constraints_validation:
            current_data = self._validate_and_fix_constraints(current_data)
        
        # ç”Ÿæˆç»“æœåˆ†æ
        result = {
            'generated_data': current_data.cpu(),
            'source_data': source_data.cpu(),
            'generation_steps': generation_steps,
            'similarity_evolution': similarity_evolution,
            'num_iterations': num_iterations,
            'eta': self.config.eta,
            'generation_config': asdict(self.config),
            'generation_timestamp': datetime.now().isoformat()
        }
        
        # è¯¦ç»†åˆ†æ
        if self.config.generate_comparison_report:
            analysis = self._analyze_generated_instance(
                source_data.cpu(), current_data.cpu(), generation_steps
            )
            result['analysis'] = analysis
        
        logger.info("å•ä¸ªMILPå®ä¾‹ç”Ÿæˆå®Œæˆ")
        
        return result
    
    def generate_multiple_instances(self, 
                                  source_data: HeteroData,
                                  num_instances: int = 5) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆå¤šä¸ªMILPå®ä¾‹
        
        Args:
            source_data: æºæ•°æ®
            num_instances: ç”Ÿæˆå®ä¾‹æ•°é‡
            
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹ç”Ÿæˆ {num_instances} ä¸ªMILPå®ä¾‹")
        
        results = []
        
        for i in range(num_instances):
            logger.info(f"ç”Ÿæˆç¬¬ {i+1}/{num_instances} ä¸ªå®ä¾‹")
            
            # æ”¹è¿›çš„éšæœºç§å­ç­–ç•¥
            import time
            import os
            enhanced_seed = (
                int(time.time() * 1000000) % 1000000 +  # å¾®ç§’æ—¶é—´æˆ³
                os.getpid() * 1000 +                    # è¿›ç¨‹ID
                i * 1337 +                              # å®ä¾‹åºå·
                hash(str(source_data)) % 10000          # æ•°æ®å“ˆå¸Œ
            ) % 2**32
            
            torch.manual_seed(enhanced_seed)
            np.random.seed(enhanced_seed % 2**31)
            
            # ä¸ºæ¯ä¸ªå®ä¾‹ä½¿ç”¨ä¸åŒçš„ç”Ÿæˆå‚æ•°
            dynamic_config = self._generate_dynamic_config(i, num_instances)
            
            result = self.generate_single_instance(
                source_data, 
                save_intermediate=False,
                dynamic_config=dynamic_config
            )
            result['instance_id'] = i
            result['dynamic_config'] = dynamic_config
            results.append(result)
        
        # åˆ†æå¤šå®ä¾‹ç”Ÿæˆç»“æœ
        multi_analysis = self._analyze_multiple_instances(results)
        
        # ä¿å­˜ç»“æœ
        if self.config.save_generated_instances:
            self._save_multiple_instances(results, multi_analysis)
        
        logger.info(f"å®Œæˆç”Ÿæˆ {num_instances} ä¸ªMILPå®ä¾‹")
        
        return results
    
    def generate_instances(self, 
                          source_data: HeteroData,
                          num_samples: int = 3) -> Dict[str, Any]:
        """
        ç”ŸæˆMILPå®ä¾‹ï¼ˆè®­ç»ƒæ—¶è´¨é‡è¯„ä¼°ä¸“ç”¨æ¥å£ï¼‰
        
        è¿™æ˜¯ä¸ºäº†ä¸training.pyä¸­çš„è´¨é‡è¯„ä¼°æ¥å£å…¼å®¹è€Œæ·»åŠ çš„åŒ…è£…æ–¹æ³•
        
        Args:
            source_data: æºæ•°æ®ï¼ˆæœ‰åå·®çš„äºŒåˆ†å›¾ï¼‰
            num_samples: ç”Ÿæˆå®ä¾‹æ•°é‡
            
        Returns:
            æ ¼å¼åŒ–çš„ç”Ÿæˆç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - generated_instances: ç”Ÿæˆçš„å®ä¾‹åˆ—è¡¨
            - generation_info: ç”Ÿæˆè¿‡ç¨‹ä¿¡æ¯
        """
        logger.info(f"ğŸ” è´¨é‡è¯„ä¼°æ¨¡å¼ï¼šç”Ÿæˆ {num_samples} ä¸ªæµ‹è¯•å®ä¾‹")
        
        # ä½¿ç”¨ç°æœ‰çš„generate_multiple_instancesæ–¹æ³•
        generation_results = self.generate_multiple_instances(
            source_data=source_data,
            num_instances=num_samples
        )
        
        # æå–ç”Ÿæˆçš„å›¾æ•°æ®
        generated_instances = []
        generation_info = {
            'num_generated': len(generation_results),
            'average_iterations': 0,
            'average_similarity': 0.0,
            'generation_success': True,
            'detailed_results': generation_results
        }
        
        # å¤„ç†ç”Ÿæˆç»“æœ
        total_iterations = 0
        total_similarity = 0.0
        
        for i, result in enumerate(generation_results):
            try:
                # æå–ç”Ÿæˆçš„å›¾æ•°æ®
                if 'generated_data' in result:
                    generated_instances.append(result['generated_data'])
                else:
                    logger.warning(f"å®ä¾‹ {i} ç¼ºå°‘ generated_data å­—æ®µ")
                    generation_info['generation_success'] = False
                
                # ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯
                if 'generation_steps' in result:
                    total_iterations += len(result['generation_steps'])
                
                if 'final_similarity' in result:
                    total_similarity += result['final_similarity']
                    
            except Exception as e:
                logger.warning(f"å¤„ç†ç”Ÿæˆç»“æœ {i} æ—¶å‡ºé”™: {e}")
                generation_info['generation_success'] = False
        
        # è®¡ç®—å¹³å‡å€¼
        if len(generation_results) > 0:
            generation_info['average_iterations'] = total_iterations / len(generation_results)
            generation_info['average_similarity'] = total_similarity / len(generation_results)
        
        logger.info(f"âœ… è´¨é‡è¯„ä¼°ç”Ÿæˆå®Œæˆï¼š{len(generated_instances)} ä¸ªæœ‰æ•ˆå®ä¾‹")
        
        return {
            'generated_instances': generated_instances,
            'generation_info': generation_info
        }
    
    def generate_with_diversity_boost(self, 
                                    source_data: HeteroData) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨å¤šæ ·æ€§å¢å¼ºç”Ÿæˆå¤šä¸ªä¸åŒçš„å®ä¾‹
        
        Args:
            source_data: æºæ•°æ®
            
        Returns:
            å¤šæ ·åŒ–ç”Ÿæˆç»“æœ
        """
        logger.info("å¼€å§‹å¤šæ ·æ€§å¢å¼ºç”Ÿæˆ")
        
        results = []
        
        # ä¸åŒçš„Î·å€¼
        eta_values = [0.05, 0.1, 0.15, 0.2, 0.3]
        
        # ä¸åŒçš„æ¸©åº¦å€¼
        temperature_values = [0.5, 1.0, 1.5, 2.0]
        
        # ç»„åˆä¸åŒå‚æ•°
        param_combinations = []
        for eta in eta_values[:3]:  # é€‰æ‹©å‰3ä¸ªÎ·å€¼
            for temp in temperature_values[:2]:  # é€‰æ‹©å‰2ä¸ªæ¸©åº¦å€¼
                param_combinations.append((eta, temp))
        
        for i, (eta, temp) in enumerate(param_combinations):
            logger.info(f"å¤šæ ·æ€§ç”Ÿæˆ {i+1}/{len(param_combinations)}: Î·={eta}, T={temp}")
            
            # ä¸´æ—¶ä¿®æ”¹é…ç½®
            original_eta = self.config.eta
            original_temp = self.config.temperature
            
            self.config.eta = eta
            self.config.temperature = temp
            self.model.config.temperature = temp
            
            # ç”Ÿæˆå®ä¾‹
            result = self.generate_single_instance(source_data, save_intermediate=False)
            result['diversity_params'] = {'eta': eta, 'temperature': temp}
            result['diversity_instance_id'] = i
            
            results.append(result)
            
            # æ¢å¤åŸé…ç½®
            self.config.eta = original_eta
            self.config.temperature = original_temp
            self.model.config.temperature = original_temp
        
        # åˆ†æå¤šæ ·æ€§ç»“æœ
        diversity_analysis = self._analyze_diversity_results(results)
        
        logger.info("å¤šæ ·æ€§å¢å¼ºç”Ÿæˆå®Œæˆ")
        
        return results
    
    def _ensure_device_consistency(self, data: HeteroData) -> HeteroData:
        """
        ç¡®ä¿HeteroDataä¸­æ‰€æœ‰å¼ é‡åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        
        Args:
            data: å¼‚æ„å›¾æ•°æ®
            
        Returns:
            è®¾å¤‡ä¸€è‡´çš„å¼‚æ„å›¾æ•°æ®
        """
        try:
            # ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
            data = data.to(self.device)
            
            # éªŒè¯è®¾å¤‡ä¸€è‡´æ€§
            constraint_device = data['constraint'].x.device
            variable_device = data['variable'].x.device
            edge_device = data[('constraint', 'connects', 'variable')].edge_index.device
            
            # ç»Ÿä¸€è®¾å¤‡æ¯”è¾ƒï¼ˆcuda:0å’Œcudaè¢«è§†ä¸ºç›¸åŒï¼‰
            devices_consistent = all(
                str(d).startswith('cuda') == str(self.device).startswith('cuda') 
                for d in [constraint_device, variable_device, edge_device]
            )
            
            if not devices_consistent:
                logger.debug(f"è®¾å¤‡ä¸ä¸€è‡´æ£€æµ‹åˆ°ï¼Œæ­£åœ¨ä¿®å¤ï¼šconstraint={constraint_device}, variable={variable_device}, edge={edge_device}, target={self.device}")
                
                # å¼ºåˆ¶ç§»åŠ¨æ‰€æœ‰ç»„ä»¶åˆ°ç›®æ ‡è®¾å¤‡
                for node_type in data.node_types:
                    if hasattr(data[node_type], 'x') and data[node_type].x is not None:
                        data[node_type].x = data[node_type].x.to(self.device)
                
                for edge_type in data.edge_types:
                    if hasattr(data[edge_type], 'edge_index') and data[edge_type].edge_index is not None:
                        data[edge_type].edge_index = data[edge_type].edge_index.to(self.device)
                    if hasattr(data[edge_type], 'edge_attr') and data[edge_type].edge_attr is not None:
                        data[edge_type].edge_attr = data[edge_type].edge_attr.to(self.device)
            
            return data
            
        except Exception as e:
            logger.error(f"è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            # å°è¯•åŸºæœ¬çš„è®¾å¤‡ç§»åŠ¨
            return data.to(self.device)
    
    def _extract_graph_state(self, data: HeteroData) -> Dict[str, Any]:
        """æå–å›¾çŠ¶æ€ä¿¡æ¯"""
        return {
            'n_constraints': data['constraint'].x.size(0),
            'n_variables': data['variable'].x.size(0),
            'n_edges': data[('constraint', 'connects', 'variable')].edge_index.size(1),
            'constraint_features_norm': torch.norm(data['constraint'].x).item(),
            'variable_features_norm': torch.norm(data['variable'].x).item(),
            'edge_weights_norm': torch.norm(
                data[('constraint', 'connects', 'variable')].edge_attr
            ).item() if hasattr(data[('constraint', 'connects', 'variable')], 'edge_attr') else 0.0
        }
    
    def _compute_similarity(self, source_data: HeteroData, generated_data: HeteroData) -> Dict[str, float]:
        """
        è®¡ç®—ç”Ÿæˆå®ä¾‹ä¸æºå®ä¾‹çš„å¤šç»´åº¦ç›¸ä¼¼åº¦ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            source_data: æºå›¾æ•°æ®
            generated_data: ç”Ÿæˆçš„å›¾æ•°æ®
            
        Returns:
            åŒ…å«å¤šä¸ªç»´åº¦ç›¸ä¼¼åº¦åˆ†æ•°çš„å­—å…¸
        """
        similarity = {}
        
        # 1. åŸºç¡€å›¾ç»“æ„ç›¸ä¼¼åº¦
        source_edges = source_data[('constraint', 'connects', 'variable')].edge_index
        generated_edges = generated_data[('constraint', 'connects', 'variable')].edge_index
        
        # 1.1 è¾¹æ•°é‡ç›¸ä¼¼åº¦
        edge_count_similarity = 1.0 - abs(
            source_edges.size(1) - generated_edges.size(1)
        ) / max(source_edges.size(1), generated_edges.size(1))
        similarity['edge_count'] = edge_count_similarity
        
        # 1.2 å›¾å¯†åº¦ç›¸ä¼¼åº¦
        n_constraints = source_data['constraint'].x.size(0)
        n_variables = source_data['variable'].x.size(0)
        max_possible_edges = n_constraints * n_variables
        
        source_density = source_edges.size(1) / max_possible_edges
        generated_density = generated_edges.size(1) / max_possible_edges
        density_similarity = 1.0 - abs(source_density - generated_density)
        similarity['density'] = density_similarity
        
        # 2. åº¦æ•°åˆ†å¸ƒç›¸ä¼¼åº¦ï¼ˆå¢å¼ºç‰ˆï¼‰
        # 2.1 çº¦æŸèŠ‚ç‚¹åº¦æ•°åˆ†å¸ƒ
        source_constraint_degrees = torch.bincount(source_edges[0])
        generated_constraint_degrees = torch.bincount(generated_edges[0])
        constraint_degree_sim = self._compute_distribution_similarity(
            source_constraint_degrees, generated_constraint_degrees
        )
        similarity['constraint_degree_distribution'] = constraint_degree_sim
        
        # 2.2 å˜é‡èŠ‚ç‚¹åº¦æ•°åˆ†å¸ƒ
        source_variable_degrees = torch.bincount(source_edges[1])
        generated_variable_degrees = torch.bincount(generated_edges[1])
        variable_degree_sim = self._compute_distribution_similarity(
            source_variable_degrees, generated_variable_degrees
        )
        similarity['variable_degree_distribution'] = variable_degree_sim
        
        # 3. å›¾è°±ç‰¹æ€§ç›¸ä¼¼åº¦
        try:
            spectral_sim = self._compute_spectral_similarity(source_data, generated_data)
            similarity.update(spectral_sim)
        except Exception as e:
            logger.warning(f"å›¾è°±ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            similarity['spectral_similarity'] = 0.5
        
        # 4. èšç±»ç³»æ•°ç›¸ä¼¼åº¦
        try:
            clustering_sim = self._compute_clustering_similarity(source_data, generated_data)
            similarity['clustering_coefficient'] = clustering_sim
        except Exception as e:
            logger.warning(f"èšç±»ç³»æ•°è®¡ç®—å¤±è´¥: {e}")
            similarity['clustering_coefficient'] = 0.5
        
        # 5. MILPç‰¹å¾ç›¸ä¼¼åº¦
        # 5.1 çº¦æŸç‰¹å¾ç›¸ä¼¼åº¦ï¼ˆåˆ†ç»´åº¦è®¡ç®—ï¼‰
        constraint_feature_sims = self._compute_feature_similarity_detailed(
            source_data['constraint'].x, generated_data['constraint'].x
        )
        for i, sim in enumerate(constraint_feature_sims):
            similarity[f'constraint_feature_dim_{i}'] = sim
        similarity['constraint_features_avg'] = np.mean(constraint_feature_sims)
        
        # 5.2 å˜é‡ç‰¹å¾ç›¸ä¼¼åº¦ï¼ˆåˆ†ç»´åº¦è®¡ç®—ï¼‰
        variable_feature_sims = self._compute_feature_similarity_detailed(
            source_data['variable'].x, generated_data['variable'].x
        )
        for i, sim in enumerate(variable_feature_sims):
            similarity[f'variable_feature_dim_{i}'] = sim
        similarity['variable_features_avg'] = np.mean(variable_feature_sims)
        
        # 6. ç¨€ç–æ€§æ¨¡å¼ç›¸ä¼¼åº¦
        sparsity_sim = self._compute_sparsity_pattern_similarity(source_data, generated_data)
        similarity['sparsity_pattern'] = sparsity_sim
        
        # 7. è¿æ¥æ¨¡å¼ç›¸ä¼¼åº¦
        connectivity_sim = self._compute_connectivity_pattern_similarity(source_data, generated_data)
        similarity['connectivity_pattern'] = connectivity_sim
        
        # 8. ç»¼åˆç›¸ä¼¼åº¦ï¼ˆåŠ æƒå¹³å‡ï¼‰
        weights = {
            'edge_count': 0.15,
            'density': 0.10,
            'constraint_degree_distribution': 0.15,
            'variable_degree_distribution': 0.15,
            'spectral_similarity': 0.10,
            'clustering_coefficient': 0.05,
            'constraint_features_avg': 0.10,
            'variable_features_avg': 0.10,
            'sparsity_pattern': 0.05,
            'connectivity_pattern': 0.05
        }
        
        weighted_similarities = []
        for key, weight in weights.items():
            if key in similarity:
                weighted_similarities.append(similarity[key] * weight)
        
        similarity['overall'] = sum(weighted_similarities) if weighted_similarities else 0.5
        
        return similarity
    
    def _validate_and_fix_constraints(self, data: HeteroData) -> HeteroData:
        """éªŒè¯å’Œä¿®å¤çº¦æŸ"""
        # è¿™é‡Œå¯ä»¥å®ç°çº¦æŸéªŒè¯å’Œä¿®å¤é€»è¾‘
        # ç›®å‰åªæ˜¯è¿”å›åŸæ•°æ®
        logger.debug("çº¦æŸéªŒè¯å’Œä¿®å¤ï¼ˆå ä½ç¬¦å®ç°ï¼‰")
        return data
    
    def _analyze_generated_instance(self, 
                                  source_data: HeteroData,
                                  generated_data: HeteroData,
                                  generation_steps: List[Dict]) -> Dict[str, Any]:
        """åˆ†æç”Ÿæˆçš„å®ä¾‹"""
        analysis = {}
        
        # åŸºæœ¬ç»Ÿè®¡
        analysis['basic_stats'] = {
            'source': self._extract_graph_state(source_data),
            'generated': self._extract_graph_state(generated_data)
        }
        
        # å˜åŒ–ç»Ÿè®¡
        analysis['changes'] = {
            'edge_count_change': (
                generated_data[('constraint', 'connects', 'variable')].edge_index.size(1) - 
                source_data[('constraint', 'connects', 'variable')].edge_index.size(1)
            ),
            'avg_constraint_degree_change': self._compute_degree_change(source_data, generated_data)
        }
        
        # ç”Ÿæˆè¿‡ç¨‹åˆ†æ
        analysis['generation_process'] = {
            'total_iterations': len(generation_steps),
            'modified_constraints': len(set(step['masked_constraint_id'] for step in generation_steps)),
            'avg_predicted_degree': np.mean([step['predicted_degree'] for step in generation_steps]),
            'avg_connections_per_iteration': np.mean([step['n_connections'] for step in generation_steps])
        }
        
        # ç›¸ä¼¼åº¦åˆ†æ
        if 'similarity' in generation_steps[-1]:
            final_similarity = generation_steps[-1]['similarity']
            analysis['final_similarity'] = final_similarity
        
        return analysis
    
    def _compute_degree_change(self, source_data: HeteroData, generated_data: HeteroData) -> float:
        """è®¡ç®—å¹³å‡åº¦æ•°å˜åŒ–"""
        source_edges = source_data[('constraint', 'connects', 'variable')].edge_index
        generated_edges = generated_data[('constraint', 'connects', 'variable')].edge_index
        
        source_degrees = torch.bincount(source_edges[0]).float()
        generated_degrees = torch.bincount(generated_edges[0]).float()
        
        # è®¡ç®—å¹³å‡åº¦æ•°
        source_avg_degree = source_degrees.mean().item()
        generated_avg_degree = generated_degrees.mean().item()
        
        return generated_avg_degree - source_avg_degree
    
    def _analyze_multiple_instances(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå¤šä¸ªç”Ÿæˆå®ä¾‹"""
        analysis = {
            'num_instances': len(results),
            'similarity_stats': {},
            'diversity_stats': {},
            'generation_stats': {}
        }
        
        # ç›¸ä¼¼åº¦ç»Ÿè®¡
        if results and 'analysis' in results[0] and 'final_similarity' in results[0]['analysis']:
            similarities = [
                r['analysis']['final_similarity']['overall'] 
                for r in results if 'analysis' in r
            ]
            
            analysis['similarity_stats'] = {
                'mean': np.mean(similarities),
                'std': np.std(similarities),
                'min': np.min(similarities),
                'max': np.max(similarities)
            }
        
        # å¤šæ ·æ€§ç»Ÿè®¡ï¼ˆå®ä¾‹é—´å·®å¼‚ï¼‰
        edge_counts = [
            r['generated_data'][('constraint', 'connects', 'variable')].edge_index.size(1)
            for r in results
        ]
        
        analysis['diversity_stats'] = {
            'edge_count_diversity': np.std(edge_counts) / np.mean(edge_counts) if np.mean(edge_counts) > 0 else 0,
            'edge_count_range': np.max(edge_counts) - np.min(edge_counts)
        }
        
        # ç”Ÿæˆè¿‡ç¨‹ç»Ÿè®¡
        iteration_counts = [r['num_iterations'] for r in results]
        analysis['generation_stats'] = {
            'avg_iterations': np.mean(iteration_counts),
            'total_iterations': np.sum(iteration_counts)
        }
        
        return analysis
    
    def _analyze_diversity_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå¤šæ ·æ€§ç”Ÿæˆç»“æœ"""
        analysis = {
            'parameter_impact': {},
            'diversity_metrics': {}
        }
        
        # å‚æ•°å½±å“åˆ†æ
        eta_values = set()
        temp_values = set()
        
        for result in results:
            if 'diversity_params' in result:
                eta_values.add(result['diversity_params']['eta'])
                temp_values.add(result['diversity_params']['temperature'])
        
        analysis['parameter_impact'] = {
            'eta_range': (min(eta_values), max(eta_values)),
            'temperature_range': (min(temp_values), max(temp_values)),
            'num_parameter_combinations': len(results)
        }
        
        return analysis
    
    def _save_intermediate_state(self, iteration: int, pre_state: Dict, post_state: Dict, info: Dict):
        """ä¿å­˜ä¸­é—´çŠ¶æ€"""
        intermediate_path = self.output_dir / "intermediate_states" 
        intermediate_path.mkdir(exist_ok=True)
        
        state_data = {
            'iteration': iteration,
            'pre_iteration_state': pre_state,
            'post_iteration_state': info,
            'iteration_info': info
        }
        
        with open(intermediate_path / f"iteration_{iteration:03d}.json", 'w') as f:
            json.dump(state_data, f, indent=2, default=str)
    
    def _save_multiple_instances(self, results: List[Dict[str, Any]], analysis: Dict[str, Any]):
        """ä¿å­˜å¤šä¸ªå®ä¾‹çš„ç»“æœ"""
        # ä¿å­˜ä¸ªåˆ«å®ä¾‹
        instances_dir = self.output_dir / "generated_instances"
        instances_dir.mkdir(exist_ok=True)
        
        for i, result in enumerate(results):
            instance_path = instances_dir / f"instance_{i:03d}.pkl"
            with open(instance_path, 'wb') as f:
                pickle.dump(result, f)
        
        # ä¿å­˜æ±‡æ€»åˆ†æ
        analysis_path = self.output_dir / "multi_instance_analysis.json"
        with open(analysis_path, 'w') as f:
            json.dump(analysis, f, indent=2, default=str)
        
        logger.info(f"å¤šå®ä¾‹ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def convert_to_bipartite_representation(self, 
                                          generated_data: HeteroData,
                                          instance_id: str = "generated") -> BipartiteGraphRepresentation:
        """
        å°†ç”Ÿæˆçš„HeteroDataè½¬æ¢å›BipartiteGraphRepresentationæ ¼å¼
        
        Args:
            generated_data: ç”Ÿæˆçš„å¼‚æ„å›¾æ•°æ®
            instance_id: å®ä¾‹ID
            
        Returns:
            äºŒåˆ†å›¾è¡¨ç¤ºå¯¹è±¡
        """
        # è¿™é‡Œéœ€è¦å®ç°ä»HeteroDataåˆ°BipartiteGraphRepresentationçš„è½¬æ¢
        # ç›®å‰è¿”å›å ä½ç¬¦
        logger.warning("BipartiteGraphRepresentationè½¬æ¢å°šæœªå®ç°")
        return None
    
    def save_results(self, results: Dict[str, Any], filename: str = "inference_results.pkl"):
        """ä¿å­˜æ¨ç†ç»“æœ"""
        result_path = self.output_dir / filename
        
        with open(result_path, 'wb') as f:
            pickle.dump(results, f)
        
        # åŒæ—¶ä¿å­˜JSONæ ¼å¼çš„æ‘˜è¦
        summary_path = self.output_dir / filename.replace('.pkl', '_summary.json')
        
        summary = {
            'experiment_name': self.config.experiment_name,
            'inference_config': asdict(self.config),
            'timestamp': datetime.now().isoformat()
        }
        
        if 'analysis' in results:
            summary['analysis'] = results['analysis']
        
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        logger.info(f"æ¨ç†ç»“æœå·²ä¿å­˜: {result_path}")
    
    def _generate_dynamic_config(self, instance_id: int, total_instances: int) -> Dict[str, Any]:
        """
        ä¸ºæ¯ä¸ªç”Ÿæˆå®ä¾‹åˆ›å»ºåŠ¨æ€é…ç½®ï¼Œå¢åŠ å¤šæ ·æ€§
        
        Args:
            instance_id: å½“å‰å®ä¾‹ID
            total_instances: æ€»å®ä¾‹æ•°
            
        Returns:
            åŠ¨æ€é…ç½®å­—å…¸
        """
        # Î·å‚æ•°å¤šæ ·åŒ–ï¼šåœ¨åŸºç¡€å€¼å‘¨å›´å˜åŒ–
        base_eta = self.config.eta
        eta_variations = [
            base_eta * 0.5,   # æ›´ä¿å®ˆï¼šæ›´ç›¸ä¼¼
            base_eta * 0.8,   # ç•¥ä¿å®ˆ
            base_eta,         # é»˜è®¤å€¼
            base_eta * 1.5,   # ç•¥æ¿€è¿›ï¼šæ›´åˆ›æ–°
            base_eta * 2.0,   # æ›´æ¿€è¿›
            base_eta * 3.0    # ææ¿€è¿›ï¼šå¾ˆä¸åŒ
        ]
        
        # æ ¹æ®å®ä¾‹IDé€‰æ‹©Î·å€¼ï¼Œç¡®ä¿è¦†ç›–ä¸åŒçš„èŒƒå›´
        eta_index = instance_id % len(eta_variations)
        dynamic_eta = eta_variations[eta_index]
        
        # æ·»åŠ å°å¹…éšæœºæ‰°åŠ¨
        eta_noise = np.random.uniform(-0.02, 0.02)
        dynamic_eta = max(0.01, min(0.8, dynamic_eta + eta_noise))
        
        # æ¸©åº¦å‚æ•°å¤šæ ·åŒ–
        base_temp = getattr(self.model.config, 'temperature', 1.0)
        temp_variations = [0.3, 0.5, 0.8, 1.0, 1.5, 2.0, 3.0]
        temp_index = (instance_id * 2) % len(temp_variations)  # ä¸åŒçš„å¾ªç¯æ¨¡å¼
        dynamic_temperature = temp_variations[temp_index]
        
        # æ·»åŠ æ¸©åº¦éšæœºå™ªå£°
        temp_noise = np.random.uniform(-0.1, 0.1)
        dynamic_temperature = max(0.1, min(5.0, dynamic_temperature + temp_noise))
        
        # çº¦æŸé€‰æ‹©ç­–ç•¥å¤šæ ·åŒ–
        strategies = [
            'random',           # éšæœºé€‰æ‹©
            'degree_based',     # åŸºäºåº¦æ•°é€‰æ‹©
            'centrality_based', # åŸºäºä¸­å¿ƒæ€§é€‰æ‹©
            'progressive',      # æ¸è¿›å¼é€‰æ‹©
            'inverse_degree'    # åå‘åº¦æ•°é€‰æ‹©
        ]
        strategy_index = (instance_id * 3) % len(strategies)  # åˆä¸€ä¸ªä¸åŒçš„å¾ªç¯
        constraint_strategy = strategies[strategy_index]
        
        # æ„å»ºåŠ¨æ€é…ç½®
        dynamic_config = {
            'eta': dynamic_eta,
            'temperature': dynamic_temperature,
            'constraint_selection_strategy': constraint_strategy,
            'instance_variation_id': instance_id,
            'randomization_seed': int(time.time() * 1000000 + instance_id) % 2**31
        }
        
        logger.debug(f"å®ä¾‹{instance_id}åŠ¨æ€é…ç½®: Î·={dynamic_eta:.3f}, temp={dynamic_temperature:.3f}, strategy={constraint_strategy}")
        
        return dynamic_config
    
    def _compute_distribution_similarity(self, dist1: torch.Tensor, dist2: torch.Tensor) -> float:
        """è®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒçš„ç›¸ä¼¼åº¦ï¼ˆæ•°å€¼ç¨³å®šæ€§å¢å¼ºç‰ˆï¼‰"""
        try:
            # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if not (torch.isfinite(dist1).all() and torch.isfinite(dist2).all()):
                logger.warning("åˆ†å¸ƒåŒ…å«éæœ‰é™å€¼")
                return 0.5
            
            # å¡«å……åˆ°ç›¸åŒé•¿åº¦
            max_len = max(len(dist1), len(dist2))
            if len(dist1) < max_len:
                dist1 = F.pad(dist1, (0, max_len - len(dist1)))
            if len(dist2) < max_len:
                dist2 = F.pad(dist2, (0, max_len - len(dist2)))
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºé›¶åˆ†å¸ƒ
            sum1 = dist1.sum().float()
            sum2 = dist2.sum().float()
            
            if sum1 < 1e-8 and sum2 < 1e-8:
                return 1.0  # ä¸¤ä¸ªéƒ½æ˜¯é›¶åˆ†å¸ƒï¼Œè®¤ä¸ºç›¸ä¼¼
            elif sum1 < 1e-8 or sum2 < 1e-8:
                return 0.0  # ä¸€ä¸ªæ˜¯é›¶åˆ†å¸ƒï¼Œä¸€ä¸ªä¸æ˜¯
            
            # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
            dist1_norm = dist1.float() / sum1
            dist2_norm = dist2.float() / sum2
            
            # è®¡ç®—å¤šç§ç›¸ä¼¼åº¦æŒ‡æ ‡çš„å¹³å‡å€¼
            similarities = []
            
            # 1. ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ•°å€¼ç¨³å®šç‰ˆï¼‰
            try:
                norm1 = torch.norm(dist1_norm)
                norm2 = torch.norm(dist2_norm)
                
                if norm1 < 1e-8 or norm2 < 1e-8:
                    cosine_sim = 1.0 if (norm1 < 1e-8 and norm2 < 1e-8) else 0.0
                else:
                    cosine_sim = F.cosine_similarity(
                        dist1_norm.unsqueeze(0), dist2_norm.unsqueeze(0)
                    ).item()
                    
                    if not np.isfinite(cosine_sim):
                        cosine_sim = 0.5
                    else:
                        cosine_sim = max(0.0, min(1.0, cosine_sim))
                        
                similarities.append(cosine_sim)
            except Exception as e:
                logger.debug(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
                similarities.append(0.5)
            
            # 2. KLæ•£åº¦ç›¸ä¼¼åº¦ï¼ˆæ•°å€¼ç¨³å®šç‰ˆï¼‰
            try:
                # æ·»åŠ å¹³æ»‘é¡¹ä»¥é¿å…log(0)
                epsilon = 1e-10
                dist1_smooth = dist1_norm + epsilon
                dist2_smooth = dist2_norm + epsilon
                
                kl_div = F.kl_div(
                    torch.log(dist1_smooth), dist2_smooth, reduction='sum'
                ).item()
                
                if not np.isfinite(kl_div) or kl_div < 0:
                    kl_sim = 0.5
                else:
                    kl_sim = np.exp(-min(kl_div, 10.0))  # é™åˆ¶æœ€å¤§KLæ•£åº¦
                    
                similarities.append(kl_sim)
            except Exception as e:
                logger.debug(f"KLæ•£åº¦è®¡ç®—å¤±è´¥: {e}")
                similarities.append(0.5)
            
            # 3. JSæ•£åº¦ç›¸ä¼¼åº¦ï¼ˆæ•°å€¼ç¨³å®šç‰ˆï¼‰
            try:
                epsilon = 1e-10
                dist1_smooth = dist1_norm + epsilon
                dist2_smooth = dist2_norm + epsilon
                m = (dist1_smooth + dist2_smooth) / 2
                
                js_div1 = F.kl_div(torch.log(dist1_smooth), m, reduction='sum')
                js_div2 = F.kl_div(torch.log(dist2_smooth), m, reduction='sum')
                js_div = 0.5 * (js_div1 + js_div2).item()
                
                if not np.isfinite(js_div) or js_div < 0:
                    js_sim = 0.5
                else:
                    js_sim = np.exp(-min(js_div, 10.0))  # é™åˆ¶æœ€å¤§JSæ•£åº¦
                    
                similarities.append(js_sim)
            except Exception as e:
                logger.debug(f"JSæ•£åº¦è®¡ç®—å¤±è´¥: {e}")
                similarities.append(0.5)
            
            # è¿”å›å¤šç§ç›¸ä¼¼åº¦çš„å¹³å‡å€¼
            if similarities:
                final_sim = sum(similarities) / len(similarities)
                return max(0.0, min(1.0, final_sim))
            else:
                return 0.5
            
        except Exception as e:
            logger.warning(f"åˆ†å¸ƒç›¸ä¼¼åº¦è®¡ç®—å¼‚å¸¸: {e}")
            return 0.5
    
    def _compute_spectral_similarity(self, source_data: HeteroData, generated_data: HeteroData) -> Dict[str, float]:
        """è®¡ç®—å›¾è°±ç‰¹æ€§ç›¸ä¼¼åº¦"""
        spectral_sim = {}
        
        try:
            import networkx as nx
            from scipy.sparse import csr_matrix
            from scipy.sparse.linalg import eigsh
            
            # è½¬æ¢ä¸ºnetworkxå›¾è¿›è¡Œè°±åˆ†æ
            def to_networkx_bipartite(data):
                G = nx.Graph()
                edges = data[('constraint', 'connects', 'variable')].edge_index.cpu().numpy()
                n_constraints = data['constraint'].x.size(0)
                
                # æ·»åŠ èŠ‚ç‚¹ï¼ˆå¸¦ç±»å‹æ ‡è¯†ï¼‰
                for i in range(n_constraints):
                    G.add_node(f'c_{i}', bipartite=0)
                for i in range(data['variable'].x.size(0)):
                    G.add_node(f'v_{i}', bipartite=1)
                
                # æ·»åŠ è¾¹
                for i in range(edges.shape[1]):
                    G.add_edge(f'c_{edges[0, i]}', f'v_{edges[1, i]}')
                
                return G
            
            source_graph = to_networkx_bipartite(source_data)
            generated_graph = to_networkx_bipartite(generated_data)
            
            # 1. æ‹‰æ™®æ‹‰æ–¯ç‰¹å¾å€¼ç›¸ä¼¼åº¦
            if len(source_graph.nodes()) > 1 and len(generated_graph.nodes()) > 1:
                source_laplacian = nx.laplacian_matrix(source_graph).astype(float)
                generated_laplacian = nx.laplacian_matrix(generated_graph).astype(float)
                
                # è®¡ç®—å‰å‡ ä¸ªç‰¹å¾å€¼
                k = min(10, min(source_laplacian.shape[0], generated_laplacian.shape[0]) - 1)
                if k > 0:
                    source_eigenvals = eigsh(source_laplacian, k=k, which='SM', return_eigenvectors=False)
                    generated_eigenvals = eigsh(generated_laplacian, k=k, which='SM', return_eigenvectors=False)
                    
                    # å½’ä¸€åŒ–ç‰¹å¾å€¼
                    source_eigenvals = np.sort(source_eigenvals)
                    generated_eigenvals = np.sort(generated_eigenvals)
                    
                    # è®¡ç®—ç‰¹å¾å€¼åˆ†å¸ƒçš„ç›¸ä¼¼åº¦
                    eigenval_sim = 1.0 - np.mean(np.abs(source_eigenvals - generated_eigenvals)) / (
                        np.mean(np.abs(source_eigenvals)) + 1e-8
                    )
                    spectral_sim['eigenvalue_similarity'] = max(0.0, min(1.0, eigenval_sim))
                else:
                    spectral_sim['eigenvalue_similarity'] = 0.5
            else:
                spectral_sim['eigenvalue_similarity'] = 0.5
            
            spectral_sim['spectral_similarity'] = spectral_sim.get('eigenvalue_similarity', 0.5)
            
        except Exception as e:
            logger.warning(f"å›¾è°±åˆ†æéœ€è¦networkxå’Œscipy: {e}")
            spectral_sim = {'spectral_similarity': 0.5, 'eigenvalue_similarity': 0.5}
        
        return spectral_sim
    
    def _compute_clustering_similarity(self, source_data: HeteroData, generated_data: HeteroData) -> float:
        """è®¡ç®—èšç±»ç³»æ•°ç›¸ä¼¼åº¦"""
        try:
            import networkx as nx
            
            def compute_bipartite_clustering(data):
                G = nx.Graph()
                edges = data[('constraint', 'connects', 'variable')].edge_index.cpu().numpy()
                
                # æ„å»ºå›¾
                for i in range(edges.shape[1]):
                    G.add_edge(f'c_{edges[0, i]}', f'v_{edges[1, i]}')
                
                if len(G.nodes()) < 3:
                    return 0.0
                
                # å¯¹äºäºŒåˆ†å›¾ï¼Œè®¡ç®—çº¦æŸèŠ‚ç‚¹çš„èšç±»ç³»æ•°
                clustering_coeffs = []
                for node in G.nodes():
                    if node.startswith('c_'):  # çº¦æŸèŠ‚ç‚¹
                        neighbors = list(G.neighbors(node))
                        if len(neighbors) < 2:
                            clustering_coeffs.append(0.0)
                            continue
                        
                        # è®¡ç®—é‚»å±…é—´çš„è¿æ¥æ•°
                        possible_edges = len(neighbors) * (len(neighbors) - 1) // 2
                        actual_edges = 0
                        for i in range(len(neighbors)):
                            for j in range(i + 1, len(neighbors)):
                                if G.has_edge(neighbors[i], neighbors[j]):
                                    actual_edges += 1
                        
                        clustering_coeffs.append(actual_edges / possible_edges if possible_edges > 0 else 0.0)
                
                return np.mean(clustering_coeffs) if clustering_coeffs else 0.0
            
            source_clustering = compute_bipartite_clustering(source_data)
            generated_clustering = compute_bipartite_clustering(generated_data)
            
            # è®¡ç®—èšç±»ç³»æ•°çš„ç›¸ä¼¼åº¦
            clustering_sim = 1.0 - abs(source_clustering - generated_clustering)
            return max(0.0, min(1.0, clustering_sim))
            
        except Exception as e:
            logger.warning(f"èšç±»ç³»æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _compute_feature_similarity_detailed(self, features1: torch.Tensor, features2: torch.Tensor) -> List[float]:
        """è®¡ç®—ç‰¹å¾çš„åˆ†ç»´åº¦ç›¸ä¼¼åº¦ï¼ˆæ•°å€¼ç¨³å®šæ€§å¢å¼ºç‰ˆï¼‰"""
        similarities = []
        
        try:
            # è®¡ç®—æ¯ä¸ªç‰¹å¾ç»´åº¦çš„ç›¸ä¼¼åº¦
            for dim in range(min(features1.size(1), features2.size(1))):
                dim1_values = features1[:, dim]
                dim2_values = features2[:, dim]
                
                # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                dim1_numpy = dim1_values.cpu().numpy()
                dim2_numpy = dim2_values.cpu().numpy()
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«NaNæˆ–æ— ç©·å€¼
                if not (np.isfinite(dim1_numpy).all() and np.isfinite(dim2_numpy).all()):
                    logger.warning(f"ç‰¹å¾ç»´åº¦ {dim} åŒ…å«éæœ‰é™å€¼ï¼Œè·³è¿‡è¯¥ç»´åº¦")
                    similarities.append(0.5)
                    continue
                
                # 1. ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ•°å€¼ç¨³å®šç‰ˆï¼‰
                try:
                    # æ£€æŸ¥å‘é‡æ˜¯å¦ä¸ºé›¶å‘é‡
                    norm1 = torch.norm(dim1_values)
                    norm2 = torch.norm(dim2_values)
                    
                    if norm1 < 1e-8 or norm2 < 1e-8:
                        # é›¶å‘é‡æƒ…å†µï¼šå¦‚æœä¸¤ä¸ªéƒ½æ˜¯é›¶å‘é‡åˆ™ç›¸ä¼¼åº¦ä¸º1ï¼Œå¦åˆ™ä¸º0
                        cosine_sim = 1.0 if (norm1 < 1e-8 and norm2 < 1e-8) else 0.0
                    else:
                        cosine_sim = F.cosine_similarity(
                            dim1_values.unsqueeze(0), dim2_values.unsqueeze(0)
                        ).item()
                        
                        # ç¡®ä¿ä½™å¼¦ç›¸ä¼¼åº¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
                        if not np.isfinite(cosine_sim):
                            cosine_sim = 0.0
                        else:
                            cosine_sim = max(-1.0, min(1.0, cosine_sim))
                            # è½¬æ¢åˆ° [0, 1] èŒƒå›´
                            cosine_sim = (cosine_sim + 1.0) / 2.0
                            
                except Exception as e:
                    logger.debug(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥ (dim {dim}): {e}")
                    cosine_sim = 0.5
                
                # 2. çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆæ•°å€¼ç¨³å®šç‰ˆï¼‰
                try:
                    # æ£€æŸ¥æ ‡å‡†å·®æ˜¯å¦ä¸ºé›¶
                    std1 = np.std(dim1_numpy)
                    std2 = np.std(dim2_numpy)
                    
                    if std1 < 1e-8 or std2 < 1e-8:
                        # æ ‡å‡†å·®ä¸ºé›¶çš„æƒ…å†µï¼šæ£€æŸ¥å‡å€¼æ˜¯å¦ç›¸ç­‰
                        mean1 = np.mean(dim1_numpy)
                        mean2 = np.mean(dim2_numpy)
                        
                        if abs(mean1 - mean2) < 1e-8:
                            pearson_corr = 1.0  # å®Œå…¨ç›¸åŒçš„å¸¸æ•°å€¼
                        else:
                            pearson_corr = 0.0  # ä¸åŒçš„å¸¸æ•°å€¼
                    else:
                        # æ ‡å‡†ç›¸å…³ç³»æ•°è®¡ç®—
                        corr_matrix = np.corrcoef(dim1_numpy, dim2_numpy)
                        
                        if corr_matrix.shape == (2, 2):
                            pearson_corr = corr_matrix[0, 1]
                        else:
                            pearson_corr = 0.0
                        
                        # æ£€æŸ¥ç»“æœçš„æœ‰æ•ˆæ€§
                        if not np.isfinite(pearson_corr):
                            pearson_corr = 0.0
                        else:
                            # è½¬æ¢åˆ° [0, 1] èŒƒå›´
                            pearson_corr = (pearson_corr + 1.0) / 2.0
                            
                except Exception as e:
                    logger.debug(f"çš®å°”é€Šç›¸å…³ç³»æ•°è®¡ç®—å¤±è´¥ (dim {dim}): {e}")
                    pearson_corr = 0.5
                
                # 3. åˆ†å¸ƒç›¸ä¼¼åº¦ï¼ˆåŸºäºç»Ÿè®¡ç‰¹å¾ï¼Œæ•°å€¼ç¨³å®šç‰ˆï¼‰
                try:
                    mean1 = dim1_values.mean().item()
                    mean2 = dim2_values.mean().item()
                    std1 = dim1_values.std().item()
                    std2 = dim2_values.std().item()
                    
                    # å‡å€¼ç›¸ä¼¼åº¦
                    if abs(mean1) + abs(mean2) < 1e-8:
                        mean_sim = 1.0  # ä¸¤ä¸ªå‡å€¼éƒ½æ¥è¿‘0
                    else:
                        mean_sim = 1.0 - abs(mean1 - mean2) / (abs(mean1) + abs(mean2) + 1e-8)
                    
                    # æ ‡å‡†å·®ç›¸ä¼¼åº¦  
                    if std1 + std2 < 1e-8:
                        std_sim = 1.0  # ä¸¤ä¸ªæ ‡å‡†å·®éƒ½æ¥è¿‘0
                    else:
                        std_sim = 1.0 - abs(std1 - std2) / (std1 + std2 + 1e-8)
                    
                    # ç¡®ä¿ç»“æœåœ¨æœ‰æ•ˆèŒƒå›´å†…
                    mean_sim = max(0.0, min(1.0, mean_sim))
                    std_sim = max(0.0, min(1.0, std_sim))
                    
                except Exception as e:
                    logger.debug(f"åˆ†å¸ƒç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥ (dim {dim}): {e}")
                    mean_sim = 0.5
                    std_sim = 0.5
                
                # 4. ç»¼åˆç›¸ä¼¼åº¦
                try:
                    dim_similarity = (cosine_sim + pearson_corr + mean_sim + std_sim) / 4.0
                    dim_similarity = max(0.0, min(1.0, dim_similarity))
                    
                    if not np.isfinite(dim_similarity):
                        dim_similarity = 0.5
                        
                    similarities.append(dim_similarity)
                    
                except Exception as e:
                    logger.debug(f"ç»¼åˆç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥ (dim {dim}): {e}")
                    similarities.append(0.5)
                
        except Exception as e:
            logger.warning(f"ç‰¹å¾ç›¸ä¼¼åº¦è®¡ç®—å¼‚å¸¸: {e}")
            # è¿”å›é»˜è®¤ç›¸ä¼¼åº¦
            similarities = [0.5] * min(features1.size(1), features2.size(1))
        
        # æœ€ç»ˆæœ‰æ•ˆæ€§æ£€æŸ¥
        if not similarities:
            similarities = [0.5] * min(features1.size(1), features2.size(1))
        
        # ç¡®ä¿æ‰€æœ‰ç›¸ä¼¼åº¦éƒ½åœ¨æœ‰æ•ˆèŒƒå›´å†…
        similarities = [max(0.0, min(1.0, sim)) for sim in similarities]
        
        return similarities
    
    def _compute_sparsity_pattern_similarity(self, source_data: HeteroData, generated_data: HeteroData) -> float:
        """è®¡ç®—ç¨€ç–æ€§æ¨¡å¼ç›¸ä¼¼åº¦"""
        try:
            source_edges = source_data[('constraint', 'connects', 'variable')].edge_index
            generated_edges = generated_data[('constraint', 'connects', 'variable')].edge_index
            
            n_constraints = source_data['constraint'].x.size(0)
            n_variables = source_data['variable'].x.size(0)
            
            # æ„å»ºç¨€ç–æ€§æ¨¡å¼å‘é‡ï¼ˆæ¯ä¸ªçº¦æŸçš„ç¨€ç–åº¦ï¼‰
            source_sparsity = torch.zeros(n_constraints)
            generated_sparsity = torch.zeros(n_constraints)
            
            for c in range(n_constraints):
                source_connections = (source_edges[0] == c).sum().item()
                generated_connections = (generated_edges[0] == c).sum().item()
                
                source_sparsity[c] = source_connections / n_variables
                generated_sparsity[c] = generated_connections / n_variables
            
            # è®¡ç®—ç¨€ç–æ€§æ¨¡å¼çš„ç›¸ä¼¼åº¦
            sparsity_sim = F.cosine_similarity(
                source_sparsity.unsqueeze(0), generated_sparsity.unsqueeze(0)
            ).item()
            
            return max(0.0, min(1.0, sparsity_sim))
            
        except Exception as e:
            logger.warning(f"ç¨€ç–æ€§æ¨¡å¼è®¡ç®—å¼‚å¸¸: {e}")
            return 0.5
    
    def _compute_connectivity_pattern_similarity(self, source_data: HeteroData, generated_data: HeteroData) -> float:
        """è®¡ç®—è¿æ¥æ¨¡å¼ç›¸ä¼¼åº¦"""
        try:
            source_edges = source_data[('constraint', 'connects', 'variable')].edge_index
            generated_edges = generated_data[('constraint', 'connects', 'variable')].edge_index
            
            # è®¡ç®—é‚»æ¥çŸ©é˜µçš„ç›¸ä¼¼æ€§
            n_constraints = source_data['constraint'].x.size(0)
            n_variables = source_data['variable'].x.size(0)
            
            # æ„å»ºäºŒåˆ†å›¾çš„å—çŸ©é˜µè¡¨ç¤º
            source_adj = torch.zeros(n_constraints, n_variables)
            generated_adj = torch.zeros(n_constraints, n_variables)
            
            source_adj[source_edges[0], source_edges[1]] = 1
            generated_adj[generated_edges[0], generated_edges[1]] = 1
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦
            intersection = (source_adj * generated_adj).sum().item()
            union = ((source_adj + generated_adj) > 0).sum().item()
            
            jaccard_sim = intersection / union if union > 0 else 0.0
            
            return max(0.0, min(1.0, jaccard_sim))
            
        except Exception as e:
            logger.warning(f"è¿æ¥æ¨¡å¼è®¡ç®—å¼‚å¸¸: {e}")
            return 0.5
    
    def validate_milp_instances(self, source_data: HeteroData, generated_data: HeteroData, 
                               source_milp_instance=None, generated_milp_instance=None) -> Dict[str, Any]:
        """
        éªŒè¯MILPå®ä¾‹çš„æ±‚è§£ç‰¹æ€§ï¼ˆæœ€é‡è¦çš„è´¨é‡è¯„ä¼°ï¼‰
        
        Args:
            source_data: æºå›¾æ•°æ®
            generated_data: ç”Ÿæˆçš„å›¾æ•°æ®  
            source_milp_instance: æºMILPå®ä¾‹ï¼ˆå¯é€‰ï¼‰
            generated_milp_instance: ç”Ÿæˆçš„MILPå®ä¾‹ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ±‚è§£éªŒè¯ç»“æœå­—å…¸
        """
        validation_results = {
            'source_solve_results': {},
            'generated_solve_results': {},
            'comparative_analysis': {},
            'validation_summary': {}
        }
        
        try:
            # å¦‚æœæ²¡æœ‰æä¾›MILPå®ä¾‹ï¼Œä»å›¾æ•°æ®é‡æ„
            if source_milp_instance is None:
                source_milp_instance = self._reconstruct_milp_from_graph(source_data)
            if generated_milp_instance is None:
                generated_milp_instance = self._reconstruct_milp_from_graph(generated_data)
            
            # 1. æ±‚è§£æºå®ä¾‹
            logger.info("æ±‚è§£æºMILPå®ä¾‹...")
            source_solve_results = self._solve_milp_instance(source_milp_instance, "source")
            validation_results['source_solve_results'] = source_solve_results
            
            # 2. æ±‚è§£ç”Ÿæˆå®ä¾‹  
            logger.info("æ±‚è§£ç”Ÿæˆçš„MILPå®ä¾‹...")
            generated_solve_results = self._solve_milp_instance(generated_milp_instance, "generated")
            validation_results['generated_solve_results'] = generated_solve_results
            
            # 3. æ¯”è¾ƒåˆ†æ
            logger.info("è¿›è¡Œæ±‚è§£ç‰¹æ€§æ¯”è¾ƒåˆ†æ...")
            comparative_analysis = self._compare_solve_results(source_solve_results, generated_solve_results)
            validation_results['comparative_analysis'] = comparative_analysis
            
            # 4. ç”ŸæˆéªŒè¯æ‘˜è¦
            validation_summary = self._generate_validation_summary(
                source_solve_results, generated_solve_results, comparative_analysis
            )
            validation_results['validation_summary'] = validation_summary
            
            logger.info("MILPå®ä¾‹æ±‚è§£éªŒè¯å®Œæˆ")
            
        except Exception as e:
            logger.error(f"MILPæ±‚è§£éªŒè¯å¤±è´¥: {e}")
            validation_results['error'] = str(e)
            validation_results['validation_summary'] = {
                'is_valid': False,
                'error_message': str(e)
            }
        
        return validation_results
    
    def _reconstruct_milp_from_graph(self, graph_data: HeteroData) -> Dict[str, Any]:
        """ä»å›¾æ•°æ®é‡æ„MILPå®ä¾‹"""
        try:
            edges = graph_data[('constraint', 'connects', 'variable')].edge_index
            n_constraints = graph_data['constraint'].x.size(0)
            n_variables = graph_data['variable'].x.size(0)
            
            # æ„å»ºçº¦æŸçŸ©é˜µ
            import scipy.sparse as sp
            constraint_matrix = sp.lil_matrix((n_constraints, n_variables))
            
            # è®¾ç½®éé›¶å…ƒç´ ï¼ˆä½¿ç”¨å›¾æ•°æ®ä¸­çš„è¾¹æƒé‡ï¼Œå¦‚æœæœ‰çš„è¯ï¼‰
            for i in range(edges.size(1)):
                constraint_idx = edges[0, i].item()
                variable_idx = edges[1, i].item()
                
                # å¦‚æœæœ‰è¾¹æƒé‡ï¼Œä½¿ç”¨è¾¹æƒé‡ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
                if hasattr(graph_data[('constraint', 'connects', 'variable')], 'edge_attr'):
                    weight = graph_data[('constraint', 'connects', 'variable')].edge_attr[i].item()
                else:
                    weight = 1.0  # é»˜è®¤æƒé‡
                
                constraint_matrix[constraint_idx, variable_idx] = weight
            
            # ä»èŠ‚ç‚¹ç‰¹å¾ä¸­æå–MILPå‚æ•°
            constraint_features = graph_data['constraint'].x.cpu().numpy()
            variable_features = graph_data['variable'].x.cpu().numpy()
            
            # æå–ç›®æ ‡å‡½æ•°ç³»æ•°ï¼ˆå‡è®¾åœ¨å˜é‡ç‰¹å¾çš„æŸä¸€ç»´ï¼‰
            objective_coeffs = variable_features[:, 0] if variable_features.shape[1] > 0 else np.ones(n_variables)
            
            # æå–å³ç«¯é¡¹ï¼ˆå‡è®¾åœ¨çº¦æŸç‰¹å¾çš„æŸä¸€ç»´ï¼‰  
            rhs_values = constraint_features[:, 0] if constraint_features.shape[1] > 0 else np.ones(n_constraints)
            
            # å˜é‡ç•Œé™ï¼ˆå‡è®¾åœ¨å˜é‡ç‰¹å¾ä¸­ï¼‰
            if variable_features.shape[1] > 2:
                lower_bounds = variable_features[:, 1] 
                upper_bounds = variable_features[:, 2]
            else:
                lower_bounds = np.zeros(n_variables)
                upper_bounds = np.ones(n_variables) * 1000  # å¤§çš„ä¸Šç•Œ
            
            milp_instance = {
                'constraint_matrix': constraint_matrix.tocsr(),
                'objective_coeffs': objective_coeffs,
                'rhs_values': rhs_values,
                'lower_bounds': lower_bounds,
                'upper_bounds': upper_bounds,
                'variable_types': ['continuous'] * n_variables,  # ç®€åŒ–ï¼šå‡è®¾éƒ½æ˜¯è¿ç»­å˜é‡
                'constraint_senses': ['<='] * n_constraints,      # ç®€åŒ–ï¼šå‡è®¾éƒ½æ˜¯<=çº¦æŸ
                'n_constraints': n_constraints,
                'n_variables': n_variables
            }
            
            return milp_instance
            
        except Exception as e:
            logger.error(f"ä»å›¾æ•°æ®é‡æ„MILPå®ä¾‹å¤±è´¥: {e}")
            raise
    
    def _solve_milp_instance(self, milp_instance: Dict[str, Any], instance_name: str) -> Dict[str, Any]:
        """ä½¿ç”¨å¤šç§æ±‚è§£å™¨æ±‚è§£MILPå®ä¾‹"""
        solve_results = {
            'instance_name': instance_name,
            'solvers_used': [],
            'solve_attempts': {},
            'best_result': None,
            'solving_statistics': {}
        }
        
        # å°è¯•ä½¿ç”¨çš„æ±‚è§£å™¨åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        solvers_to_try = ['CVXPY_DEFAULT', 'CVXPY_ECOS', 'CVXPY_SCS']
        
        try:
            # æ·»åŠ å•†ä¸šæ±‚è§£å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                import gurobipy
                solvers_to_try.insert(0, 'GUROBI')
            except ImportError:
                pass
            
            try:
                import cplex
                solvers_to_try.insert(0, 'CPLEX')
            except ImportError:
                pass
        except:
            pass
        
        best_solve_time = float('inf')
        best_objective = None
        
        for solver_name in solvers_to_try[:3]:  # é™åˆ¶å°è¯•çš„æ±‚è§£å™¨æ•°é‡
            try:
                logger.debug(f"å°è¯•ä½¿ç”¨æ±‚è§£å™¨: {solver_name}")
                solve_attempt = self._solve_with_specific_solver(milp_instance, solver_name)
                solve_results['solve_attempts'][solver_name] = solve_attempt
                solve_results['solvers_used'].append(solver_name)
                
                # æ›´æ–°æœ€ä½³ç»“æœ
                if solve_attempt['status'] == 'optimal' and solve_attempt['solve_time'] < best_solve_time:
                    best_solve_time = solve_attempt['solve_time']
                    best_objective = solve_attempt['objective_value']
                    solve_results['best_result'] = solve_attempt
                    solve_results['best_result']['solver'] = solver_name
                
            except Exception as e:
                logger.warning(f"æ±‚è§£å™¨ {solver_name} å¤±è´¥: {e}")
                solve_results['solve_attempts'][solver_name] = {
                    'status': 'error',
                    'error_message': str(e),
                    'solve_time': None,
                    'objective_value': None
                }
        
        # ç”Ÿæˆæ±‚è§£ç»Ÿè®¡
        solve_results['solving_statistics'] = self._compute_solving_statistics(solve_results)
        
        return solve_results
    
    def _solve_with_specific_solver(self, milp_instance: Dict[str, Any], solver_name: str) -> Dict[str, Any]:
        """ä½¿ç”¨ç‰¹å®šæ±‚è§£å™¨æ±‚è§£"""
        import time
        start_time = time.time()
        
        try:
            if solver_name.startswith('CVXPY'):
                return self._solve_with_cvxpy(milp_instance, solver_name)
            elif solver_name == 'GUROBI':
                return self._solve_with_gurobi(milp_instance)
            elif solver_name == 'CPLEX':
                return self._solve_with_cplex(milp_instance)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ±‚è§£å™¨: {solver_name}")
                
        except Exception as e:
            solve_time = time.time() - start_time
            return {
                'status': 'error',
                'error_message': str(e),
                'solve_time': solve_time,
                'objective_value': None,
                'solution': None
            }
    
    def _solve_with_cvxpy(self, milp_instance: Dict[str, Any], solver_name: str) -> Dict[str, Any]:
        """ä½¿ç”¨CVXPYæ±‚è§£"""
        import cvxpy as cp
        import time
        
        start_time = time.time()
        
        try:
            # åˆ›å»ºå˜é‡
            n_vars = milp_instance['n_variables']
            x = cp.Variable(n_vars)
            
            # ç›®æ ‡å‡½æ•°
            objective = cp.Minimize(milp_instance['objective_coeffs'] @ x)
            
            # çº¦æŸ
            constraints = []
            
            # ä¸»è¦çº¦æŸ Ax <= b
            A = milp_instance['constraint_matrix']
            b = milp_instance['rhs_values']
            constraints.append(A @ x <= b)
            
            # å˜é‡ç•Œé™
            constraints.append(x >= milp_instance['lower_bounds'])
            constraints.append(x <= milp_instance['upper_bounds'])
            
            # åˆ›å»ºé—®é¢˜
            problem = cp.Problem(objective, constraints)
            
            # é€‰æ‹©æ±‚è§£å™¨
            if solver_name == 'CVXPY_ECOS':
                solver = cp.ECOS
            elif solver_name == 'CVXPY_SCS':
                solver = cp.SCS
            else:
                solver = None  # ä½¿ç”¨é»˜è®¤æ±‚è§£å™¨
            
            # æ±‚è§£
            if solver:
                problem.solve(solver=solver, verbose=False)
            else:
                problem.solve(verbose=False)
            
            solve_time = time.time() - start_time
            
            # æ£€æŸ¥çŠ¶æ€
            if problem.status == cp.OPTIMAL:
                return {
                    'status': 'optimal',
                    'objective_value': problem.value,
                    'solve_time': solve_time,
                    'solution': x.value,
                    'num_variables': n_vars,
                    'num_constraints': milp_instance['n_constraints']
                }
            elif problem.status == cp.INFEASIBLE:
                return {
                    'status': 'infeasible',
                    'objective_value': None,
                    'solve_time': solve_time,
                    'solution': None,
                    'num_variables': n_vars,
                    'num_constraints': milp_instance['n_constraints']
                }
            else:
                return {
                    'status': 'unknown',
                    'objective_value': problem.value,
                    'solve_time': solve_time,
                    'solution': x.value if x.value is not None else None,
                    'num_variables': n_vars,
                    'num_constraints': milp_instance['n_constraints']
                }
                
        except Exception as e:
            solve_time = time.time() - start_time
            raise Exception(f"CVXPYæ±‚è§£å¤±è´¥: {e}")
    
    def _solve_with_gurobi(self, milp_instance: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨Gurobiæ±‚è§£ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        try:
            import gurobipy as gp
            from gurobipy import GRB
            import time
            
            start_time = time.time()
            
            # åˆ›å»ºæ¨¡å‹
            model = gp.Model("milp_validation")
            model.setParam('OutputFlag', 0)  # é™é»˜æ¨¡å¼
            model.setParam('TimeLimit', 60)  # 60ç§’æ—¶é—´é™åˆ¶
            
            # åˆ›å»ºå˜é‡
            n_vars = milp_instance['n_variables']
            x = model.addVars(n_vars, lb=milp_instance['lower_bounds'], 
                             ub=milp_instance['upper_bounds'], name="x")
            
            # è®¾ç½®ç›®æ ‡å‡½æ•°
            obj_coeffs = milp_instance['objective_coeffs']
            model.setObjective(gp.quicksum(obj_coeffs[i] * x[i] for i in range(n_vars)), GRB.MINIMIZE)
            
            # æ·»åŠ çº¦æŸ
            A = milp_instance['constraint_matrix']
            b = milp_instance['rhs_values']
            
            for i in range(milp_instance['n_constraints']):
                row = A.getrow(i)
                lhs = gp.quicksum(row.data[j] * x[row.indices[j]] for j in range(len(row.data)))
                model.addConstr(lhs <= b[i], f"constraint_{i}")
            
            # æ±‚è§£
            model.optimize()
            solve_time = time.time() - start_time
            
            if model.status == GRB.OPTIMAL:
                solution = [x[i].x for i in range(n_vars)]
                return {
                    'status': 'optimal',
                    'objective_value': model.objVal,
                    'solve_time': solve_time,
                    'solution': solution,
                    'num_variables': n_vars,
                    'num_constraints': milp_instance['n_constraints'],
                    'mip_gap': model.MIPGap if hasattr(model, 'MIPGap') else 0.0
                }
            elif model.status == GRB.INFEASIBLE:
                return {
                    'status': 'infeasible',
                    'objective_value': None,
                    'solve_time': solve_time,
                    'solution': None,
                    'num_variables': n_vars,
                    'num_constraints': milp_instance['n_constraints']
                }
            else:
                return {
                    'status': 'unknown',
                    'objective_value': model.objVal if model.objVal != GRB.INFINITY else None,
                    'solve_time': solve_time,
                    'solution': None,
                    'num_variables': n_vars,
                    'num_constraints': milp_instance['n_constraints']
                }
                
        except Exception as e:
            raise Exception(f"Gurobiæ±‚è§£å¤±è´¥: {e}")
    
    def _solve_with_cplex(self, milp_instance: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨CPLEXæ±‚è§£ï¼ˆå ä½ç¬¦å®ç°ï¼‰"""
        # è¿™é‡Œå¯ä»¥å®ç°CPLEXæ±‚è§£å™¨é›†æˆ
        # å½“å‰è¿”å›æœªå®ç°çŠ¶æ€
        return {
            'status': 'not_implemented',
            'error_message': 'CPLEXæ±‚è§£å™¨é›†æˆå°šæœªå®ç°',
            'solve_time': 0.0,
            'objective_value': None,
            'solution': None
        }
    
    def _compute_solving_statistics(self, solve_results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ±‚è§£ç»Ÿè®¡ä¿¡æ¯"""
        statistics = {
            'num_solvers_tried': len(solve_results['solvers_used']),
            'successful_solvers': [],
            'failed_solvers': [],
            'optimal_solutions_found': 0,
            'infeasible_results': 0,
            'average_solve_time': 0.0,
            'solve_time_variance': 0.0
        }
        
        solve_times = []
        
        for solver, result in solve_results['solve_attempts'].items():
            if result['status'] == 'optimal':
                statistics['successful_solvers'].append(solver)
                statistics['optimal_solutions_found'] += 1
                if result['solve_time'] is not None:
                    solve_times.append(result['solve_time'])
            elif result['status'] == 'infeasible':
                statistics['infeasible_results'] += 1
            else:
                statistics['failed_solvers'].append(solver)
        
        if solve_times:
            statistics['average_solve_time'] = np.mean(solve_times)
            statistics['solve_time_variance'] = np.var(solve_times)
        
        return statistics
    
    def _compare_solve_results(self, source_results: Dict[str, Any], 
                              generated_results: Dict[str, Any]) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªå®ä¾‹çš„æ±‚è§£ç»“æœ"""
        comparison = {
            'feasibility_consistency': False,
            'objective_value_comparison': {},
            'solve_time_comparison': {},
            'difficulty_assessment': {},
            'quality_score': 0.0
        }
        
        try:
            source_best = source_results.get('best_result')
            generated_best = generated_results.get('best_result')
            
            # 1. å¯è¡Œæ€§ä¸€è‡´æ€§æ£€æŸ¥
            if source_best and generated_best:
                source_feasible = source_best['status'] == 'optimal'
                generated_feasible = generated_best['status'] == 'optimal'
                comparison['feasibility_consistency'] = source_feasible == generated_feasible
                
                # 2. ç›®æ ‡å€¼æ¯”è¾ƒï¼ˆå¦‚æœéƒ½å¯è¡Œï¼‰
                if source_feasible and generated_feasible:
                    source_obj = source_best['objective_value']
                    generated_obj = generated_best['objective_value']
                    
                    comparison['objective_value_comparison'] = {
                        'source_objective': source_obj,
                        'generated_objective': generated_obj,
                        'relative_difference': abs(source_obj - generated_obj) / (abs(source_obj) + 1e-8),
                        'objectives_similar': abs(source_obj - generated_obj) / (abs(source_obj) + 1e-8) < 0.5
                    }
                    
                    # 3. æ±‚è§£æ—¶é—´æ¯”è¾ƒ
                    source_time = source_best['solve_time']
                    generated_time = generated_best['solve_time']
                    
                    comparison['solve_time_comparison'] = {
                        'source_solve_time': source_time,
                        'generated_solve_time': generated_time,
                        'time_ratio': generated_time / (source_time + 1e-8),
                        'similar_difficulty': 0.1 <= (generated_time / (source_time + 1e-8)) <= 10.0
                    }
                    
                    # 4. éš¾åº¦è¯„ä¼°
                    comparison['difficulty_assessment'] = {
                        'source_complexity_score': self._assess_instance_complexity(source_results),
                        'generated_complexity_score': self._assess_instance_complexity(generated_results),
                        'complexity_preserved': True  # ç®€åŒ–è¯„ä¼°
                    }
                    
                    # 5. ç»¼åˆè´¨é‡è¯„åˆ†
                    quality_components = [
                        1.0 if comparison['feasibility_consistency'] else 0.0,
                        0.8 if comparison['objective_value_comparison']['objectives_similar'] else 0.2,
                        0.8 if comparison['solve_time_comparison']['similar_difficulty'] else 0.2,
                        0.8 if comparison['difficulty_assessment']['complexity_preserved'] else 0.2
                    ]
                    comparison['quality_score'] = np.mean(quality_components)
            
        except Exception as e:
            logger.warning(f"æ±‚è§£ç»“æœæ¯”è¾ƒå¤±è´¥: {e}")
            comparison['error'] = str(e)
            comparison['quality_score'] = 0.0
        
        return comparison
    
    def _assess_instance_complexity(self, solve_results: Dict[str, Any]) -> float:
        """è¯„ä¼°MILPå®ä¾‹çš„å¤æ‚åº¦"""
        try:
            best_result = solve_results.get('best_result')
            if not best_result:
                return 0.5
            
            # åŸºäºæ±‚è§£æ—¶é—´ã€å˜é‡æ•°ã€çº¦æŸæ•°ç­‰å› ç´ è¯„ä¼°å¤æ‚åº¦
            solve_time = best_result.get('solve_time', 0.0)
            num_vars = best_result.get('num_variables', 0)
            num_constraints = best_result.get('num_constraints', 0)
            
            # ç®€åŒ–çš„å¤æ‚åº¦è¯„åˆ†
            time_score = min(1.0, solve_time / 10.0)  # 10ç§’ä¸ºé«˜å¤æ‚åº¦åŸºå‡†
            size_score = min(1.0, (num_vars + num_constraints) / 10000.0)  # 10kä¸ºé«˜å¤æ‚åº¦åŸºå‡†
            
            complexity_score = (time_score + size_score) / 2.0
            return complexity_score
            
        except Exception as e:
            logger.warning(f"å¤æ‚åº¦è¯„ä¼°å¤±è´¥: {e}")
            return 0.5
    
    def _generate_validation_summary(self, source_results: Dict[str, Any], 
                                   generated_results: Dict[str, Any],
                                   comparative_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æ‘˜è¦"""
        summary = {
            'is_valid': False,
            'validation_score': 0.0,
            'key_findings': [],
            'recommendations': [],
            'detailed_assessment': {}
        }
        
        try:
            # åŸºæœ¬éªŒè¯æ£€æŸ¥
            source_solvable = source_results.get('best_result', {}).get('status') == 'optimal'
            generated_solvable = generated_results.get('best_result', {}).get('status') == 'optimal'
            
            summary['detailed_assessment'] = {
                'source_solvable': source_solvable,
                'generated_solvable': generated_solvable,
                'feasibility_consistent': comparative_analysis.get('feasibility_consistency', False),
                'quality_score': comparative_analysis.get('quality_score', 0.0)
            }
            
            # å…³é”®å‘ç°
            if source_solvable and generated_solvable:
                summary['key_findings'].append("âœ… æºå®ä¾‹å’Œç”Ÿæˆå®ä¾‹éƒ½å¯ä»¥æˆåŠŸæ±‚è§£")
                if comparative_analysis.get('feasibility_consistency'):
                    summary['key_findings'].append("âœ… å¯è¡Œæ€§çŠ¶æ€ä¸€è‡´")
            elif not source_solvable and not generated_solvable:
                summary['key_findings'].append("âš ï¸ æºå®ä¾‹å’Œç”Ÿæˆå®ä¾‹éƒ½ä¸å¯è¡Œï¼ˆå¯èƒ½æ˜¯æœŸæœ›çš„ï¼‰")
            else:
                summary['key_findings'].append("âŒ å¯è¡Œæ€§çŠ¶æ€ä¸ä¸€è‡´ï¼Œç”Ÿæˆè´¨é‡æœ‰é—®é¢˜")
            
            # éªŒè¯åˆ†æ•°è®¡ç®—
            quality_score = comparative_analysis.get('quality_score', 0.0)
            summary['validation_score'] = quality_score
            
            # éªŒè¯åˆ¤å®š
            if quality_score >= 0.7:
                summary['is_valid'] = True
                summary['key_findings'].append(f"ğŸ‰ ç”Ÿæˆè´¨é‡ä¼˜ç§€ (å¾—åˆ†: {quality_score:.3f})")
                summary['recommendations'].append("ç”Ÿæˆçš„MILPå®ä¾‹è´¨é‡è‰¯å¥½ï¼Œå¯ç”¨äºç ”ç©¶å’Œæµ‹è¯•")
            elif quality_score >= 0.5:
                summary['is_valid'] = True
                summary['key_findings'].append(f"âš ï¸ ç”Ÿæˆè´¨é‡ä¸€èˆ¬ (å¾—åˆ†: {quality_score:.3f})")
                summary['recommendations'].append("ç”Ÿæˆçš„å®ä¾‹åŸºæœ¬å¯ç”¨ï¼Œä½†å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹")
            else:
                summary['is_valid'] = False
                summary['key_findings'].append(f"âŒ ç”Ÿæˆè´¨é‡ä¸ä½³ (å¾—åˆ†: {quality_score:.3f})")
                summary['recommendations'].append("éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹æˆ–è°ƒæ•´ç”Ÿæˆå‚æ•°")
            
        except Exception as e:
            logger.error(f"éªŒè¯æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            summary['error'] = str(e)
            summary['key_findings'].append(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        
        return summary


def create_inference_engine(model: G2MILPGenerator,
                          config: InferenceConfig = None) -> G2MILPInference:
    """
    åˆ›å»ºG2MILPæ¨ç†å™¨çš„å·¥å‚å‡½æ•°
    
    Args:
        model: è®­ç»ƒå¥½çš„G2MILPç”Ÿæˆå™¨
        config: æ¨ç†é…ç½®
        
    Returns:
        G2MILPæ¨ç†å™¨å®ä¾‹
    """
    return G2MILPInference(model, config)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("G2MILPæ¨ç†æ¨¡å—æµ‹è¯•")
    print("=" * 40)
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    inference_config = InferenceConfig(
        eta=0.1,
        temperature=1.0,
        sample_from_prior=True,
        compute_similarity_metrics=True
    )
    
    print(f"æ¨ç†é…ç½®:")
    print(f"- Î· (eta): {inference_config.eta}")
    print(f"- Temperature: {inference_config.temperature}")
    print(f"- Sample from prior: {inference_config.sample_from_prior}")
    print(f"- Device: {inference_config.device}")
    print("æ¨ç†å™¨é…ç½®åˆ›å»ºæˆåŠŸ!")