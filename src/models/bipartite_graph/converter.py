"""
G2MILPä¸»è½¬æ¢å™¨
æ•´åˆCVXPYæå–å™¨å’ŒäºŒåˆ†å›¾æ„å»ºå™¨ï¼Œæä¾›å®Œæ•´çš„è½¬æ¢æµç¨‹

ä¸»è¦åŠŸèƒ½:
1. ç«¯åˆ°ç«¯çš„CVXPYé—®é¢˜åˆ°äºŒåˆ†å›¾è½¬æ¢
2. æ‰¹é‡å¤„ç†å¤šä¸ªMILPå®ä¾‹
3. é›†æˆéªŒè¯å’Œé”™è¯¯å¤„ç†
4. æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–
5. æ”¯æŒä¸åŒçš„è½¬æ¢é…ç½®
"""

import cvxpy as cp
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import json
import pickle

from .data_structures import BipartiteGraph, GraphStatistics
from .extractor import CVXPYToMILPExtractor, MILPStandardForm
from .builder import BipartiteGraphBuilder

logger = logging.getLogger(__name__)


@dataclass
class ConversionConfig:
    """è½¬æ¢é…ç½®"""
    # æå–å™¨é…ç½®
    use_sparse_matrix: bool = True          # ä½¿ç”¨ç¨€ç–çŸ©é˜µ
    extraction_tolerance: float = 1e-12     # æå–å®¹å·®
    
    # æ„å»ºå™¨é…ç½®  
    normalize_coefficients: bool = True      # å½’ä¸€åŒ–ç³»æ•°
    sparse_threshold: float = 0.1           # ç¨€ç–é˜ˆå€¼
    batch_size: int = 1000                  # æ‰¹å¤„ç†å¤§å°
    
    # è¾“å‡ºé…ç½®
    save_intermediate_results: bool = False  # ä¿å­˜ä¸­é—´ç»“æœ
    output_directory: Optional[str] = None   # è¾“å‡ºç›®å½•
    
    # æ€§èƒ½é…ç½®
    memory_limit_gb: float = 8.0            # å†…å­˜é™åˆ¶(GB)
    enable_parallel: bool = False           # å¯ç”¨å¹¶è¡Œå¤„ç†
    max_workers: int = 4                    # æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    
    # éªŒè¯é…ç½®
    validate_graph: bool = True             # éªŒè¯å›¾ç»“æ„
    compute_statistics: bool = True         # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯


@dataclass
class ConversionResult:
    """è½¬æ¢ç»“æœ"""
    # è½¬æ¢äº§ç‰©
    bipartite_graph: BipartiteGraph
    standard_form: MILPStandardForm
    
    # è½¬æ¢ç»Ÿè®¡
    conversion_id: str
    source_problem_name: str
    start_time: datetime
    end_time: datetime
    total_duration: float
    
    # é˜¶æ®µè€—æ—¶
    extraction_duration: float
    building_duration: float
    validation_duration: float = 0.0
    
    # é—®é¢˜ç»Ÿè®¡
    problem_statistics: Dict[str, Any] = field(default_factory=dict)
    
    # è½¬æ¢é…ç½®
    config: ConversionConfig = None
    
    # é”™è¯¯ä¿¡æ¯
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–è½¬æ¢æ‘˜è¦"""
        return {
            'conversion_id': self.conversion_id,
            'source_problem': self.source_problem_name,
            'success': len(self.errors) == 0,
            'total_duration': self.total_duration,
            'graph_statistics': {
                'n_variable_nodes': len(self.bipartite_graph.variable_nodes),
                'n_constraint_nodes': len(self.bipartite_graph.constraint_nodes),
                'n_edges': len(self.bipartite_graph.edges),
                'density': self.bipartite_graph.statistics.density
            },
            'problem_statistics': self.problem_statistics,
            'errors': self.errors,
            'warnings': self.warnings
        }


class G2MILPConverter:
    """
    G2MILPä¸»è½¬æ¢å™¨
    æä¾›CVXPYé—®é¢˜åˆ°äºŒåˆ†å›¾çš„å®Œæ•´è½¬æ¢æµç¨‹
    """
    
    def __init__(self, config: ConversionConfig = None):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            config: è½¬æ¢é…ç½®å¯¹è±¡
        """
        self.config = config or ConversionConfig()
        
        # åˆ›å»ºç»„ä»¶
        self.extractor = None
        self.builder = BipartiteGraphBuilder(
            normalize_coefficients=self.config.normalize_coefficients,
            sparse_threshold=self.config.sparse_threshold,
            batch_size=self.config.batch_size
        )
        
        # è½¬æ¢å†å²
        self.conversion_history: List[ConversionResult] = []
        
        # æ€§èƒ½ç›‘æ§
        self.performance_stats: Dict[str, Any] = {
            'total_conversions': 0,
            'successful_conversions': 0,
            'failed_conversions': 0,
            'average_duration': 0.0,
            'memory_usage': []
        }
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if self.config.output_directory:
            self.output_dir = Path(self.config.output_directory)
            self.output_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.output_dir = None
        
        logger.info("G2MILPè½¬æ¢å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  é…ç½®: ç¨€ç–çŸ©é˜µ={self.config.use_sparse_matrix}, å½’ä¸€åŒ–={self.config.normalize_coefficients}")
        logger.info(f"  è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def convert_problem(self, 
                       problem: cp.Problem,
                       problem_name: str = None,
                       graph_id: str = None) -> ConversionResult:
        """
        è½¬æ¢å•ä¸ªCVXPYé—®é¢˜
        
        Args:
            problem: CVXPYé—®é¢˜å¯¹è±¡
            problem_name: é—®é¢˜åç§°
            graph_id: å›¾æ ‡è¯†ç¬¦
            
        Returns:
            è½¬æ¢ç»“æœå¯¹è±¡
        """
        # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
        conversion_id = f"conversion_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]}"
        if problem_name is None:
            problem_name = f"problem_{conversion_id}"
        if graph_id is None:
            graph_id = f"graph_{conversion_id}"
        
        logger.info("=" * 70)
        logger.info(f"å¼€å§‹G2MILPè½¬æ¢: {conversion_id}")
        logger.info(f"é—®é¢˜åç§°: {problem_name}")
        logger.info("=" * 70)
        
        start_time = datetime.now()
        result = ConversionResult(
            bipartite_graph=None,
            standard_form=None,
            conversion_id=conversion_id,
            source_problem_name=problem_name,
            start_time=start_time,
            end_time=None,
            total_duration=0.0,
            extraction_duration=0.0,
            building_duration=0.0,
            config=self.config
        )
        
        try:
            # 1. æå–MILPæ ‡å‡†å½¢å¼
            logger.info("æ­¥éª¤1: æå–MILPæ ‡å‡†å½¢å¼...")
            extract_start = datetime.now()
            
            self.extractor = CVXPYToMILPExtractor(problem, problem_name)
            standard_form = self.extractor.extract(
                use_sparse=self.config.use_sparse_matrix,
                tolerance=self.config.extraction_tolerance
            )
            
            result.standard_form = standard_form
            result.extraction_duration = (datetime.now() - extract_start).total_seconds()
            
            # ä¿å­˜ä¸­é—´ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.config.save_intermediate_results and self.output_dir:
                self._save_standard_form(standard_form, conversion_id)
            
            # 2. æ„å»ºäºŒåˆ†å›¾
            logger.info("æ­¥éª¤2: æ„å»ºäºŒåˆ†å›¾...")
            build_start = datetime.now()
            
            bipartite_graph = self.builder.build_graph(standard_form, graph_id)
            result.bipartite_graph = bipartite_graph
            result.building_duration = (datetime.now() - build_start).total_seconds()
            
            # 3. éªŒè¯å’Œç»Ÿè®¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.config.validate_graph or self.config.compute_statistics:
                logger.info("æ­¥éª¤3: éªŒè¯å’Œç»Ÿè®¡...")
                validate_start = datetime.now()
                
                if self.config.validate_graph:
                    self._validate_conversion_result(result)
                
                if self.config.compute_statistics:
                    result.problem_statistics = self._compute_problem_statistics(result)
                
                result.validation_duration = (datetime.now() - validate_start).total_seconds()
            
            # 4. å®Œæˆè½¬æ¢
            result.end_time = datetime.now()
            result.total_duration = (result.end_time - start_time).total_seconds()
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self._update_performance_stats(result)
            
            # ä¿å­˜ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.output_dir:
                self._save_conversion_result(result)
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self.conversion_history.append(result)
            
            logger.info("=" * 70)
            logger.info("âœ… G2MILPè½¬æ¢å®Œæˆ!")
            logger.info("=" * 70)
            logger.info(f"â±ï¸  æ€»è€—æ—¶: {result.total_duration:.3f} ç§’")
            logger.info(f"   - æå–: {result.extraction_duration:.3f} ç§’")
            logger.info(f"   - æ„å»º: {result.building_duration:.3f} ç§’")
            logger.info(f"   - éªŒè¯: {result.validation_duration:.3f} ç§’")
            logger.info(f"ğŸ“Š å›¾ç»Ÿè®¡:")
            logger.info(f"   - å˜é‡èŠ‚ç‚¹: {len(bipartite_graph.variable_nodes)}")
            logger.info(f"   - çº¦æŸèŠ‚ç‚¹: {len(bipartite_graph.constraint_nodes)}")
            logger.info(f"   - è¾¹æ•°é‡: {len(bipartite_graph.edges)}")
            logger.info(f"   - å›¾å¯†åº¦: {bipartite_graph.statistics.density:.4f}")
            logger.info("=" * 70)
            
            return result
            
        except Exception as e:
            # å¤„ç†è½¬æ¢é”™è¯¯
            result.errors.append(str(e))
            result.end_time = datetime.now()
            result.total_duration = (result.end_time - start_time).total_seconds()
            
            logger.error(f"G2MILPè½¬æ¢å¤±è´¥: {e}")
            
            # æ›´æ–°å¤±è´¥ç»Ÿè®¡
            self.performance_stats['failed_conversions'] += 1
            self.conversion_history.append(result)
            
            raise
    
    def convert_batch(self, 
                     problems: List[cp.Problem],
                     problem_names: List[str] = None,
                     graph_ids: List[str] = None) -> List[ConversionResult]:
        """
        æ‰¹é‡è½¬æ¢CVXPYé—®é¢˜
        
        Args:
            problems: CVXPYé—®é¢˜åˆ—è¡¨
            problem_names: é—®é¢˜åç§°åˆ—è¡¨
            graph_ids: å›¾æ ‡è¯†ç¬¦åˆ—è¡¨
            
        Returns:
            è½¬æ¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æ‰¹é‡G2MILPè½¬æ¢: {len(problems)} ä¸ªé—®é¢˜")
        
        # å‡†å¤‡åç§°å’ŒID
        if problem_names is None:
            problem_names = [f"batch_problem_{i:03d}" for i in range(len(problems))]
        if graph_ids is None:
            graph_ids = [f"batch_graph_{i:03d}" for i in range(len(problems))]
        
        results = []
        
        for i, problem in enumerate(problems):
            try:
                logger.info(f"è½¬æ¢ç¬¬ {i+1}/{len(problems)} ä¸ªé—®é¢˜...")
                
                result = self.convert_problem(
                    problem=problem,
                    problem_name=problem_names[i],
                    graph_id=graph_ids[i]
                )
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"ç¬¬ {i+1} ä¸ªé—®é¢˜è½¬æ¢å¤±è´¥: {e}")
                continue
        
        logger.info(f"æ‰¹é‡è½¬æ¢å®Œæˆï¼ŒæˆåŠŸè½¬æ¢ {len(results)} ä¸ªé—®é¢˜")
        return results
    
    def convert_from_milp_instances(self,
                                  milp_instances: List[Any],
                                  instance_id_field: str = 'instance_id') -> List[ConversionResult]:
        """
        ä»MILPå®ä¾‹å¯¹è±¡æ‰¹é‡è½¬æ¢
        
        Args:
            milp_instances: MILPå®ä¾‹åˆ—è¡¨ï¼ˆå¦‚BiasedMILPGeneratorçš„è¾“å‡ºï¼‰
            instance_id_field: å®ä¾‹IDå­—æ®µå
            
        Returns:
            è½¬æ¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"ä»MILPå®ä¾‹æ‰¹é‡è½¬æ¢: {len(milp_instances)} ä¸ªå®ä¾‹")
        
        results = []
        
        for i, instance in enumerate(milp_instances):
            try:
                # è·å–CVXPYé—®é¢˜å¯¹è±¡
                if hasattr(instance, 'cvxpy_problem') and instance.cvxpy_problem is not None:
                    problem = instance.cvxpy_problem
                    problem_name = getattr(instance, instance_id_field, f"milp_instance_{i:03d}")
                    graph_id = f"graph_from_{problem_name}"
                    
                    logger.info(f"è½¬æ¢MILPå®ä¾‹ {i+1}/{len(milp_instances)}: {problem_name}")
                    
                    result = self.convert_problem(
                        problem=problem,
                        problem_name=problem_name,
                        graph_id=graph_id
                    )
                    
                    # æ·»åŠ MILPå®ä¾‹çš„å…ƒä¿¡æ¯
                    if hasattr(instance, 'metadata'):
                        result.bipartite_graph.metadata['milp_instance'] = instance.metadata
                    if hasattr(instance, 'perturbation_config'):
                        result.bipartite_graph.metadata['perturbation_config'] = instance.perturbation_config
                    
                    results.append(result)
                    
                else:
                    logger.warning(f"ç¬¬ {i+1} ä¸ªå®ä¾‹æ²¡æœ‰æœ‰æ•ˆçš„CVXPYé—®é¢˜å¯¹è±¡")
                    
            except Exception as e:
                logger.error(f"è½¬æ¢ç¬¬ {i+1} ä¸ªMILPå®ä¾‹å¤±è´¥: {e}")
                continue
        
        logger.info(f"MILPå®ä¾‹æ‰¹é‡è½¬æ¢å®Œæˆï¼ŒæˆåŠŸè½¬æ¢ {len(results)} ä¸ªå®ä¾‹")
        return results
    
    def _validate_conversion_result(self, result: ConversionResult):
        """éªŒè¯è½¬æ¢ç»“æœ"""
        # åŸºæœ¬å®Œæ•´æ€§æ£€æŸ¥
        if result.standard_form is None:
            result.errors.append("æ ‡å‡†å½¢å¼å¯¹è±¡ä¸ºç©º")
            return
        
        if result.bipartite_graph is None:
            result.errors.append("äºŒåˆ†å›¾å¯¹è±¡ä¸ºç©º")
            return
        
        # ç»´åº¦ä¸€è‡´æ€§æ£€æŸ¥
        if result.standard_form.n_variables != len(result.bipartite_graph.variable_nodes):
            result.errors.append("å˜é‡èŠ‚ç‚¹æ•°é‡ä¸æ ‡å‡†å½¢å¼ä¸ä¸€è‡´")
        
        if result.standard_form.n_constraints != len(result.bipartite_graph.constraint_nodes):
            result.errors.append("çº¦æŸèŠ‚ç‚¹æ•°é‡ä¸æ ‡å‡†å½¢å¼ä¸ä¸€è‡´")
        
        # è¾¹æ•°é‡åˆç†æ€§æ£€æŸ¥
        expected_max_edges = result.standard_form.n_variables * result.standard_form.n_constraints
        actual_edges = len(result.bipartite_graph.edges)
        
        if actual_edges > expected_max_edges:
            result.errors.append(f"è¾¹æ•°é‡å¼‚å¸¸: {actual_edges} > {expected_max_edges}")
        
        # ç‰¹å¾å‘é‡æœ‰æ•ˆæ€§æ£€æŸ¥
        invalid_features = 0
        for var_node in result.bipartite_graph.variable_nodes.values():
            try:
                features = var_node.get_feature_vector()
                if len(features) != 9:
                    invalid_features += 1
                elif np.any(np.isnan(features)) or np.any(np.isinf(features)):
                    invalid_features += 1
            except Exception:
                invalid_features += 1
        
        if invalid_features > 0:
            result.warnings.append(f"{invalid_features} ä¸ªå˜é‡èŠ‚ç‚¹çš„ç‰¹å¾å‘é‡æ— æ•ˆ")
        
        logger.info(f"è½¬æ¢éªŒè¯å®Œæˆ: {len(result.errors)} ä¸ªé”™è¯¯, {len(result.warnings)} ä¸ªè­¦å‘Š")
    
    def _compute_problem_statistics(self, result: ConversionResult) -> Dict[str, Any]:
        """è®¡ç®—é—®é¢˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        if result.standard_form:
            stats['standard_form'] = {
                'n_variables': result.standard_form.n_variables,
                'n_constraints': result.standard_form.n_constraints,
                'objective_sense': result.standard_form.objective_sense,
                'nnz_objective': np.count_nonzero(result.standard_form.objective_coefficients),
                'variable_types': {
                    'continuous': sum(1 for vt in result.standard_form.variable_types if vt.value == 'continuous'),
                    'binary': sum(1 for vt in result.standard_form.variable_types if vt.value == 'binary'),
                    'integer': sum(1 for vt in result.standard_form.variable_types if vt.value == 'integer')
                }
            }
        
        if result.bipartite_graph:
            graph_stats = result.bipartite_graph.statistics
            stats['bipartite_graph'] = {
                'density': graph_stats.density,
                'avg_variable_degree': graph_stats.avg_variable_degree,
                'avg_constraint_degree': graph_stats.avg_constraint_degree,
                'max_variable_degree': graph_stats.max_variable_degree,
                'max_constraint_degree': graph_stats.max_constraint_degree,
                'coefficient_stats': graph_stats.coefficient_stats
            }
        
        return stats
    
    def _update_performance_stats(self, result: ConversionResult):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats['total_conversions'] += 1
        
        if len(result.errors) == 0:
            self.performance_stats['successful_conversions'] += 1
        else:
            self.performance_stats['failed_conversions'] += 1
        
        # æ›´æ–°å¹³å‡è€—æ—¶
        total_successful = self.performance_stats['successful_conversions']
        if total_successful > 0:
            current_avg = self.performance_stats['average_duration']
            new_avg = ((current_avg * (total_successful - 1)) + result.total_duration) / total_successful
            self.performance_stats['average_duration'] = new_avg
    
    def _save_standard_form(self, standard_form: MILPStandardForm, conversion_id: str):
        """ä¿å­˜MILPæ ‡å‡†å½¢å¼"""
        if self.output_dir is None:
            return
        
        filepath = self.output_dir / f"{conversion_id}_standard_form.pkl"
        
        try:
            with open(filepath, 'wb') as f:
                pickle.dump(standard_form, f)
            logger.debug(f"æ ‡å‡†å½¢å¼å·²ä¿å­˜: {filepath}")
        except Exception as e:
            logger.warning(f"ä¿å­˜æ ‡å‡†å½¢å¼å¤±è´¥: {e}")
    
    def _save_conversion_result(self, result: ConversionResult):
        """ä¿å­˜è½¬æ¢ç»“æœ"""
        if self.output_dir is None:
            return
        
        # ä¿å­˜äºŒåˆ†å›¾
        graph_filepath = self.output_dir / f"{result.conversion_id}_bipartite_graph.pkl"
        try:
            with open(graph_filepath, 'wb') as f:
                pickle.dump(result.bipartite_graph, f)
        except Exception as e:
            logger.warning(f"ä¿å­˜äºŒåˆ†å›¾å¤±è´¥: {e}")
        
        # ä¿å­˜è½¬æ¢æ‘˜è¦
        summary_filepath = self.output_dir / f"{result.conversion_id}_summary.json"
        try:
            with open(summary_filepath, 'w', encoding='utf-8') as f:
                json.dump(result.get_summary(), f, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            logger.warning(f"ä¿å­˜è½¬æ¢æ‘˜è¦å¤±è´¥: {e}")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            'performance_statistics': self.performance_stats.copy(),
            'conversion_history_count': len(self.conversion_history),
            'recent_conversions': [
                result.get_summary() for result in self.conversion_history[-10:]
            ]
        }
    
    def export_batch_results(self, 
                           results: List[ConversionResult],
                           export_path: str = None) -> str:
        """
        å¯¼å‡ºæ‰¹é‡è½¬æ¢ç»“æœ
        
        Args:
            results: è½¬æ¢ç»“æœåˆ—è¡¨
            export_path: å¯¼å‡ºè·¯å¾„
            
        Returns:
            å¯¼å‡ºæ–‡ä»¶è·¯å¾„
        """
        if export_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            export_path = f"g2milp_batch_results_{timestamp}.json"
        
        export_data = {
            'export_info': {
                'timestamp': datetime.now().isoformat(),
                'total_results': len(results),
                'successful_results': len([r for r in results if len(r.errors) == 0])
            },
            'results': [result.get_summary() for result in results],
            'performance_report': self.get_performance_report()
        }
        
        try:
            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"æ‰¹é‡ç»“æœå·²å¯¼å‡º: {export_path}")
            return export_path
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ‰¹é‡ç»“æœå¤±è´¥: {e}")
            return ""


def convert_cvxpy_to_bipartite_graph(problem: cp.Problem,
                                   problem_name: str = None,
                                   config: ConversionConfig = None) -> BipartiteGraph:
    """
    ä¾¿æ·å‡½æ•°ï¼šå°†CVXPYé—®é¢˜è½¬æ¢ä¸ºäºŒåˆ†å›¾
    
    Args:
        problem: CVXPYé—®é¢˜å¯¹è±¡
        problem_name: é—®é¢˜åç§°
        config: è½¬æ¢é…ç½®
        
    Returns:
        äºŒåˆ†å›¾å¯¹è±¡
    """
    converter = G2MILPConverter(config)
    result = converter.convert_problem(problem, problem_name)
    
    if result.errors:
        raise RuntimeError(f"è½¬æ¢å¤±è´¥: {result.errors}")
    
    return result.bipartite_graph


if __name__ == "__main__":
    """æµ‹è¯•G2MILPè½¬æ¢å™¨"""
    import sys
    from pathlib import Path
    
    # æ·»åŠ é¡¹ç›®è·¯å¾„
    project_root = Path(__file__).parent.parent.parent.parent
    sys.path.insert(0, str(project_root))
    
    # åˆ›å»ºæµ‹è¯•é—®é¢˜
    logger.info("åˆ›å»ºæµ‹è¯•CVXPYé—®é¢˜...")
    
    x = cp.Variable(3, name='x')
    y = cp.Variable(2, boolean=True, name='y')
    
    constraints = [
        x[0] + x[1] + x[2] <= 10,
        2*x[0] - x[1] == 5,
        x >= 0,
        y[0] + y[1] <= 1
    ]
    
    objective = cp.Minimize(3*x[0] + 2*x[1] + x[2] + 5*y[0] + 3*y[1])
    problem = cp.Problem(objective, constraints)
    
    try:
        # åˆ›å»ºè½¬æ¢é…ç½®
        config = ConversionConfig(
            use_sparse_matrix=True,
            normalize_coefficients=True,
            validate_graph=True,
            compute_statistics=True
        )
        
        # æµ‹è¯•å•ä¸ªé—®é¢˜è½¬æ¢
        converter = G2MILPConverter(config)
        result = converter.convert_problem(problem, "æµ‹è¯•é—®é¢˜")
        
        print("âœ… G2MILPè½¬æ¢æµ‹è¯•æˆåŠŸ!")
        print(f"è½¬æ¢ID: {result.conversion_id}")
        print(f"æ€»è€—æ—¶: {result.total_duration:.3f} ç§’")
        print("\n" + result.bipartite_graph.summary())
        
        # æµ‹è¯•æ‰¹é‡è½¬æ¢
        problems = [problem] * 3
        batch_results = converter.convert_batch(problems)
        
        print(f"\nâœ… æ‰¹é‡è½¬æ¢æµ‹è¯•æˆåŠŸ: {len(batch_results)} ä¸ªç»“æœ")
        
        # è·å–æ€§èƒ½æŠ¥å‘Š
        performance_report = converter.get_performance_report()
        print(f"\næ€§èƒ½æŠ¥å‘Š:")
        print(f"  æ€»è½¬æ¢æ¬¡æ•°: {performance_report['performance_statistics']['total_conversions']}")
        print(f"  æˆåŠŸæ¬¡æ•°: {performance_report['performance_statistics']['successful_conversions']}")
        print(f"  å¹³å‡è€—æ—¶: {performance_report['performance_statistics']['average_duration']:.3f} ç§’")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()