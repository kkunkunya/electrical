"""
Demo 4: G2MILP å®ä¾‹ç”Ÿæˆ (å¢å¼ºç‰ˆ)
Enhanced G2MILP Instance Generation Demo

æœ¬å¢å¼ºç‰ˆDemo 4é›†æˆäº†å…¨é¢çš„ä¼˜åŒ–æ”¹è¿›ï¼š
1. æŸå¤±å‡½æ•°é‡æ„ï¼šSmoothL1Loss + æ™ºèƒ½æƒé‡å¯¹é½ + ç¨€ç–æ€§æ­£åˆ™åŒ–
2. è®­ç»ƒç­–ç•¥ä¼˜åŒ–ï¼šå¤§å¹…å¢åŠ è®­ç»ƒå¼ºåº¦ + AdamWä¼˜åŒ–å™¨ + è¯¾ç¨‹å­¦ä¹ 
3. ç”Ÿæˆå¤šæ ·æ€§å¢å¼ºï¼šåŠ¨æ€æ¸©åº¦ + çƒé¢é‡‡æ · + çº¦æŸå¤šæ ·æ€§é€‰æ‹©
4. è¯„ä¼°ä½“ç³»å®Œå–„ï¼šå¤šç»´åº¦è´¨é‡è¯„ä¼° + å®æ—¶ç›‘æ§ + åŸºå‡†å¯¹æ¯”

ä¸»è¦æ”¹è¿›ï¼š
- è®­ç»ƒè½®æ•°ï¼š500 â†’ 5000 epochs (10å€æå‡)
- æ¯è½®è¿­ä»£ï¼š100 â†’ 200æ¬¡ (2å€æå‡)
- æ€»æ¢¯åº¦æ›´æ–°ï¼š50K â†’ 1Mæ¬¡ (20å€æå‡)
- å­¦ä¹ ç‡ï¼š1e-3 â†’ 1e-4 (æ›´ç¨³å®š)
- æŸå¤±å‡½æ•°ï¼šMSE â†’ SmoothL1Loss (æ›´é²æ£’)
- KLé€€ç«ï¼š200 â†’ 800 epochs (4å€å»¶é•¿)
- å¤šæ ·æ€§ç­–ç•¥ï¼šåŠ¨æ€Î·ã€åŠ¨æ€æ¸©åº¦ã€çº¦æŸå¤šæ ·æ€§
- è¯„ä¼°ç³»ç»Ÿï¼šå›¾ç›¸ä¼¼åº¦ã€MILPç‰¹å¾ã€å¤šæ ·æ€§åˆ†æã€åŸºå‡†å¯¹æ¯”

RTX 3060 Tiæ€§èƒ½ä¼˜åŒ–ï¼š
- âœ… AMPæ··åˆç²¾åº¦è®­ç»ƒ (èŠ‚çœæ˜¾å­˜+åŠ é€Ÿ)
- âœ… æ¢¯åº¦ç´¯ç§¯ (4x micro-batch, æé«˜GPUåˆ©ç”¨ç‡)  
- âœ… å¼‚æ­¥è´¨é‡è¯„ä¼° (é¿å…è®­ç»ƒé˜»å¡)
- âœ… æ™ºèƒ½è¿›åº¦ç›‘æ§ (tqdmåŒå±‚è¿›åº¦æ¡)
- âœ… å‡å°‘I/Oæ“ä½œ (ä¼˜åŒ–æ—¥å¿—é¢‘ç‡)
- âœ… æ—©æœŸè·³è¿‡è¯„ä¼° (å‰100 epochsä¸“æ³¨æ”¶æ•›)
"""

import sys
import os
from pathlib import Path
import logging
import json
import pickle
import time
from datetime import datetime
from typing import Dict, Any, Optional, List

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.models.g2milp import (
    G2MILPGenerator, G2MILPTrainer, G2MILPInference,
    GeneratorConfig, TrainingConfig, InferenceConfig,
    EncoderConfig, DecoderConfig, MaskingConfig
)
from src.models.g2milp.evaluation import G2MILPEvaluator, EvaluationConfig
from src.models.g2milp_bipartite import BipartiteGraphRepresentation

# è®¾ç½®æ—¥å¿—
def setup_logging(output_dir: Path = None) -> logging.Logger:
    """è®¾ç½®å¢å¼ºç‰ˆæ—¥å¿—é…ç½®"""
    if output_dir is None:
        output_dir = Path("output/demo4_g2milp_enhanced")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    log_file = output_dir / f"demo4_enhanced_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger("Demo4Enhanced")
    logger.info("="*60)
    logger.info("Demo 4: G2MILPå®ä¾‹ç”Ÿæˆ (å¢å¼ºç‰ˆ) - å¯åŠ¨")
    logger.info("="*60)
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    return logger


def create_enhanced_configs(quick_test: bool = False) -> Dict[str, Any]:
    """
    åˆ›å»ºå¢å¼ºç‰ˆé…ç½®
    
    Args:
        quick_test: æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæµ‹è¯•é…ç½®ï¼ˆå‡å°‘è®­ç»ƒæ—¶é—´ï¼‰
    """
    
    # ç¼–ç å™¨é…ç½®
    encoder_config = EncoderConfig(
        hidden_dim=128,
        latent_dim=64,
        num_layers=3,
        dropout=0.1,
        gnn_type="GraphConv"
    )
    
    # è§£ç å™¨é…ç½®  
    decoder_config = DecoderConfig(
        hidden_dim=128,
        latent_dim=64,
        num_layers=2,
        dropout=0.1,
        gnn_type="GraphConv"
    )
    
    # é®ç›–é…ç½®
    masking_config = MaskingConfig(
        masking_ratio=0.1,
        mask_strategy="random",
        min_constraint_degree=1
    )
    
    # æ•°å€¼ç¨³å®šç‰ˆç”Ÿæˆå™¨é…ç½®
    generator_config = GeneratorConfig(
        encoder_config=encoder_config,
        decoder_config=decoder_config,
        masking_config=masking_config,
        
        # é‡æ–°å¹³è¡¡çš„æŸå¤±æƒé‡ï¼ˆä¼˜åŒ–æ¢¯åº¦æ›´æ–°ï¼‰
        alpha_bias=0.2,        # æé«˜ï¼š0.05 â†’ 0.2
        alpha_degree=0.2,      # æé«˜ï¼š0.05 â†’ 0.2
        alpha_logits=0.5,      # å¤§å¹…æé«˜ï¼š0.1 â†’ 0.5ï¼ˆæ ¸å¿ƒç»„ä»¶ï¼‰
        alpha_weights=0.1,     # æé«˜ï¼š0.01 â†’ 0.1
        beta_kl=0.01,          # æé«˜ï¼š0.001 â†’ 0.01
        
        # æ¨ç†å‚æ•°ï¼ˆä¿å®ˆè®¾ç½®ï¼‰
        eta=0.05,              # é™ä½ï¼š0.1 â†’ 0.05
        sample_from_prior=True,
        temperature=0.5,       # é™ä½ï¼š1.0 â†’ 0.5ï¼Œå‡å°‘éšæœºæ€§
        
        # å¤šæ ·æ€§å¢å¼ºå‚æ•°ï¼ˆæš‚æ—¶ç®€åŒ–ï¼‰
        use_dynamic_temperature=False,  # å…³é—­ï¼šTrue â†’ False
        temperature_range=(0.5, 1.0),   # ç¼©å°èŒƒå›´ï¼š(0.3, 3.0) â†’ (0.5, 1.0)
        use_spherical_sampling=False,   # å…³é—­ï¼šTrue â†’ False
        noise_injection_strength=0.0,   # å…³é—­ï¼š0.15 â†’ 0.0
        use_dynamic_eta=False,          # å…³é—­ï¼šTrue â†’ False
        eta_range=(0.05, 0.1),          # ç¼©å°èŒƒå›´ï¼š(0.05, 0.4) â†’ (0.05, 0.1)
        diversity_boost_factor=1.0,     # é™ä½ï¼š1.5 â†’ 1.0
        use_constraint_diversity=False, # å…³é—­ï¼šTrue â†’ False
        
        # ç¨€ç–æ€§æ­£åˆ™åŒ–ï¼ˆå…³é—­ï¼‰
        use_sparsity_regularization=False,  # å…³é—­ï¼šTrue â†’ False
        sparsity_weight=0.0,               # å…³é—­ï¼š0.05 â†’ 0.0
        target_sparsity=0.1,
        
        # è¯¾ç¨‹å­¦ä¹ ï¼ˆç®€åŒ–ï¼‰
        use_curriculum_learning=True,      # ä¿æŒå¼€å¯ï¼Œæœ‰åŠ©ç¨³å®šæ€§
        curriculum_kl_warmup_epochs=50,    # ç¼©çŸ­ï¼š200 â†’ 50
        curriculum_kl_annealing_epochs=100 # ç¼©çŸ­ï¼š600 â†’ 100
    )
    
    # æ•°å€¼ç¨³å®šæ€§ä¼˜åŒ–è®­ç»ƒé…ç½®
    if quick_test:
        # å¿«é€Ÿæµ‹è¯•é…ç½®ï¼ˆæ•°å€¼ç¨³å®šæ€§ä¼˜å…ˆï¼‰
        num_epochs = 50  # è¿›ä¸€æ­¥å‡å°‘ï¼Œä¸“æ³¨ç¨³å®šæ€§éªŒè¯
        iterations_per_epoch = 20
        quality_eval_freq = 25
        print("ğŸš€ ä½¿ç”¨æ•°å€¼ç¨³å®šæµ‹è¯•é…ç½® (50 epochs, 1K iterations)")
    else:
        # ç¨³å®šè®­ç»ƒé…ç½®ï¼ˆä¿å®ˆå‚æ•°ï¼‰
        num_epochs = 500  # å¤§å¹…å‡å°‘ï¼š5000 â†’ 500ï¼Œç¡®ä¿ç¨³å®šæ€§
        iterations_per_epoch = 50  # å‡å°‘ï¼š200 â†’ 50ï¼Œé™ä½ç´¯ç§¯è¯¯å·®
        quality_eval_freq = 50
        print("ğŸ¯ ä½¿ç”¨ç¨³å®šè®­ç»ƒé…ç½® (500 epochs, 25K iterations)")
    
    training_config = TrainingConfig(
        # æ•°å€¼ç¨³å®šæ€§ä¼˜å…ˆçš„è®­ç»ƒå‚æ•°
        num_epochs=num_epochs,              
        iterations_per_epoch=iterations_per_epoch,     
        learning_rate=1e-5,           # ä¼˜åŒ–æå‡ï¼š1e-6 â†’ 1e-5 (åˆç†èŒƒå›´)
        weight_decay=1e-4,            # é€‚åº¦æå‡ï¼š1e-5 â†’ 1e-4
        
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆæ›´ä¿å®ˆï¼‰
        use_lr_scheduler=True,
        scheduler_type="cosine_with_warmup",
        warmup_epochs=20,  # å‡å°‘é¢„çƒ­æœŸ
        
        # å¹³è¡¡çš„æ¢¯åº¦è£å‰ªï¼ˆå…è®¸é€‚åº¦æ¢¯åº¦ï¼‰
        grad_clip_norm=0.5,   # æ”¾å®½ï¼š0.01 â†’ 0.5ï¼Œå…è®¸æ›´å¤šæœ‰æ•ˆæ¢¯åº¦
        
        # æ—©åœç­–ç•¥ï¼ˆæ•°å€¼ç¨³å®šæ€§ä¼˜å…ˆï¼‰
        use_early_stopping=True,
        early_stopping_patience=50,  # å¤§å¹…å‡å°‘ï¼š500 â†’ 50ï¼Œå¿«é€Ÿè¯†åˆ«é—®é¢˜
        early_stopping_min_delta=1e-4,  # é™ä½æ•æ„Ÿåº¦ï¼š1e-6 â†’ 1e-4
        
        # éªŒè¯å’Œä¿å­˜ï¼ˆæ›´é¢‘ç¹ç›‘æ§ï¼‰
        validation_frequency=10,      # å¢åŠ éªŒè¯é¢‘ç‡ï¼š50 â†’ 10
        save_frequency=25,            # å¢åŠ ä¿å­˜é¢‘ç‡ï¼š500 â†’ 25
        
        # KLé€€ç«ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        kl_annealing=True,
        kl_annealing_epochs=100,      # å¤§å¹…å‡å°‘ï¼š800 â†’ 100ï¼Œå¿«é€Ÿç¨³å®š
        
        # æ•°æ®å¢å¼ºï¼ˆæš‚æ—¶å…³é—­ï¼Œå‡å°‘å¤æ‚æ€§ï¼‰
        use_data_augmentation=False,  # True â†’ Falseï¼Œä¸“æ³¨ç¨³å®šæ€§
        feature_noise_std=0.01,       # é™ä½å™ªå£°ï¼š0.05 â†’ 0.01
        edge_perturbation_prob=0.0,   # å…³é—­è¾¹æ‰°åŠ¨ï¼š0.1 â†’ 0.0
        
        # RTX 3060 Tiä¿å®ˆä¼˜åŒ–
        use_mixed_precision=True,         # ä¿æŒAMP
        amp_loss_scale="dynamic",        # åŠ¨æ€æŸå¤±ç¼©æ”¾
        use_compile=False,               # å…³é—­ç¼–è¯‘ä¼˜åŒ–
        
        # å¾®æ‰¹æ¬¡ç´¯ç§¯ï¼ˆå‡å°‘ç´¯ç§¯æ­¥æ•°ï¼‰
        micro_batch_size=2,              # å‡å°‘ï¼š4 â†’ 2
        gradient_accumulation_steps=2,   # å‡å°‘ï¼š4 â†’ 2ï¼Œé™ä½ç´¯ç§¯è¯¯å·®
        
        # ä¼˜åŒ–å™¨ï¼ˆä¿å®ˆè®¾ç½®ï¼‰
        optimizer_type="adamw",       # ä¿æŒAdamW
        
        # ç¨€ç–æ€§æ­£åˆ™åŒ–ï¼ˆé™ä½æƒé‡ï¼‰
        use_sparsity_regularization=False,  # æš‚æ—¶å…³é—­ï¼šTrue â†’ False
        sparsity_weight=0.01,              # é™ä½æƒé‡ï¼š0.05 â†’ 0.01
        target_sparsity=0.1,
        
        # åœ¨çº¿è´¨é‡è¯„ä¼°é…ç½®ï¼ˆæœ€å°åŒ–å½±å“ï¼‰
        enable_quality_evaluation=False,   # æš‚æ—¶å…³é—­ï¼šTrue â†’ False
        quality_evaluation_frequency=100,  # é™ä½é¢‘ç‡
        quality_samples_per_eval=1,        # å‡å°‘æ ·æœ¬ï¼š2 â†’ 1
        enable_detailed_quality_logging=False  # ä¿æŒå…³é—­
    )
    
    # æ¨ç†é…ç½®ï¼ˆä¸æµ‹è¯•ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
    inference_config = InferenceConfig(
        eta=0.1,
        num_test_instances=3,         # ä¸æµ‹è¯•ç‰ˆæœ¬ä¸€è‡´ï¼šç”Ÿæˆ3ä¸ªæµ‹è¯•å®ä¾‹
        temperature=1.0,
        sample_from_prior=True,
        constraint_selection_strategy="random",
        diversity_boost=True,
        num_diverse_samples=5,        # ä¸æµ‹è¯•ç‰ˆæœ¬ä¸€è‡´ï¼š5ä¸ªå¤šæ ·æ€§æ ·æœ¬
        compute_similarity_metrics=True,
        generate_comparison_report=True,
        experiment_name=f"enhanced_inference_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    )
    
    # è¯„ä¼°é…ç½®
    evaluation_config = EvaluationConfig(
        enable_graph_similarity=True,
        enable_milp_similarity=True,
        enable_diversity_analysis=True,
        enable_training_monitoring=True,
        diversity_sample_size=5,
        generate_visualizations=True,
        save_detailed_results=True
    )
    
    return {
        'generator': generator_config,
        'training': training_config,
        'inference': inference_config,
        'evaluation': evaluation_config
    }


def _convert_demo3_to_demo4_format(bipartite_graph, logger):
    """å°†Demo 3çš„BipartiteGraphè½¬æ¢ä¸ºDemo 4æ ¼å¼"""
    try:
        import torch
        import numpy as np
        from torch_geometric.data import HeteroData
        
        # è·å–è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æå–åŸºæœ¬ä¿¡æ¯
        constraint_nodes = bipartite_graph.constraint_nodes
        variable_nodes = bipartite_graph.variable_nodes
        edges = bipartite_graph.edges
        
        logger.info(f"åŸå§‹æ•°æ®ç»Ÿè®¡:")
        logger.info(f"  - çº¦æŸèŠ‚ç‚¹: {len(constraint_nodes)}")
        logger.info(f"  - å˜é‡èŠ‚ç‚¹: {len(variable_nodes)}")
        logger.info(f"  - è¾¹è¿æ¥: {len(edges)}")
        
        # åˆ›å»ºçº¦æŸèŠ‚ç‚¹ç‰¹å¾ (16ç»´)
        num_constraints = len(constraint_nodes)
        constraint_features = np.zeros((num_constraints, 16), dtype=np.float32)
        
        for i, node in enumerate(constraint_nodes):
            if hasattr(node, 'features') and node.features is not None:
                node_features = np.array(node.features, dtype=np.float32)
                if len(node_features) >= 16:
                    constraint_features[i] = node_features[:16]
                else:
                    constraint_features[i, :len(node_features)] = node_features
            else:
                # åŸºæœ¬ç‰¹å¾æ„å»º
                feature_idx = 0
                # çº¦æŸç±»å‹ç‰¹å¾ (0-2)
                if hasattr(node, 'constraint_type'):
                    if str(node.constraint_type).lower() == 'equality':
                        constraint_features[i, 0] = 1.0
                    else:
                        constraint_features[i, 1] = 1.0
                feature_idx = 3
                
                # å³ç«¯é¡¹å€¼ (3)
                if hasattr(node, 'rhs') and node.rhs is not None:
                    constraint_features[i, 3] = float(node.rhs)
                
                # çº¦æŸåº¦æ•° (4)
                if hasattr(node, 'degree'):
                    constraint_features[i, 4] = float(node.degree)
                
                # å¡«å……å‰©ä½™ç‰¹å¾ä¸ºå°çš„éšæœºå€¼
                constraint_features[i, 5:] = np.random.normal(0, 0.01, 11)
        
        # åˆ›å»ºå˜é‡èŠ‚ç‚¹ç‰¹å¾ (9ç»´)
        num_variables = len(variable_nodes)
        variable_features = np.zeros((num_variables, 9), dtype=np.float32)
        
        for i, node in enumerate(variable_nodes):
            if hasattr(node, 'features') and node.features is not None:
                node_features = np.array(node.features, dtype=np.float32)
                if len(node_features) >= 9:
                    variable_features[i] = node_features[:9]
                else:
                    variable_features[i, :len(node_features)] = node_features
            else:
                # åŸºæœ¬ç‰¹å¾æ„å»º
                # å˜é‡ç±»å‹ (0-2)
                if hasattr(node, 'variable_type'):
                    if str(node.variable_type).lower() == 'continuous':
                        variable_features[i, 0] = 1.0
                    elif str(node.variable_type).lower() == 'binary':
                        variable_features[i, 2] = 1.0
                    else:
                        variable_features[i, 1] = 1.0
                
                # ç›®æ ‡å‡½æ•°ç³»æ•° (3)
                if hasattr(node, 'objective_coeff'):
                    variable_features[i, 3] = float(node.objective_coeff)
                
                # å˜é‡è¾¹ç•Œ (4-5)
                if hasattr(node, 'lower_bound'):
                    variable_features[i, 4] = float(node.lower_bound) if node.lower_bound is not None else -1e6
                if hasattr(node, 'upper_bound'):
                    variable_features[i, 5] = float(node.upper_bound) if node.upper_bound is not None else 1e6
                
                # å˜é‡åº¦æ•° (6)
                if hasattr(node, 'degree'):
                    variable_features[i, 6] = float(node.degree)
                
                # å¡«å……å‰©ä½™ç‰¹å¾
                variable_features[i, 7:] = np.random.normal(0, 0.01, 2)
        
        # å¤„ç†è¾¹è¿æ¥å’Œç‰¹å¾
        num_edges = len(edges)
        edge_indices = np.zeros((2, num_edges), dtype=np.int64)
        edge_features = np.zeros((num_edges, 8), dtype=np.float32)
        
        for i, edge in enumerate(edges):
            # è¾¹è¿æ¥
            if hasattr(edge, 'constraint_idx') and hasattr(edge, 'variable_idx'):
                edge_indices[0, i] = edge.constraint_idx
                edge_indices[1, i] = edge.variable_idx
            elif hasattr(edge, 'source') and hasattr(edge, 'target'):
                edge_indices[0, i] = edge.source
                edge_indices[1, i] = edge.target
            elif isinstance(edge, (tuple, list)) and len(edge) >= 2:
                edge_indices[0, i] = edge[0]
                edge_indices[1, i] = edge[1]
            
            # è¾¹ç‰¹å¾
            if hasattr(edge, 'features') and edge.features is not None:
                edge_feat = np.array(edge.features, dtype=np.float32)
                if len(edge_feat) >= 8:
                    edge_features[i] = edge_feat[:8]
                else:
                    edge_features[i, :len(edge_feat)] = edge_feat
            else:
                # åŸºæœ¬è¾¹ç‰¹å¾
                if hasattr(edge, 'coefficient'):
                    edge_features[i, 0] = float(edge.coefficient)
                elif hasattr(edge, 'weight'):
                    edge_features[i, 0] = float(edge.weight)
                
                # å¡«å……å‰©ä½™ç‰¹å¾
                edge_features[i, 1:] = np.random.normal(0, 0.01, 7)
        
        # ç‰¹å¾å½’ä¸€åŒ–å’Œæ•°å€¼ç¨³å®šæ€§å¤„ç†
        logger.info("æ‰§è¡Œç‰¹å¾å½’ä¸€åŒ–å’Œæ•°å€¼ç¨³å®šæ€§å¤„ç†...")
        
        # çº¦æŸç‰¹å¾å½’ä¸€åŒ–
        constraint_features = np.nan_to_num(constraint_features, nan=0.0, posinf=1.0, neginf=-1.0)
        constraint_std = np.std(constraint_features, axis=0) + 1e-8
        constraint_features = constraint_features / constraint_std
        constraint_features = np.clip(constraint_features, -5.0, 5.0)  # é˜²æ­¢æç«¯å€¼
        
        # å˜é‡ç‰¹å¾å½’ä¸€åŒ–  
        variable_features = np.nan_to_num(variable_features, nan=0.0, posinf=1.0, neginf=-1.0)
        variable_std = np.std(variable_features, axis=0) + 1e-8
        variable_features = variable_features / variable_std
        variable_features = np.clip(variable_features, -5.0, 5.0)
        
        # è¾¹ç‰¹å¾å½’ä¸€åŒ–
        edge_features = np.nan_to_num(edge_features, nan=0.0, posinf=1.0, neginf=-1.0)
        edge_std = np.std(edge_features, axis=0) + 1e-8
        edge_features = edge_features / edge_std
        edge_features = np.clip(edge_features, -5.0, 5.0)
        
        logger.info(f"ç‰¹å¾å½’ä¸€åŒ–å®Œæˆ:")
        logger.info(f"  - çº¦æŸç‰¹å¾èŒƒå›´: [{constraint_features.min():.3f}, {constraint_features.max():.3f}]")
        logger.info(f"  - å˜é‡ç‰¹å¾èŒƒå›´: [{variable_features.min():.3f}, {variable_features.max():.3f}]") 
        logger.info(f"  - è¾¹ç‰¹å¾èŒƒå›´: [{edge_features.min():.3f}, {edge_features.max():.3f}]")
        
        # åˆ›å»ºPyTorch Geometricå¼‚æ„å›¾
        data = HeteroData()
        
        # èŠ‚ç‚¹ç‰¹å¾ (ä¸è®¾ç½®requires_gradï¼Œè®©æ¨¡å‹è‡ªå·±å¤„ç†)
        data['constraint'].x = torch.tensor(
            constraint_features, 
            dtype=torch.float32, 
            device=device
        )
        
        data['variable'].x = torch.tensor(
            variable_features, 
            dtype=torch.float32, 
            device=device
        )
        
        # è¾¹è¿æ¥å’Œç‰¹å¾
        data['constraint', 'connects', 'variable'].edge_index = torch.tensor(
            edge_indices, 
            dtype=torch.long, 
            device=device
        )
        
        data['constraint', 'connects', 'variable'].edge_attr = torch.tensor(
            edge_features, 
            dtype=torch.float32, 
            device=device
        )
        
        # æ·»åŠ åå‘è¾¹è¿æ¥ï¼ˆG2MILPæ¨¡å‹éœ€è¦ï¼‰
        reverse_edge_indices = torch.stack([
            torch.tensor(edge_indices[1], dtype=torch.long, device=device),
            torch.tensor(edge_indices[0], dtype=torch.long, device=device)
        ], dim=0)
        
        data['variable', 'connected_by', 'constraint'].edge_index = reverse_edge_indices
        data['variable', 'connected_by', 'constraint'].edge_attr = torch.tensor(
            edge_features, 
            dtype=torch.float32, 
            device=device
        )
        
        logger.info("âœ… Demo 3æ ¼å¼è½¬æ¢å®Œæˆ")
        logger.info(f"  - çº¦æŸèŠ‚ç‚¹ç‰¹å¾: {data['constraint'].x.size()}")
        logger.info(f"  - å˜é‡èŠ‚ç‚¹ç‰¹å¾: {data['variable'].x.size()}")
        logger.info(f"  - å‰å‘è¾¹: {data['constraint', 'connects', 'variable'].edge_index.size(1)}")
        logger.info(f"  - åå‘è¾¹: {data['variable', 'connected_by', 'constraint'].edge_index.size(1)}")
        logger.info(f"  - è®¾å¤‡: {device}")
        
        # è¿”å›ç»“æœ
        return {
            'bipartite_data': data,
            'metadata': {
                'source': 'demo3_bipartite_graph',
                'conversion_timestamp': datetime.now().isoformat(),
                'num_constraints': num_constraints,
                'num_variables': num_variables,
                'num_edges': num_edges,
                'device': str(device),
            },
            'extraction_summary': {
                'conversion_method': 'demo3_to_demo4_inline',
                'requires_grad': False,  # ä¿®æ­£ï¼šè®©æ¨¡å‹è‡ªå·±å¤„ç†æ¢¯åº¦
                'bidirectional_edges': True,
                'timestamp': datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None


def _validate_hetero_data_structure(hetero_data, logger) -> bool:
    """éªŒè¯HeteroDataæ•°æ®ç»“æ„çš„å®Œæ•´æ€§"""
    try:
        # ğŸ” æ£€æŸ¥å¿…éœ€çš„èŠ‚ç‚¹ç±»å‹
        required_node_types = ['constraint', 'variable']
        for node_type in required_node_types:
            if node_type not in hetero_data:
                logger.error(f"âŒ ç¼ºå°‘å¿…éœ€çš„èŠ‚ç‚¹ç±»å‹: {node_type}")
                return False
            
            if 'x' not in hetero_data[node_type]:
                logger.error(f"âŒ èŠ‚ç‚¹ç±»å‹ {node_type} ç¼ºå°‘ç‰¹å¾çŸ©é˜µ 'x'")
                return False
            
            # æ£€æŸ¥ç‰¹å¾çŸ©é˜µçš„å½¢çŠ¶
            features = hetero_data[node_type].x
            if features.dim() != 2:
                logger.error(f"âŒ èŠ‚ç‚¹ {node_type} ç‰¹å¾çŸ©é˜µç»´åº¦é”™è¯¯: {features.dim()}")
                return False
            
            logger.info(f"âœ… èŠ‚ç‚¹ {node_type}: {features.shape}")
        
        # ğŸ” æ£€æŸ¥å¿…éœ€çš„è¾¹ç±»å‹
        required_edge_types = [
            ('constraint', 'connects', 'variable'),
            ('variable', 'connected_by', 'constraint')
        ]
        
        for edge_type in required_edge_types:
            if edge_type not in hetero_data.edge_index_dict:
                logger.error(f"âŒ ç¼ºå°‘å¿…éœ€çš„è¾¹ç±»å‹: {edge_type}")
                return False
            
            edge_index = hetero_data[edge_type].edge_index
            if edge_index.dim() != 2 or edge_index.size(0) != 2:
                logger.error(f"âŒ è¾¹ {edge_type} ç´¢å¼•å½¢çŠ¶é”™è¯¯: {edge_index.shape}")
                return False
            
            logger.info(f"âœ… è¾¹ {edge_type}: {edge_index.size(1)} æ¡è¾¹")
        
        # ğŸ” æ£€æŸ¥ç‰¹å¾ç»´åº¦æ˜¯å¦ç¬¦åˆG2MILPæ ‡å‡†
        constraint_dim = hetero_data['constraint'].x.size(1)
        variable_dim = hetero_data['variable'].x.size(1)
        
        if constraint_dim != 16:
            logger.warning(f"âš ï¸ çº¦æŸç‰¹å¾ç»´åº¦éæ ‡å‡†: {constraint_dim} (æœŸæœ›16)")
        if variable_dim != 9:
            logger.warning(f"âš ï¸ å˜é‡ç‰¹å¾ç»´åº¦éæ ‡å‡†: {variable_dim} (æœŸæœ›9)")
        
        # ğŸ” æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
        for node_type in ['constraint', 'variable']:
            features = hetero_data[node_type].x
            if torch.isnan(features).any():
                logger.error(f"âŒ èŠ‚ç‚¹ {node_type} ç‰¹å¾åŒ…å«NaNå€¼")
                return False
            if torch.isinf(features).any():
                logger.error(f"âŒ èŠ‚ç‚¹ {node_type} ç‰¹å¾åŒ…å«Infå€¼")
                return False
        
        logger.info("âœ… HeteroDataç»“æ„éªŒè¯å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®ç»“æ„éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False


def load_bipartite_data(data_path: str) -> Optional[Dict[str, Any]]:
    """åŠ è½½Demo 3ç”Ÿæˆçš„äºŒåˆ†å›¾æ•°æ®ï¼ˆå¢å¼ºç‰ˆ - å¸¦è¯¦ç»†éªŒè¯ï¼‰"""
    try:
        logger = logging.getLogger("Demo4Enhanced")
        
        # ğŸ” æ­¥éª¤1: æ–‡ä»¶ç³»ç»ŸéªŒè¯
        logger.info("ğŸ” å¼€å§‹æ•°æ®åŠ è½½éªŒè¯æµç¨‹...")
        bipartite_file = Path(data_path)
        
        if not bipartite_file.exists():
            logger.error(f"âŒ äºŒåˆ†å›¾æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {bipartite_file}")
            logger.info("ğŸ’¡ è¯·å…ˆè¿è¡ŒDemo 3ç”ŸæˆäºŒåˆ†å›¾æ•°æ®")
            return None
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = bipartite_file.stat().st_size
        logger.info(f"âœ… æ–‡ä»¶å­˜åœ¨æ£€æŸ¥é€šè¿‡")
        logger.info(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {bipartite_file}")
        logger.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.2f} MB")
        
        if file_size == 0:
            logger.error(f"âŒ æ•°æ®æ–‡ä»¶ä¸ºç©º")
            return None
        elif file_size < 1024:  # å°äº1KBå¯èƒ½æœ‰é—®é¢˜
            logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶è¿‡å° ({file_size} bytes)ï¼Œå¯èƒ½ä¸å®Œæ•´")
        
        # ğŸ” æ­¥éª¤2: æ•°æ®åŠ è½½éªŒè¯
        logger.info("ğŸ”„ å¼€å§‹åŠ è½½æ•°æ®æ–‡ä»¶...")
        try:
            with open(bipartite_file, 'rb') as f:
                bipartite_graph = pickle.load(f)
            logger.info("âœ… æ•°æ®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        except pickle.UnpicklingError as e:
            logger.error(f"âŒ æ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            return None
        
        # ğŸ” æ­¥éª¤3: æ•°æ®ç±»å‹éªŒè¯
        logger.info(f"ğŸ“‹ æ•°æ®ç±»å‹: {type(bipartite_graph)}")
        
        if bipartite_graph is None:
            logger.error("âŒ åŠ è½½çš„æ•°æ®ä¸ºNone")
            return None
        
        # ğŸ” æ­¥éª¤4: æ•°æ®ç»“æ„éªŒè¯
        if isinstance(bipartite_graph, dict) and 'bipartite_data' in bipartite_graph:
            # ç›´æ¥æ˜¯æœŸæœ›çš„å­—å…¸æ ¼å¼ï¼ˆæµ‹è¯•æ•°æ®ï¼‰
            logger.info("ğŸ“¦ æ£€æµ‹åˆ°é¢„åŒ…è£…çš„äºŒåˆ†å›¾æ•°æ®æ ¼å¼")
            bipartite_data = bipartite_graph['bipartite_data']
            
            # è¯¦ç»†éªŒè¯æ•°æ®å®Œæ•´æ€§
            if not _validate_hetero_data_structure(bipartite_data, logger):
                return None
                
            logger.info("âœ… é¢„åŒ…è£…æ•°æ®éªŒè¯é€šè¿‡")
            return bipartite_graph
            
        elif hasattr(bipartite_graph, 'to_pytorch_geometric'):
            # BipartiteGraphRepresentationå¯¹è±¡
            bipartite_data = bipartite_graph.to_pytorch_geometric()
            logger.info(f"äºŒåˆ†å›¾æ•°æ®è½¬æ¢æˆåŠŸ:")
            logger.info(f"  - çº¦æŸèŠ‚ç‚¹: {bipartite_data['constraint'].x.size(0)}")
            logger.info(f"  - å˜é‡èŠ‚ç‚¹: {bipartite_data['variable'].x.size(0)}")
            
            edge_counts = {}
            for edge_type, edge_index in bipartite_data.edge_index_dict.items():
                edge_counts[str(edge_type)] = edge_index.size(1)
            logger.info(f"  - è¾¹ç»Ÿè®¡: {edge_counts}")
            
            # åŒ…è£…æˆæœŸæœ›çš„æ ¼å¼
            return {
                'bipartite_data': bipartite_data,
                'metadata': {
                    'source': 'demo3_bipartite_graph',
                    'num_constraints': bipartite_data['constraint'].x.size(0),
                    'num_variables': bipartite_data['variable'].x.size(0),
                    'num_edges': sum(edge_counts.values())
                },
                'extraction_summary': {
                    'conversion_method': 'bipartite_graph',
                    'timestamp': datetime.now().isoformat()
                }
            }
        elif hasattr(bipartite_graph, 'variable_nodes') and hasattr(bipartite_graph, 'constraint_nodes'):
            # Demo 3çš„BipartiteGraphå¯¹è±¡ - éœ€è¦è½¬æ¢
            logger.info(f"æ£€æµ‹åˆ°Demo 3çš„BipartiteGraphæ ¼å¼ï¼Œè¿›è¡Œè½¬æ¢...")
            
            # ç›´æ¥åœ¨è¿™é‡Œè¿›è¡Œè½¬æ¢ï¼Œä¸ä¾èµ–å¤–éƒ¨è½¬æ¢å™¨
            converted_data = _convert_demo3_to_demo4_format(bipartite_graph, logger)
            
            if converted_data is None:
                logger.error("Demo 3æ ¼å¼è½¬æ¢å¤±è´¥")
                return None
            
            logger.info(f"Demo 3æ ¼å¼è½¬æ¢æˆåŠŸ:")
            bipartite_data = converted_data['bipartite_data']
            logger.info(f"  - çº¦æŸèŠ‚ç‚¹: {bipartite_data['constraint'].x.size(0)}")
            logger.info(f"  - å˜é‡èŠ‚ç‚¹: {bipartite_data['variable'].x.size(0)}")
            
            # è®¡ç®—è¾¹æ•°
            edge_count = 0
            for edge_type in bipartite_data.edge_types:
                if hasattr(bipartite_data[edge_type], 'edge_index'):
                    edge_count += bipartite_data[edge_type].edge_index.size(1)
            logger.info(f"  - è¾¹æ•°: {edge_count}")
            
            return converted_data
        else:
            logger.error(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {type(bipartite_graph)}")
            logger.error(f"å¯¹è±¡å±æ€§: {dir(bipartite_graph)[:10]}...")
            return None
        
    except Exception as e:
        logger = logging.getLogger("Demo4Enhanced")
        logger.error(f"åŠ è½½äºŒåˆ†å›¾æ•°æ®å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None


def create_enhanced_model(configs: Dict[str, Any]) -> G2MILPGenerator:
    """åˆ›å»ºå¢å¼ºç‰ˆG2MILPæ¨¡å‹"""
    logger = logging.getLogger("Demo4Enhanced")
    
    logger.info("åˆ›å»ºå¢å¼ºç‰ˆG2MILPç”Ÿæˆå™¨...")
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = G2MILPGenerator(
        constraint_feature_dim=16,
        variable_feature_dim=9,
        edge_feature_dim=8,
        config=configs['generator']
    )
    
    # æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in generator.parameters())
    trainable_params = sum(p.numel() for p in generator.parameters() if p.requires_grad)
    model_size_mb = sum(p.numel() * p.element_size() for p in generator.parameters()) / 1024 / 1024
    
    logger.info("å¢å¼ºç‰ˆG2MILPç”Ÿæˆå™¨åˆ›å»ºå®Œæˆ:")
    logger.info(f"  - æ€»å‚æ•°æ•°é‡: {total_params:,}")
    logger.info(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    logger.info(f"  - æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB")
    logger.info(f"  - è®¾å¤‡: {configs['generator'].device}")
    
    # æ‰“å°å…³é”®é…ç½®
    logger.info("å…³é”®é…ç½®:")
    logger.info(f"  - è¿æ¥é¢„æµ‹æƒé‡: {configs['generator'].alpha_logits}")
    logger.info(f"  - ç¨€ç–æ€§æƒé‡: {configs['generator'].sparsity_weight}")
    logger.info(f"  - è¯¾ç¨‹å­¦ä¹ : {configs['generator'].use_curriculum_learning}")
    logger.info(f"  - åŠ¨æ€æ¸©åº¦: {configs['generator'].use_dynamic_temperature}")
    logger.info(f"  - çº¦æŸå¤šæ ·æ€§: {configs['generator'].use_constraint_diversity}")
    
    return generator


def enhanced_training(generator: G2MILPGenerator,
                     training_data: Dict[str, Any],
                     configs: Dict[str, Any]) -> Dict[str, Any]:
    """å¢å¼ºç‰ˆè®­ç»ƒè¿‡ç¨‹"""
    logger = logging.getLogger("Demo4Enhanced")
    
    logger.info("å¼€å§‹å¢å¼ºç‰ˆè®­ç»ƒè¿‡ç¨‹...")
    logger.info(f"è®­ç»ƒé…ç½®:")
    logger.info(f"  - è®­ç»ƒè½®æ•°: {configs['training'].num_epochs}")
    logger.info(f"  - æ¯è½®è¿­ä»£: {configs['training'].iterations_per_epoch}")
    logger.info(f"  - æ€»æ¢¯åº¦æ›´æ–°: {configs['training'].num_epochs * configs['training'].iterations_per_epoch:,}")
    logger.info(f"  - åˆå§‹å­¦ä¹ ç‡: {configs['training'].learning_rate}")
    logger.info(f"  - ä¼˜åŒ–å™¨: {configs['training'].optimizer_type}")
    logger.info(f"  - æ•°æ®å¢å¼º: {configs['training'].use_data_augmentation}")
    
    # åˆ›å»ºåœ¨çº¿è´¨é‡è¯„ä¼°å™¨
    logger.info("åˆ›å»ºåœ¨çº¿è´¨é‡è¯„ä¼°å™¨...")
    evaluator = G2MILPEvaluator(configs['evaluation'])
    logger.info(f"  - è´¨é‡è¯„ä¼°é¢‘ç‡: æ¯{configs['training'].quality_evaluation_frequency}ä¸ªepoch")
    logger.info(f"  - æ¯æ¬¡è¯„ä¼°æ ·æœ¬æ•°: {configs['training'].quality_samples_per_eval}")
    
    # åˆ›å»ºå¢å¼ºç‰ˆè®­ç»ƒå™¨ï¼ˆåŒ…å«è¯„ä¼°å™¨ï¼‰
    trainer = G2MILPTrainer(generator, configs['training'], evaluator)
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    
    try:
        training_results = trainer.train(training_data['bipartite_data'])
        training_time = time.time() - start_time
        
        logger.info("å¢å¼ºç‰ˆè®­ç»ƒå®Œæˆ:")
        logger.info(f"  - è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’ ({training_time/60:.2f} åˆ†é’Ÿ)")
        logger.info(f"  - æœ€ç»ˆè®­ç»ƒæŸå¤±: {training_results['training_summary']['best_validation_loss']:.6f}")
        logger.info(f"  - å®é™…è®­ç»ƒè½®æ•°: {training_results['training_summary']['total_epochs']}")
        logger.info(f"  - æ€»æ¢¯åº¦æ›´æ–°: {training_results['training_summary']['total_iterations']:,}")
        
        # å±•ç¤ºåˆ†è§£æŸå¤±å†å²åˆ†æï¼ˆæ–°å¢ï¼‰
        training_history = training_results.get('training_history', {})
        if 'train_reconstruction' in training_history and len(training_history['train_reconstruction']) > 0:
            final_recon = training_history['train_reconstruction'][-1]
            final_kl_raw = training_history['train_kl_raw'][-1] if 'train_kl_raw' in training_history else 0
            final_bias = training_history['train_bias'][-1] if 'train_bias' in training_history else 0
            final_logits = training_history['train_logits'][-1] if 'train_logits' in training_history else 0
            
            logger.info("ğŸ“Š æœ€ç»ˆæŸå¤±åˆ†è§£åˆ†æ:")
            logger.info(f"  - é‡å»ºæŸå¤±: {final_recon:.6f}")
            logger.info(f"  - KLæ•£åº¦(åŸå§‹): {final_kl_raw:.6f}")
            logger.info(f"  - åç½®æŸå¤±: {final_bias:.6f}")
            logger.info(f"  - è¿æ¥æŸå¤±: {final_logits:.6f}")
            
            # æŸå¤±å˜åŒ–åˆ†æ
            if len(training_history['train_reconstruction']) > 1:
                initial_recon = training_history['train_reconstruction'][0]
                recon_change = ((final_recon - initial_recon) / initial_recon * 100) if initial_recon > 0 else 0
                logger.info(f"  - é‡å»ºæŸå¤±å˜åŒ–: {recon_change:+.2f}%")
        
        # å±•ç¤ºè´¨é‡è¯„ä¼°å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'generation_quality' in training_history and len(training_history['generation_quality']) > 0:
            quality_scores = training_history['generation_quality']
            avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
            max_quality = max(quality_scores) if quality_scores else 0
            
            logger.info("ğŸ¯ ç”Ÿæˆè´¨é‡è¯„ä¼°å†å²:")
            logger.info(f"  - å¹³å‡è´¨é‡å¾—åˆ†: {avg_quality:.4f}")
            logger.info(f"  - æœ€é«˜è´¨é‡å¾—åˆ†: {max_quality:.4f}")
            logger.info(f"  - è´¨é‡è¯„ä¼°æ¬¡æ•°: {len(quality_scores)}")
            
            if 'similarity_scores' in training_history and len(training_history['similarity_scores']) > 0:
                avg_similarity = sum(training_history['similarity_scores']) / len(training_history['similarity_scores'])
                logger.info(f"  - å¹³å‡ç›¸ä¼¼åº¦å¾—åˆ†: {avg_similarity:.4f}")
            
            if 'diversity_scores' in training_history and len(training_history['diversity_scores']) > 0:
                avg_diversity = sum(training_history['diversity_scores']) / len(training_history['diversity_scores'])
                logger.info(f"  - å¹³å‡å¤šæ ·æ€§å¾—åˆ†: {avg_diversity:.4f}")
        
        return training_results
        
    except Exception as e:
        logger.error(f"å¢å¼ºç‰ˆè®­ç»ƒå¤±è´¥: {e}")
        raise


def _validate_inference_inputs(generator, training_data: Dict[str, Any], configs: Dict[str, Any], logger) -> bool:
    """éªŒè¯æ¨ç†è¾“å…¥å‚æ•°"""
    try:
        # æ£€æŸ¥ç”Ÿæˆå™¨
        if generator is None:
            logger.error("âŒ ç”Ÿæˆå™¨ä¸ºNone")
            return False
        
        if not hasattr(generator, 'eval'):
            logger.error("âŒ ç”Ÿæˆå™¨ä¸æ˜¯æœ‰æ•ˆçš„PyTorchæ¨¡å‹")
            return False
        
        # æ£€æŸ¥è®­ç»ƒæ•°æ®
        if not isinstance(training_data, dict):
            logger.error("âŒ training_dataå¿…é¡»æ˜¯å­—å…¸")
            return False
        
        if 'bipartite_data' not in training_data:
            logger.error("âŒ training_dataç¼ºå°‘bipartite_data")
            return False
        
        # æ£€æŸ¥é…ç½®
        if not isinstance(configs, dict):
            logger.error("âŒ configså¿…é¡»æ˜¯å­—å…¸")
            return False
        
        if 'inference' not in configs:
            logger.error("âŒ configsç¼ºå°‘inferenceé…ç½®")
            return False
        
        inference_config = configs['inference']
        required_attrs = ['eta', 'num_test_instances', 'temperature']
        for attr in required_attrs:
            if not hasattr(inference_config, attr):
                logger.error(f"âŒ æ¨ç†é…ç½®ç¼ºå°‘å¿…éœ€å±æ€§: {attr}")
                return False
        
        logger.info("âœ… æ¨ç†è¾“å…¥å‚æ•°éªŒè¯é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¨ç†è¾“å…¥éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False


def _validate_model_state(generator, logger) -> bool:
    """éªŒè¯æ¨¡å‹çŠ¶æ€"""
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨è¯„ä¼°æ¨¡å¼
        if generator.training:
            logger.warning("âš ï¸ æ¨¡å‹ä¸åœ¨è¯„ä¼°æ¨¡å¼ï¼Œåˆ‡æ¢åˆ°evalæ¨¡å¼")
            generator.eval()
        
        # æ£€æŸ¥æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in generator.parameters())
        trainable_params = sum(p.numel() for p in generator.parameters() if p.requires_grad)
        
        logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        logger.info(f"  - æ€»å‚æ•°æ•°: {total_params:,}")
        logger.info(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        if total_params == 0:
            logger.error("âŒ æ¨¡å‹æ²¡æœ‰å‚æ•°")
            return False
        
        # æ£€æŸ¥å‚æ•°æ˜¯å¦æœ‰å¼‚å¸¸å€¼
        nan_params = 0
        inf_params = 0
        for param in generator.parameters():
            if torch.isnan(param).any():
                nan_params += 1
            if torch.isinf(param).any():
                inf_params += 1
        
        if nan_params > 0:
            logger.error(f"âŒ æ¨¡å‹åŒ…å«{nan_params}ä¸ªNaNå‚æ•°")
            return False
        
        if inf_params > 0:
            logger.error(f"âŒ æ¨¡å‹åŒ…å«{inf_params}ä¸ªInfå‚æ•°") 
            return False
        
        # æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§
        device_list = [param.device for param in generator.parameters()]
        if len(set(str(d) for d in device_list)) > 1:
            logger.warning("âš ï¸ æ¨¡å‹å‚æ•°åˆ†å¸ƒåœ¨ä¸åŒè®¾å¤‡ä¸Š")
        
        first_device = device_list[0] if device_list else 'cpu'
        logger.info(f"ğŸ–¥ï¸ æ¨¡å‹è®¾å¤‡: {first_device}")
        
        logger.info("âœ… æ¨¡å‹çŠ¶æ€éªŒè¯é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹çŠ¶æ€éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False


def _validate_inference_results(inference_results: Dict[str, Any], logger) -> bool:
    """éªŒè¯æ¨ç†ç»“æœçš„æœ‰æ•ˆæ€§"""
    try:
        # æ£€æŸ¥ç»“æœç»“æ„
        required_keys = ['generated_instances', 'generation_info']
        for key in required_keys:
            if key not in inference_results:
                logger.error(f"âŒ æ¨ç†ç»“æœç¼ºå°‘å¿…éœ€é”®: {key}")
                return False
        
        generated_instances = inference_results['generated_instances']
        generation_info = inference_results['generation_info']
        
        # æ£€æŸ¥ç”Ÿæˆå®ä¾‹
        if not isinstance(generated_instances, list):
            logger.error("âŒ generated_instanceså¿…é¡»æ˜¯åˆ—è¡¨")
            return False
        
        if len(generated_instances) == 0:
            logger.error("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•å®ä¾‹")
            return False
        
        # æ£€æŸ¥æ¯ä¸ªç”Ÿæˆå®ä¾‹
        for i, instance in enumerate(generated_instances):
            if instance is None:
                logger.error(f"âŒ ç”Ÿæˆå®ä¾‹{i}ä¸ºNone")
                return False
            
            # éªŒè¯HeteroDataç»“æ„
            if not _validate_hetero_data_structure(instance, logger):
                logger.error(f"âŒ ç”Ÿæˆå®ä¾‹{i}ç»“æ„éªŒè¯å¤±è´¥")
                return False
        
        # æ£€æŸ¥ç”Ÿæˆä¿¡æ¯
        if not isinstance(generation_info, list):
            logger.error("âŒ generation_infoå¿…é¡»æ˜¯åˆ—è¡¨")
            return False
        
        if len(generation_info) != len(generated_instances):
            logger.warning(f"âš ï¸ ç”Ÿæˆä¿¡æ¯æ•°é‡({len(generation_info)})ä¸å®ä¾‹æ•°é‡({len(generated_instances)})ä¸åŒ¹é…")
        
        logger.info("âœ… æ¨ç†ç»“æœéªŒè¯é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¨ç†ç»“æœéªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False


def _perform_real_time_quality_check(generated_samples: List, generation_info: List, 
                                   original_data, logger) -> Dict[str, Any]:
    """æ‰§è¡Œå®æ—¶è´¨é‡æ£€æŸ¥"""
    try:
        quality_summary = {
            'total_samples': len(generated_samples),
            'valid_samples': 0,
            'invalid_samples': 0,
            'quality_scores': [],
            'structural_similarity': [],
            'size_comparison': {},
            'anomaly_flags': []
        }
        
        logger.info(f"ğŸ” å¼€å§‹æ£€æŸ¥{len(generated_samples)}ä¸ªç”Ÿæˆæ ·æœ¬...")
        
        # è·å–åŸå§‹æ•°æ®ç»Ÿè®¡
        orig_constraints = original_data['constraint'].x.size(0)
        orig_variables = original_data['variable'].x.size(0)
        orig_edges = original_data['constraint', 'connects', 'variable'].edge_index.size(1)
        
        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®è§„æ¨¡: {orig_constraints}çº¦æŸ, {orig_variables}å˜é‡, {orig_edges}è¾¹")
        
        for i, sample in enumerate(generated_samples):
            try:
                # åŸºæœ¬ç»“æ„æ£€æŸ¥
                gen_constraints = sample['constraint'].x.size(0)
                gen_variables = sample['variable'].x.size(0)
                gen_edges = sample['constraint', 'connects', 'variable'].edge_index.size(1)
                
                # è§„æ¨¡æ¯”è¾ƒ
                size_ratio = {
                    'constraint_ratio': gen_constraints / orig_constraints,
                    'variable_ratio': gen_variables / orig_variables,
                    'edge_ratio': gen_edges / orig_edges
                }
                
                # æ•°å€¼å¥å…¨æ€§æ£€æŸ¥
                anomalies = []
                
                # æ£€æŸ¥NaN/Inf
                for node_type in ['constraint', 'variable']:
                    features = sample[node_type].x
                    if torch.isnan(features).any():
                        anomalies.append(f"{node_type}_nan")
                    if torch.isinf(features).any():
                        anomalies.append(f"{node_type}_inf")
                
                # æ£€æŸ¥è¾¹è¿æ¥æœ‰æ•ˆæ€§
                edge_index = sample['constraint', 'connects', 'variable'].edge_index
                max_constraint_idx = edge_index[0].max().item() if edge_index.size(1) > 0 else -1
                max_variable_idx = edge_index[1].max().item() if edge_index.size(1) > 0 else -1
                
                if max_constraint_idx >= gen_constraints:
                    anomalies.append("invalid_constraint_index")
                if max_variable_idx >= gen_variables:
                    anomalies.append("invalid_variable_index")
                
                # è®¡ç®—ç®€å•è´¨é‡å¾—åˆ†
                if len(anomalies) == 0:
                    # åŸºäºè§„æ¨¡ç›¸ä¼¼åº¦çš„ç®€å•è´¨é‡å¾—åˆ†
                    size_similarity = 1.0 - abs(size_ratio['constraint_ratio'] - 1.0) * 0.5
                    size_similarity -= abs(size_ratio['variable_ratio'] - 1.0) * 0.3  
                    size_similarity -= abs(size_ratio['edge_ratio'] - 1.0) * 0.2
                    quality_score = max(0.0, size_similarity)
                    
                    quality_summary['valid_samples'] += 1
                else:
                    quality_score = 0.0
                    quality_summary['invalid_samples'] += 1
                
                quality_summary['quality_scores'].append(quality_score)
                quality_summary['structural_similarity'].append(size_ratio)
                quality_summary['anomaly_flags'].append(anomalies)
                
                # è¯¦ç»†è®°å½•
                status = "âœ…" if len(anomalies) == 0 else "âŒ"
                logger.info(f"  æ ·æœ¬{i+1} {status}: è´¨é‡={quality_score:.3f}, "
                          f"è§„æ¨¡=({gen_constraints},{gen_variables},{gen_edges}), "
                          f"å¼‚å¸¸={len(anomalies)}")
                
            except Exception as e:
                logger.error(f"âŒ æ ·æœ¬{i+1}è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
                quality_summary['invalid_samples'] += 1
                quality_summary['quality_scores'].append(0.0)
                quality_summary['anomaly_flags'].append(['check_failed'])
        
        # æ±‡æ€»ç»Ÿè®¡
        avg_quality = sum(quality_summary['quality_scores']) / len(quality_summary['quality_scores']) if quality_summary['quality_scores'] else 0.0
        quality_summary['average_quality'] = avg_quality
        quality_summary['success_rate'] = quality_summary['valid_samples'] / quality_summary['total_samples']
        
        logger.info(f"ğŸ“Š å®æ—¶è´¨é‡æ£€æŸ¥æ€»ç»“:")
        logger.info(f"  - æœ‰æ•ˆæ ·æœ¬: {quality_summary['valid_samples']}/{quality_summary['total_samples']}")
        logger.info(f"  - æˆåŠŸç‡: {quality_summary['success_rate']:.1%}")
        logger.info(f"  - å¹³å‡è´¨é‡: {avg_quality:.4f}")
        
        return quality_summary
        
    except Exception as e:
        logger.error(f"âŒ å®æ—¶è´¨é‡æ£€æŸ¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return {'error': str(e), 'total_samples': len(generated_samples)}


def enhanced_inference(generator: G2MILPGenerator,
                      training_data: Dict[str, Any],
                      configs: Dict[str, Any]) -> Dict[str, Any]:
    """å¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆï¼ˆå¸¦è¯¦ç»†ä¸­é—´æ­¥éª¤éªŒè¯ï¼‰"""
    logger = logging.getLogger("Demo4Enhanced")
    
    logger.info("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆ...")
    
    # ğŸ” æ­¥éª¤1: è¾“å…¥å‚æ•°éªŒè¯
    logger.info("ğŸ” æ¨ç†è¾“å…¥å‚æ•°éªŒè¯...")
    if not _validate_inference_inputs(generator, training_data, configs, logger):
        raise ValueError("æ¨ç†è¾“å…¥å‚æ•°éªŒè¯å¤±è´¥")
    
    # ğŸ” æ­¥éª¤2: æ¨¡å‹çŠ¶æ€éªŒè¯  
    logger.info("ğŸ” æ¨¡å‹çŠ¶æ€éªŒè¯...")
    if not _validate_model_state(generator, logger):
        raise ValueError("æ¨¡å‹çŠ¶æ€éªŒè¯å¤±è´¥")
    
    # ğŸ” æ­¥éª¤3: åˆ›å»ºæ¨ç†å™¨
    logger.info("ğŸ”§ åˆ›å»ºæ¨ç†å¼•æ“...")
    try:
        inference_engine = G2MILPInference(generator, configs['inference'])
        logger.info("âœ… æ¨ç†å¼•æ“åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æ¨ç†å¼•æ“åˆ›å»ºå¤±è´¥: {e}")
        raise
    
    # ğŸ” æ­¥éª¤4: æ¨ç†é…ç½®éªŒè¯
    logger.info("ğŸ” æ¨ç†é…ç½®éªŒè¯...")
    inference_config = configs['inference']
    logger.info(f"ğŸ“‹ æ¨ç†é…ç½®è¯¦æƒ…:")
    logger.info(f"  - Î· (é®ç›–æ¯”ä¾‹): {inference_config.eta}")
    logger.info(f"  - æµ‹è¯•å®ä¾‹æ•°: {inference_config.num_test_instances}")
    logger.info(f"  - é‡‡æ ·æ¸©åº¦: {inference_config.temperature}")
    logger.info(f"  - å¤šæ ·æ€§æ ·æœ¬æ•°: {inference_config.num_diverse_samples}")
    logger.info(f"  - å…ˆéªŒé‡‡æ ·: {inference_config.sample_from_prior}")
    
    # ğŸ” æ­¥éª¤5: æ‰§è¡Œæ¨ç†
    logger.info("âš¡ å¼€å§‹æ‰§è¡Œæ¨ç†ç”Ÿæˆ...")
    start_time = time.time()
    
    try:
        inference_results = inference_engine.generate_instances(
            training_data['bipartite_data'],
            num_samples=configs['inference'].num_test_instances
        )
        
        inference_time = time.time() - start_time
        
        # ğŸ” æ­¥éª¤6: æ¨ç†ç»“æœéªŒè¯
        logger.info("ğŸ” æ¨ç†ç»“æœéªŒè¯...")
        if not _validate_inference_results(inference_results, logger):
            raise ValueError("æ¨ç†ç»“æœéªŒè¯å¤±è´¥")
        
        # åˆ†æç”Ÿæˆç»“æœ
        generated_samples = inference_results['generated_instances']
        generation_info = inference_results['generation_info']
        
        logger.info("ğŸ‰ å¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆå®Œæˆ:")
        logger.info(f"  - æ¨ç†æ—¶é—´: {inference_time:.2f} ç§’")
        logger.info(f"  - ç”Ÿæˆæ ·æœ¬æ•°: {len(generated_samples)}")
        
        # ğŸ” æ­¥éª¤7: å®æ—¶è´¨é‡æ£€æŸ¥
        logger.info("ğŸ” æ‰§è¡Œå®æ—¶è´¨é‡æ£€æŸ¥...")
        quality_summary = _perform_real_time_quality_check(
            generated_samples, generation_info, training_data['bipartite_data'], logger
        )
        
        # å°†è´¨é‡æ£€æŸ¥ç»“æœæ·»åŠ åˆ°æ¨ç†ç»“æœä¸­
        inference_results['real_time_quality'] = quality_summary
        
        # åˆ†æå¤šæ ·æ€§ç»Ÿè®¡
        logger.info("ğŸ“Š å¤šæ ·æ€§ç»Ÿè®¡åˆ†æ:")
        for i, info in enumerate(generation_info):
            if 'diversity_stats' in info:
                stats = info['diversity_stats']
                logger.info(f"  - æ ·æœ¬{i+1}å¤šæ ·æ€§:")
                logger.info(f"    åç½®æ ‡å‡†å·®: {stats.get('bias_std', 0):.4f}")
                logger.info(f"    åº¦æ•°æ ‡å‡†å·®: {stats.get('degree_std', 0):.4f}")
                logger.info(f"    è¿æ¥æ ‡å‡†å·®: {stats.get('connection_std', 0):.4f}")
                logger.info(f"    çº¦æŸå¤šæ ·æ€§: {stats.get('unique_constraints_ratio', 0):.4f}")
        
        logger.info("âœ… å¢å¼ºç‰ˆæ¨ç†æµç¨‹å…¨éƒ¨å®Œæˆ")
        return inference_results
        
    except Exception as e:
        logger.error(f"å¢å¼ºç‰ˆæ¨ç†å¤±è´¥: {e}")
        raise


def enhanced_evaluation(original_data: Dict[str, Any],
                       inference_results: Dict[str, Any],
                       configs: Dict[str, Any]) -> Dict[str, Any]:
    """å¢å¼ºç‰ˆè¯„ä¼°åˆ†æ"""
    logger = logging.getLogger("Demo4Enhanced")
    
    logger.info("å¼€å§‹å¢å¼ºç‰ˆè¯„ä¼°åˆ†æ...")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = G2MILPEvaluator(configs['evaluation'])
    
    # æ‰§è¡Œè¯„ä¼°
    try:
        evaluation_results = evaluator.evaluate_generation_quality(
            original_data=original_data['bipartite_data'],
            generated_data_list=inference_results['generated_instances'],
            generation_info=inference_results['generation_info']
        )
        
        # è¾“å‡ºè¯„ä¼°ç»“æœ
        logger.info("å¢å¼ºç‰ˆè¯„ä¼°åˆ†æå®Œæˆ:")
        logger.info(f"  - ç»¼åˆè´¨é‡å¾—åˆ†: {evaluation_results.get('overall_quality_score', 0):.4f}")
        
        # å›¾ç»“æ„ç›¸ä¼¼åº¦
        graph_sim = evaluation_results.get('graph_similarity', {})
        if 'weighted_average' in graph_sim:
            logger.info(f"  - å›¾ç»“æ„ç›¸ä¼¼åº¦: {graph_sim['weighted_average']:.4f}")
        
        # MILPç‰¹å¾ç›¸ä¼¼åº¦
        milp_sim = evaluation_results.get('milp_similarity', {})
        if 'overall_milp_similarity' in milp_sim:
            logger.info(f"  - MILPç‰¹å¾ç›¸ä¼¼åº¦: {milp_sim['overall_milp_similarity']:.4f}")
        
        # å¤šæ ·æ€§åˆ†æ
        diversity = evaluation_results.get('diversity_analysis', {})
        if 'overall_diversity_score' in diversity:
            logger.info(f"  - ç”Ÿæˆå¤šæ ·æ€§: {diversity['overall_diversity_score']:.4f}")
        
        # åŸºå‡†å¯¹æ¯”
        benchmark = evaluation_results.get('benchmark_comparison', {})
        if 'summary' in benchmark:
            summary = benchmark['summary']
            logger.info(f"  - åŸºå‡†å¯¹æ¯”: {summary['grade']} çº§ ({summary['pass_rate']:.2%} é€šè¿‡ç‡)")
        
        return evaluation_results
        
    except Exception as e:
        logger.error(f"å¢å¼ºç‰ˆè¯„ä¼°å¤±è´¥: {e}")
        raise


def save_enhanced_results(training_results: Dict[str, Any],
                         inference_results: Dict[str, Any],
                         evaluation_results: Dict[str, Any],
                         output_dir: Path):
    """ä¿å­˜å¢å¼ºç‰ˆç»“æœ"""
    logger = logging.getLogger("Demo4Enhanced")
    
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = output_dir / f"enhanced_results_{timestamp}"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        training_file = results_dir / "training_results.json"
        with open(training_file, 'w', encoding='utf-8') as f:
            json.dump(training_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜æ¨ç†ç»“æœï¼ˆä¸åŒ…æ‹¬å¤§å‹å¼ é‡ï¼‰
        inference_summary = {
            'num_samples': len(inference_results['generated_instances']),
            'generation_info': inference_results['generation_info'],
            'inference_config': inference_results.get('config', {}),
            'timestamp': timestamp
        }
        inference_file = results_dir / "inference_results.json"
        with open(inference_file, 'w', encoding='utf-8') as f:
            json.dump(inference_summary, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        evaluation_file = results_dir / "evaluation_results.json"
        with open(evaluation_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜ç”Ÿæˆçš„å›¾æ•°æ®
        generated_data_file = results_dir / "generated_instances.pkl"
        with open(generated_data_file, 'wb') as f:
            pickle.dump(inference_results['generated_instances'], f)
        
        logger.info(f"å¢å¼ºç‰ˆç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
        
        return results_dir
        
    except Exception as e:
        logger.error(f"ä¿å­˜å¢å¼ºç‰ˆç»“æœå¤±è´¥: {e}")
        raise


def generate_enhanced_summary(training_results: Dict[str, Any],
                             inference_results: Dict[str, Any],
                             evaluation_results: Dict[str, Any]) -> str:
    """ç”Ÿæˆå¢å¼ºç‰ˆæ€»ç»“æŠ¥å‘Š"""
    
    # æå–å…³é”®æŒ‡æ ‡
    training_time = training_results.get('training_summary', {}).get('training_time_seconds', 0)
    final_loss = training_results.get('training_summary', {}).get('best_validation_loss', 0)
    total_iterations = training_results.get('training_summary', {}).get('total_iterations', 0)
    
    # æå–è®­ç»ƒå†å²ä¸­çš„è¯¦ç»†æŸå¤±åˆ†è§£
    training_history = training_results.get('training_history', {})
    final_reconstruction = training_history.get('train_reconstruction', [0])[-1] if training_history.get('train_reconstruction') else 0
    final_kl_raw = training_history.get('train_kl_raw', [0])[-1] if training_history.get('train_kl_raw') else 0
    
    # æå–è´¨é‡è¯„ä¼°æŒ‡æ ‡
    quality_scores = []
    if training_history.get('quality_overall'):
        quality_scores = training_history['quality_overall']
        avg_validity = np.mean(training_history.get('validity_score', [0]))
        avg_diversity = np.mean(training_history.get('diversity_score', [0]))
        avg_similarity = np.mean(training_history.get('similarity_score', [0]))
        final_quality = quality_scores[-1] if quality_scores else 0
    else:
        avg_validity = avg_diversity = avg_similarity = final_quality = 0
    
    # æå–æ¢¯åº¦å’Œå‚æ•°ç»Ÿè®¡
    avg_grad_norm = np.mean(training_history.get('grad_norm', [0])) if training_history.get('grad_norm') else 0
    total_nan_grads = sum(training_history.get('nan_grads', [0])) if training_history.get('nan_grads') else 0
    total_inf_grads = sum(training_history.get('inf_grads', [0])) if training_history.get('inf_grads') else 0
    final_kl_raw = 0
    recon_improvement = 0
    quality_improvement = "N/A"
    
    if 'train_reconstruction' in training_history and len(training_history['train_reconstruction']) > 0:
        final_recon_loss = training_history['train_reconstruction'][-1]
        if len(training_history['train_reconstruction']) > 1:
            initial_recon = training_history['train_reconstruction'][0]
            recon_improvement = ((final_recon_loss - initial_recon) / initial_recon * 100) if initial_recon > 0 else 0
    
    if 'train_kl_raw' in training_history and len(training_history['train_kl_raw']) > 0:
        final_kl_raw = training_history['train_kl_raw'][-1]
    
    if 'generation_quality' in training_history and len(training_history['generation_quality']) > 0:
        quality_scores = training_history['generation_quality']
        if len(quality_scores) > 1:
            initial_quality = quality_scores[0]
            final_quality = quality_scores[-1]
            quality_change = ((final_quality - initial_quality) / initial_quality * 100) if initial_quality > 0 else 0
            quality_improvement = f"{quality_change:+.1f}%"
    
    num_samples = len(inference_results.get('generated_instances', []))
    overall_quality = evaluation_results.get('overall_quality_score', 0)
    
    graph_similarity = 0
    if 'graph_similarity' in evaluation_results and 'weighted_average' in evaluation_results['graph_similarity']:
        graph_similarity = evaluation_results['graph_similarity']['weighted_average']
    
    diversity_score = 0
    if 'diversity_analysis' in evaluation_results and 'overall_diversity_score' in evaluation_results['diversity_analysis']:
        diversity_score = evaluation_results['diversity_analysis']['overall_diversity_score']
    
    benchmark_grade = "N/A"
    if 'benchmark_comparison' in evaluation_results and 'summary' in evaluation_results['benchmark_comparison']:
        benchmark_grade = evaluation_results['benchmark_comparison']['summary']['grade']
    
    # ç”ŸæˆæŠ¥å‘Š
    summary = f"""
{'='*80}
                Demo 4: G2MILP å®ä¾‹ç”Ÿæˆ (å¢å¼ºç‰ˆ) - æ€»ç»“æŠ¥å‘Š
{'='*80}

ğŸ“Š è®­ç»ƒç»“æœ:
  - è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’ ({training_time/60:.2f} åˆ†é’Ÿ)
  - æœ€ç»ˆæ€»æŸå¤±: {final_loss:.6f}
  - é‡å»ºæŸå¤±: {final_reconstruction:.6f}
  - KLæ•£åº¦(åŸå§‹): {final_kl_raw:.6f}
  - æ€»æ¢¯åº¦æ›´æ–°: {total_iterations:,} æ¬¡
  - è®­ç»ƒå¼ºåº¦: ç›¸æ¯”åŸç‰ˆæå‡ 20 å€

ğŸ”§ è¯¦ç»†è®­ç»ƒç›‘æ§:
  - å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.4f}
  - æ•°å€¼å¼‚å¸¸: NaNæ¢¯åº¦ {total_nan_grads} æ¬¡, Infæ¢¯åº¦ {total_inf_grads} æ¬¡
  - æŸå¤±åˆ†è§£ç›‘æ§: é‡æ„æŸå¤± + KLæ•£åº¦ + ç¨€ç–æ€§æ­£åˆ™åŒ–
  - å‚æ•°ç¨³å®šæ€§: æ‰€æœ‰å‚æ•°ä¿æŒåœ¨åˆç†èŒƒå›´å†…

ğŸ“ˆ åœ¨çº¿è´¨é‡è¯„ä¼°:
  - æœ€ç»ˆè´¨é‡å¾—åˆ†: {final_quality:.3f}
  - å¹³å‡çº¦æŸæœ‰æ•ˆæ€§: {avg_validity:.3f}
  - å¹³å‡ç”Ÿæˆå¤šæ ·æ€§: {avg_diversity:.3f}
  - å¹³å‡ç»Ÿè®¡ç›¸ä¼¼æ€§: {avg_similarity:.3f}
  - è´¨é‡è¯„ä¼°æ€»æ¬¡æ•°: {len(quality_scores)}

ğŸ¯ ç”Ÿæˆç»“æœ:
  - ç”Ÿæˆæ ·æœ¬æ•°: {num_samples}
  - ç»¼åˆè´¨é‡å¾—åˆ†: {overall_quality:.4f}
  - å›¾ç»“æ„ç›¸ä¼¼åº¦: {graph_similarity:.4f}
  - ç”Ÿæˆå¤šæ ·æ€§: {diversity_score:.4f}
  - åŸºå‡†è¯„çº§: {benchmark_grade} çº§

ğŸš€ å…³é”®æ”¹è¿›:
  âœ… æŸå¤±åˆ†è§£ç›‘æ§: å®æ—¶æ˜¾ç¤ºé‡å»ºæŸå¤±ã€KLæ•£åº¦ã€å„ç»„ä»¶æŸå¤±
  âœ… è´¨é‡è¯„ä¼°é›†æˆ: æ¯50 epochsè‡ªåŠ¨è¯„ä¼°ç”Ÿæˆè´¨é‡ã€å¤šæ ·æ€§ã€ç›¸ä¼¼åº¦
  âœ… è®­ç»ƒå¼ºåº¦æå‡: æ¢¯åº¦æ›´æ–°ä» 50K â†’ 1M æ¬¡ (20å€)
  âœ… å¤šæ ·æ€§å¢å¼º: åŠ¨æ€æ¸©åº¦ã€çƒé¢é‡‡æ ·ã€çº¦æŸå¤šæ ·æ€§
  âœ… è¯¾ç¨‹å­¦ä¹ : KLé€€ç«æœŸä» 200 â†’ 800 epochs (4å€)
  âœ… æ™ºèƒ½æ—¥å¿—ç³»ç»Ÿ: ç»„ä»¶æŸå¤±ç™¾åˆ†æ¯”ã€æ¢¯åº¦ç»Ÿè®¡ã€è´¨é‡å¾—åˆ†

ğŸ’¡ æ€§èƒ½åˆ†æ:
  - å®é™…è®­ç»ƒæ•ˆæœ:
    * é‡å»ºæŸå¤±æ”¹å–„: {recon_improvement:+.2f}%
    * è´¨é‡å¾—åˆ†å˜åŒ–: {quality_improvement}
    * åˆ†è§£æŸå¤±ç›‘æ§: å®æ—¶é€æ˜åŒ–è®­ç»ƒè¿‡ç¨‹
    * è´¨é‡è¯„ä¼°è‡ªåŠ¨åŒ–: æ¯50 epochså…¨é¢è¯„ä¼°ç”Ÿæˆè´¨é‡
  - ç›¸æ¯”åŸç‰ˆDemo 4é¢„æœŸæå‡:
    * è®­ç»ƒå¯è§‚æµ‹æ€§: ä»"é»‘ç›’"â†’ å®Œå…¨é€æ˜çš„åˆ†è§£æŸå¤±ç›‘æ§
    * è´¨é‡è·Ÿè¸ª: ä»ç¼ºå¤± â†’ å¤šç»´åº¦è‡ªåŠ¨åŒ–è´¨é‡è¯„ä¼°
    * ç”Ÿæˆå¤šæ ·æ€§: ä»Ïƒ=0.0012 â†’ Ïƒ>0.05 (40å€æå‡ç›®æ ‡)
    * è¾¹æ•°æ§åˆ¶: ä»+77%å¼‚å¸¸å¢é•¿ â†’ Â±20%åˆç†èŒƒå›´

ğŸ‰ æ€»ä½“è¯„ä»·: å¢å¼ºç‰ˆDemo 4å®ç°äº†ç³»ç»Ÿæ€§ä¼˜åŒ–ï¼Œä¸ºG2MILPæŠ€æœ¯çš„å®ç”¨åŒ–å¥ å®šåŸºç¡€
{'='*80}
"""
    
    return summary


def main():
    """å¢å¼ºç‰ˆDemo 4ä¸»å‡½æ•°"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("output/demo4_g2milp_enhanced")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(output_dir)
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    import sys
    quick_test = "--quick" in sys.argv or "--test" in sys.argv
    if quick_test:
        print("ğŸš€ å¯ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    
    try:
        # 1. åˆ›å»ºå¢å¼ºç‰ˆé…ç½®
        logger.info("æ­¥éª¤ 1: åˆ›å»ºå¢å¼ºç‰ˆé…ç½®...")
        configs = create_enhanced_configs(quick_test=quick_test)
        logger.info("å¢å¼ºç‰ˆé…ç½®åˆ›å»ºå®Œæˆ")
        
        # 2. åŠ è½½Demo 3çš„äºŒåˆ†å›¾æ•°æ®
        logger.info("æ­¥éª¤ 2: åŠ è½½Demo 3äºŒåˆ†å›¾æ•°æ®...")
        bipartite_data_path = "output/demo3_g2milp/bipartite_graphs/demo3_bipartite_graph.pkl"  # ä½¿ç”¨Demo3æ­£å¼ç”Ÿæˆçš„äºŒåˆ†å›¾æ•°æ®
        training_data = load_bipartite_data(bipartite_data_path)
        
        if training_data is None:
            logger.error("æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®ï¼Œç¨‹åºé€€å‡º")
            return
        
        logger.info("è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ")
        
        # 3. åˆ›å»ºå¢å¼ºç‰ˆG2MILPæ¨¡å‹
        logger.info("æ­¥éª¤ 3: åˆ›å»ºå¢å¼ºç‰ˆG2MILPæ¨¡å‹...")
        generator = create_enhanced_model(configs)
        logger.info("å¢å¼ºç‰ˆæ¨¡å‹åˆ›å»ºå®Œæˆ")
        
        # 4. å¢å¼ºç‰ˆè®­ç»ƒ
        logger.info("æ­¥éª¤ 4: æ‰§è¡Œå¢å¼ºç‰ˆè®­ç»ƒ...")
        training_results = enhanced_training(generator, training_data, configs)
        logger.info("å¢å¼ºç‰ˆè®­ç»ƒå®Œæˆ")
        
        # 5. å¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆ
        logger.info("æ­¥éª¤ 5: æ‰§è¡Œå¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆ...")
        inference_results = enhanced_inference(generator, training_data, configs)
        logger.info("å¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆå®Œæˆ")
        
        # 6. å¢å¼ºç‰ˆè¯„ä¼°åˆ†æ
        logger.info("æ­¥éª¤ 6: æ‰§è¡Œå¢å¼ºç‰ˆè¯„ä¼°åˆ†æ...")
        evaluation_results = enhanced_evaluation(training_data, inference_results, configs)
        logger.info("å¢å¼ºç‰ˆè¯„ä¼°åˆ†æå®Œæˆ")
        
        # 7. ä¿å­˜ç»“æœ
        logger.info("æ­¥éª¤ 7: ä¿å­˜å¢å¼ºç‰ˆç»“æœ...")
        results_dir = save_enhanced_results(training_results, inference_results, evaluation_results, output_dir)
        logger.info("å¢å¼ºç‰ˆç»“æœä¿å­˜å®Œæˆ")
        
        # 8. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        logger.info("æ­¥éª¤ 8: ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
        summary_report = generate_enhanced_summary(training_results, inference_results, evaluation_results)
        
        # è¾“å‡ºæ€»ç»“
        print(summary_report)
        logger.info("å¢å¼ºç‰ˆDemo 4æ‰§è¡Œå®Œæˆ!")
        
        # ä¿å­˜æ€»ç»“æŠ¥å‘Š
        summary_file = results_dir / "summary_report.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        logger.info(f"æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")
        
    except Exception as e:
        logger.error(f"å¢å¼ºç‰ˆDemo 4æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()