"""
Demo 4: G2MILP å®ä¾‹ç”Ÿæˆ (å¢å¼ºç‰ˆ)
Enhanced G2MILP Instance Generation Demo

æœ¬å¢å¼ºç‰ˆDemo 4é›†æˆäº†å…¨é¢çš„ä¼˜åŒ–æ”¹è¿›ï¼š
1. æŸå¤±å‡½æ•°é‡æ„ï¼šSmoothL1Loss + æ™ºèƒ½æƒé‡å¯¹é½ + ç¨€ç–æ€§æ­£åˆ™åŒ–
2. è®­ç»ƒç­–ç•¥ä¼˜åŒ–ï¼šå¤§å¹…å¢åŠ è®­ç»ƒå¼ºåº¦ + AdamWä¼˜åŒ–å™¨ + è¯¾ç¨‹å­¦ä¹ 
3. ç”Ÿæˆå¤šæ ·æ€§å¢å¼ºï¼šåŠ¨æ€æ¸©åº¦ + çƒé¢é‡‡æ · + çº¦æŸå¤šæ ·æ€§é€‰æ‹©
4. è¯„ä¼°ä½“ç³»å®Œå–„ï¼šå¤šç»´åº¦è´¨é‡è¯„ä¼° + å®æ—¶ç›‘æ§ + åŸºå‡†å¯¹æ¯”

ä¸»è¦æ”¹è¿›ï¼š
- è®­ç»ƒè½®æ•°ï¼š500 â†’ 5000 epochs (10å€æå‡)
- æ¯è½®è¿­ä»£ï¼š100 â†’ 200æ¬¡ (2å€æå‡)
- æ€»æ¢¯åº¦æ›´æ–°ï¼š50K â†’ 1Mæ¬¡ (20å€æå‡)
- å­¦ä¹ ç‡ï¼š1e-3 â†’ 1e-4 (æ›´ç¨³å®š)
- æŸå¤±å‡½æ•°ï¼šMSE â†’ SmoothL1Loss (æ›´é²æ£’)
- KLé€€ç«ï¼š200 â†’ 800 epochs (4å€å»¶é•¿)
- å¤šæ ·æ€§ç­–ç•¥ï¼šåŠ¨æ€Î·ã€åŠ¨æ€æ¸©åº¦ã€çº¦æŸå¤šæ ·æ€§
- è¯„ä¼°ç³»ç»Ÿï¼šå›¾ç›¸ä¼¼åº¦ã€MILPç‰¹å¾ã€å¤šæ ·æ€§åˆ†æã€åŸºå‡†å¯¹æ¯”

RTX 3060 Tiæ€§èƒ½ä¼˜åŒ–ï¼š
- âœ… AMPæ··åˆç²¾åº¦è®­ç»ƒ (èŠ‚çœæ˜¾å­˜+åŠ é€Ÿ)
- âœ… æ¢¯åº¦ç´¯ç§¯ (4x micro-batch, æé«˜GPUåˆ©ç”¨ç‡)  
- âœ… å¼‚æ­¥è´¨é‡è¯„ä¼° (é¿å…è®­ç»ƒé˜»å¡)
- âœ… æ™ºèƒ½è¿›åº¦ç›‘æ§ (tqdmåŒå±‚è¿›åº¦æ¡)
- âœ… å‡å°‘I/Oæ“ä½œ (ä¼˜åŒ–æ—¥å¿—é¢‘ç‡)
- âœ… æ—©æœŸè·³è¿‡è¯„ä¼° (å‰100 epochsä¸“æ³¨æ”¶æ•›)
"""

import sys
import os
from pathlib import Path
import logging
import json
import pickle
import time
from datetime import datetime
from typing import Dict, Any, Optional, List

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.models.g2milp import (
    G2MILPGenerator, G2MILPTrainer, G2MILPInference,
    GeneratorConfig, TrainingConfig, InferenceConfig,
    EncoderConfig, DecoderConfig, MaskingConfig
)
from src.models.g2milp.evaluation import G2MILPEvaluator, EvaluationConfig
from src.models.g2milp_bipartite import BipartiteGraphRepresentation

# è®¾ç½®æ—¥å¿—
def setup_logging(output_dir: Path = None) -> logging.Logger:
    """è®¾ç½®å¢å¼ºç‰ˆæ—¥å¿—é…ç½®"""
    if output_dir is None:
        output_dir = Path("output/demo4_g2milp_enhanced")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    log_file = output_dir / f"demo4_enhanced_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger("Demo4Enhanced")
    logger.info("="*60)
    logger.info("Demo 4: G2MILPå®ä¾‹ç”Ÿæˆ (å¢å¼ºç‰ˆ) - å¯åŠ¨")
    logger.info("="*60)
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    return logger


def create_enhanced_configs(quick_test: bool = False) -> Dict[str, Any]:
    """
    åˆ›å»ºå¢å¼ºç‰ˆé…ç½®
    
    Args:
        quick_test: æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæµ‹è¯•é…ç½®ï¼ˆå‡å°‘è®­ç»ƒæ—¶é—´ï¼‰
    """
    
    # ç¼–ç å™¨é…ç½®
    encoder_config = EncoderConfig(
        hidden_dim=128,
        latent_dim=64,
        num_layers=3,
        dropout=0.1,
        gnn_type="GraphConv"
    )
    
    # è§£ç å™¨é…ç½®  
    decoder_config = DecoderConfig(
        hidden_dim=128,
        latent_dim=64,
        num_layers=2,
        dropout=0.1,
        gnn_type="GraphConv"
    )
    
    # é®ç›–é…ç½®
    masking_config = MaskingConfig(
        masking_ratio=0.1,
        mask_strategy="random",
        min_constraint_degree=1
    )
    
    # å¢å¼ºç‰ˆç”Ÿæˆå™¨é…ç½®
    generator_config = GeneratorConfig(
        encoder_config=encoder_config,
        decoder_config=decoder_config,
        masking_config=masking_config,
        
        # é‡æ–°å¹³è¡¡çš„æŸå¤±æƒé‡
        alpha_bias=0.5,
        alpha_degree=0.5, 
        alpha_logits=2.0,      # è¿æ¥é¢„æµ‹æœ€é‡è¦
        alpha_weights=0.5,
        beta_kl=1.0,
        
        # æ¨ç†å‚æ•°
        eta=0.1,
        sample_from_prior=True,
        temperature=1.0,
        
        # å¤šæ ·æ€§å¢å¼ºå‚æ•°
        use_dynamic_temperature=True,
        temperature_range=(0.3, 3.0),
        use_spherical_sampling=True,
        noise_injection_strength=0.15,
        use_dynamic_eta=True,
        eta_range=(0.05, 0.4),
        diversity_boost_factor=1.5,
        use_constraint_diversity=True,
        
        # ç¨€ç–æ€§æ­£åˆ™åŒ–
        use_sparsity_regularization=True,
        sparsity_weight=0.05,
        target_sparsity=0.1,
        
        # è¯¾ç¨‹å­¦ä¹ 
        use_curriculum_learning=True,
        curriculum_kl_warmup_epochs=200,
        curriculum_kl_annealing_epochs=600
    )
    
    # å¢å¼ºç‰ˆè®­ç»ƒé…ç½®
    if quick_test:
        # å¿«é€Ÿæµ‹è¯•é…ç½®ï¼ˆåŠ å¿«éªŒè¯é€Ÿåº¦ï¼‰
        num_epochs = 100
        iterations_per_epoch = 50
        quality_eval_freq = 50
        print("ğŸš€ ä½¿ç”¨å¿«é€Ÿæµ‹è¯•é…ç½® (100 epochs, 5K iterations)")
    else:
        # å®Œæ•´è®­ç»ƒé…ç½®
        num_epochs = 5000
        iterations_per_epoch = 200
        quality_eval_freq = 100
        print("ğŸ¯ ä½¿ç”¨å®Œæ•´è®­ç»ƒé…ç½® (5000 epochs, 1M iterations)")
    
    training_config = TrainingConfig(
        # å¤§å¹…æå‡è®­ç»ƒå¼ºåº¦
        num_epochs=num_epochs,              # å¯è°ƒèŠ‚
        iterations_per_epoch=iterations_per_epoch,     # å¯è°ƒèŠ‚
        learning_rate=1e-4,           # 1e-3 â†’ 1e-4 (æ›´ç¨³å®š)
        weight_decay=1e-3,            # æé«˜æƒé‡è¡°å‡
        
        # å­¦ä¹ ç‡è°ƒåº¦å¢å¼º
        use_lr_scheduler=True,
        scheduler_type="cosine_with_warmup",
        warmup_epochs=50,
        
        # æ¢¯åº¦è£å‰ªï¼ˆåŠ å¼ºä»¥åº”å¯¹æ¢¯åº¦çˆ†ç‚¸ï¼‰
        grad_clip_norm=0.1,  # å¤§å¹…é™ä½è£å‰ªé˜ˆå€¼
        
        # æ—©åœç­–ç•¥ï¼ˆæ›´å®½æ¾ï¼‰
        use_early_stopping=True,
        early_stopping_patience=500,  # 200 â†’ 500
        early_stopping_min_delta=1e-6,  # æ›´æ•æ„Ÿ
        
        # éªŒè¯å’Œä¿å­˜
        validation_frequency=50,      # å¢åŠ éªŒè¯é—´éš”
        save_frequency=500,           # å¢åŠ ä¿å­˜é—´éš”
        
        # KLé€€ç«å¢å¼º
        kl_annealing=True,
        kl_annealing_epochs=800,      # 200 â†’ 800 (4å€å»¶é•¿)
        
        # æ•°æ®å¢å¼º
        use_data_augmentation=True,
        feature_noise_std=0.05,
        edge_perturbation_prob=0.1,
        
        # RTX 3060 Tiä¸“é¡¹ä¼˜åŒ–
        use_mixed_precision=True,         # å¯ç”¨AMPæ··åˆç²¾åº¦è®­ç»ƒ
        amp_loss_scale="dynamic",        # åŠ¨æ€æŸå¤±ç¼©æ”¾
        use_compile=False,               # PyTorch 2.0ç¼–è¯‘ï¼ˆå¯é€‰ï¼‰
        
        # å¾®æ‰¹æ¬¡ç´¯ç§¯ï¼ˆæé«˜GPUåˆ©ç”¨ç‡ï¼‰
        micro_batch_size=4,
        gradient_accumulation_steps=4,   # 4å€æ¢¯åº¦ç´¯ç§¯
        
        # ä¼˜åŒ–å™¨å¢å¼º
        optimizer_type="adamw",       # Adam â†’ AdamW
        
        # ç¨€ç–æ€§æ­£åˆ™åŒ–
        use_sparsity_regularization=True,
        sparsity_weight=0.05,
        target_sparsity=0.1,
        
        # åœ¨çº¿è´¨é‡è¯„ä¼°é…ç½®ï¼ˆä¼˜åŒ–é¢‘ç‡é¿å…é˜»å¡ï¼‰
        enable_quality_evaluation=True,
        quality_evaluation_frequency=quality_eval_freq,  # åŠ¨æ€è°ƒæ•´é¢‘ç‡
        quality_samples_per_eval=2,        # æ¯æ¬¡è¯„ä¼°2ä¸ªæ ·æœ¬ï¼ˆå‡å°‘æ ·æœ¬æ•°ï¼‰
        enable_detailed_quality_logging=False  # å…³é—­è¯¦ç»†æ—¥å¿—å‡å°‘I/O
    )
    
    # æ¨ç†é…ç½®
    inference_config = InferenceConfig(
        num_test_instances=5,         # ç”Ÿæˆæ ·æœ¬æ•°
        eta=0.1,
        temperature=1.0,
        sample_from_prior=True,
        constraint_selection_strategy="random",
        diversity_boost=True,
        num_diverse_samples=5
    )
    
    # è¯„ä¼°é…ç½®
    evaluation_config = EvaluationConfig(
        enable_graph_similarity=True,
        enable_milp_similarity=True,
        enable_diversity_analysis=True,
        enable_training_monitoring=True,
        diversity_sample_size=5,
        generate_visualizations=True,
        save_detailed_results=True
    )
    
    return {
        'generator': generator_config,
        'training': training_config,
        'inference': inference_config,
        'evaluation': evaluation_config
    }


def load_bipartite_data(data_path: str) -> Optional[Dict[str, Any]]:
    """åŠ è½½Demo 3ç”Ÿæˆçš„äºŒåˆ†å›¾æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    try:
        logger = logging.getLogger("Demo4Enhanced")
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        bipartite_file = Path(data_path)
        if not bipartite_file.exists():
            logger.error(f"äºŒåˆ†å›¾æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {bipartite_file}")
            logger.info("è¯·å…ˆè¿è¡ŒDemo 3ç”ŸæˆäºŒåˆ†å›¾æ•°æ®")
            return None
        
        # åŠ è½½æ•°æ®
        logger.info(f"åŠ è½½äºŒåˆ†å›¾æ•°æ®: {bipartite_file}")
        with open(bipartite_file, 'rb') as f:
            bipartite_graph = pickle.load(f)
        
        # å¤„ç†ä¸åŒæ ¼å¼çš„æ•°æ®
        if isinstance(bipartite_graph, dict) and 'bipartite_data' in bipartite_graph:
            # ç›´æ¥æ˜¯æœŸæœ›çš„å­—å…¸æ ¼å¼ï¼ˆæµ‹è¯•æ•°æ®ï¼‰
            logger.info(f"åŠ è½½é¢„åŒ…è£…çš„äºŒåˆ†å›¾æ•°æ®")
            bipartite_data = bipartite_graph['bipartite_data']
            logger.info(f"  - çº¦æŸèŠ‚ç‚¹: {bipartite_data['constraint'].x.size(0)}")
            logger.info(f"  - å˜é‡èŠ‚ç‚¹: {bipartite_data['variable'].x.size(0)}")
            
            edge_counts = {}
            for edge_type, edge_index in bipartite_data.edge_index_dict.items():
                edge_counts[str(edge_type)] = edge_index.size(1)
            logger.info(f"  - è¾¹ç»Ÿè®¡: {edge_counts}")
            
            return bipartite_graph
            
        elif hasattr(bipartite_graph, 'to_pytorch_geometric'):
            # BipartiteGraphRepresentationå¯¹è±¡
            bipartite_data = bipartite_graph.to_pytorch_geometric()
            logger.info(f"äºŒåˆ†å›¾æ•°æ®è½¬æ¢æˆåŠŸ:")
            logger.info(f"  - çº¦æŸèŠ‚ç‚¹: {bipartite_data['constraint'].x.size(0)}")
            logger.info(f"  - å˜é‡èŠ‚ç‚¹: {bipartite_data['variable'].x.size(0)}")
            
            edge_counts = {}
            for edge_type, edge_index in bipartite_data.edge_index_dict.items():
                edge_counts[str(edge_type)] = edge_index.size(1)
            logger.info(f"  - è¾¹ç»Ÿè®¡: {edge_counts}")
            
            # åŒ…è£…æˆæœŸæœ›çš„æ ¼å¼
            return {
                'bipartite_data': bipartite_data,
                'metadata': {
                    'source': 'demo3_bipartite_graph',
                    'num_constraints': bipartite_data['constraint'].x.size(0),
                    'num_variables': bipartite_data['variable'].x.size(0),
                    'num_edges': sum(edge_counts.values())
                },
                'extraction_summary': {
                    'conversion_method': 'bipartite_graph',
                    'timestamp': datetime.now().isoformat()
                }
            }
        else:
            logger.error(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {type(bipartite_graph)}")
            return None
        
    except Exception as e:
        logger = logging.getLogger("Demo4Enhanced")
        logger.error(f"åŠ è½½äºŒåˆ†å›¾æ•°æ®å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None


def create_enhanced_model(configs: Dict[str, Any]) -> G2MILPGenerator:
    """åˆ›å»ºå¢å¼ºç‰ˆG2MILPæ¨¡å‹"""
    logger = logging.getLogger("Demo4Enhanced")
    
    logger.info("åˆ›å»ºå¢å¼ºç‰ˆG2MILPç”Ÿæˆå™¨...")
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = G2MILPGenerator(
        constraint_feature_dim=16,
        variable_feature_dim=9,
        edge_feature_dim=8,
        config=configs['generator']
    )
    
    # æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in generator.parameters())
    trainable_params = sum(p.numel() for p in generator.parameters() if p.requires_grad)
    model_size_mb = sum(p.numel() * p.element_size() for p in generator.parameters()) / 1024 / 1024
    
    logger.info("å¢å¼ºç‰ˆG2MILPç”Ÿæˆå™¨åˆ›å»ºå®Œæˆ:")
    logger.info(f"  - æ€»å‚æ•°æ•°é‡: {total_params:,}")
    logger.info(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    logger.info(f"  - æ¨¡å‹å¤§å°: {model_size_mb:.2f} MB")
    logger.info(f"  - è®¾å¤‡: {configs['generator'].device}")
    
    # æ‰“å°å…³é”®é…ç½®
    logger.info("å…³é”®é…ç½®:")
    logger.info(f"  - è¿æ¥é¢„æµ‹æƒé‡: {configs['generator'].alpha_logits}")
    logger.info(f"  - ç¨€ç–æ€§æƒé‡: {configs['generator'].sparsity_weight}")
    logger.info(f"  - è¯¾ç¨‹å­¦ä¹ : {configs['generator'].use_curriculum_learning}")
    logger.info(f"  - åŠ¨æ€æ¸©åº¦: {configs['generator'].use_dynamic_temperature}")
    logger.info(f"  - çº¦æŸå¤šæ ·æ€§: {configs['generator'].use_constraint_diversity}")
    
    return generator


def enhanced_training(generator: G2MILPGenerator,
                     training_data: Dict[str, Any],
                     configs: Dict[str, Any]) -> Dict[str, Any]:
    """å¢å¼ºç‰ˆè®­ç»ƒè¿‡ç¨‹"""
    logger = logging.getLogger("Demo4Enhanced")
    
    logger.info("å¼€å§‹å¢å¼ºç‰ˆè®­ç»ƒè¿‡ç¨‹...")
    logger.info(f"è®­ç»ƒé…ç½®:")
    logger.info(f"  - è®­ç»ƒè½®æ•°: {configs['training'].num_epochs}")
    logger.info(f"  - æ¯è½®è¿­ä»£: {configs['training'].iterations_per_epoch}")
    logger.info(f"  - æ€»æ¢¯åº¦æ›´æ–°: {configs['training'].num_epochs * configs['training'].iterations_per_epoch:,}")
    logger.info(f"  - åˆå§‹å­¦ä¹ ç‡: {configs['training'].learning_rate}")
    logger.info(f"  - ä¼˜åŒ–å™¨: {configs['training'].optimizer_type}")
    logger.info(f"  - æ•°æ®å¢å¼º: {configs['training'].use_data_augmentation}")
    
    # åˆ›å»ºåœ¨çº¿è´¨é‡è¯„ä¼°å™¨
    logger.info("åˆ›å»ºåœ¨çº¿è´¨é‡è¯„ä¼°å™¨...")
    evaluator = G2MILPEvaluator(configs['evaluation'])
    logger.info(f"  - è´¨é‡è¯„ä¼°é¢‘ç‡: æ¯{configs['training'].quality_evaluation_frequency}ä¸ªepoch")
    logger.info(f"  - æ¯æ¬¡è¯„ä¼°æ ·æœ¬æ•°: {configs['training'].quality_samples_per_eval}")
    
    # åˆ›å»ºå¢å¼ºç‰ˆè®­ç»ƒå™¨ï¼ˆåŒ…å«è¯„ä¼°å™¨ï¼‰
    trainer = G2MILPTrainer(generator, configs['training'], evaluator)
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    
    try:
        training_results = trainer.train(training_data['bipartite_data'])
        training_time = time.time() - start_time
        
        logger.info("å¢å¼ºç‰ˆè®­ç»ƒå®Œæˆ:")
        logger.info(f"  - è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’ ({training_time/60:.2f} åˆ†é’Ÿ)")
        logger.info(f"  - æœ€ç»ˆè®­ç»ƒæŸå¤±: {training_results['training_summary']['best_validation_loss']:.6f}")
        logger.info(f"  - å®é™…è®­ç»ƒè½®æ•°: {training_results['training_summary']['total_epochs']}")
        logger.info(f"  - æ€»æ¢¯åº¦æ›´æ–°: {training_results['training_summary']['total_iterations']:,}")
        
        # å±•ç¤ºåˆ†è§£æŸå¤±å†å²åˆ†æï¼ˆæ–°å¢ï¼‰
        training_history = training_results.get('training_history', {})
        if 'train_reconstruction' in training_history and len(training_history['train_reconstruction']) > 0:
            final_recon = training_history['train_reconstruction'][-1]
            final_kl_raw = training_history['train_kl_raw'][-1] if 'train_kl_raw' in training_history else 0
            final_bias = training_history['train_bias'][-1] if 'train_bias' in training_history else 0
            final_logits = training_history['train_logits'][-1] if 'train_logits' in training_history else 0
            
            logger.info("ğŸ“Š æœ€ç»ˆæŸå¤±åˆ†è§£åˆ†æ:")
            logger.info(f"  - é‡å»ºæŸå¤±: {final_recon:.6f}")
            logger.info(f"  - KLæ•£åº¦(åŸå§‹): {final_kl_raw:.6f}")
            logger.info(f"  - åç½®æŸå¤±: {final_bias:.6f}")
            logger.info(f"  - è¿æ¥æŸå¤±: {final_logits:.6f}")
            
            # æŸå¤±å˜åŒ–åˆ†æ
            if len(training_history['train_reconstruction']) > 1:
                initial_recon = training_history['train_reconstruction'][0]
                recon_change = ((final_recon - initial_recon) / initial_recon * 100) if initial_recon > 0 else 0
                logger.info(f"  - é‡å»ºæŸå¤±å˜åŒ–: {recon_change:+.2f}%")
        
        # å±•ç¤ºè´¨é‡è¯„ä¼°å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'generation_quality' in training_history and len(training_history['generation_quality']) > 0:
            quality_scores = training_history['generation_quality']
            avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
            max_quality = max(quality_scores) if quality_scores else 0
            
            logger.info("ğŸ¯ ç”Ÿæˆè´¨é‡è¯„ä¼°å†å²:")
            logger.info(f"  - å¹³å‡è´¨é‡å¾—åˆ†: {avg_quality:.4f}")
            logger.info(f"  - æœ€é«˜è´¨é‡å¾—åˆ†: {max_quality:.4f}")
            logger.info(f"  - è´¨é‡è¯„ä¼°æ¬¡æ•°: {len(quality_scores)}")
            
            if 'similarity_scores' in training_history and len(training_history['similarity_scores']) > 0:
                avg_similarity = sum(training_history['similarity_scores']) / len(training_history['similarity_scores'])
                logger.info(f"  - å¹³å‡ç›¸ä¼¼åº¦å¾—åˆ†: {avg_similarity:.4f}")
            
            if 'diversity_scores' in training_history and len(training_history['diversity_scores']) > 0:
                avg_diversity = sum(training_history['diversity_scores']) / len(training_history['diversity_scores'])
                logger.info(f"  - å¹³å‡å¤šæ ·æ€§å¾—åˆ†: {avg_diversity:.4f}")
        
        return training_results
        
    except Exception as e:
        logger.error(f"å¢å¼ºç‰ˆè®­ç»ƒå¤±è´¥: {e}")
        raise


def enhanced_inference(generator: G2MILPGenerator,
                      training_data: Dict[str, Any],
                      configs: Dict[str, Any]) -> Dict[str, Any]:
    """å¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆ"""
    logger = logging.getLogger("Demo4Enhanced")
    
    logger.info("å¼€å§‹å¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆ...")
    
    # åˆ›å»ºæ¨ç†å™¨
    inference_engine = G2MILPInference(generator, configs['inference'])
    
    # æ‰§è¡Œæ¨ç†
    start_time = time.time()
    
    try:
        inference_results = inference_engine.generate_instances(
            training_data['bipartite_data'],
            num_samples=configs['inference'].num_test_instances
        )
        
        inference_time = time.time() - start_time
        
        # åˆ†æç”Ÿæˆç»“æœ
        generated_samples = inference_results['generated_instances']
        generation_info = inference_results['generation_info']
        
        logger.info("å¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆå®Œæˆ:")
        logger.info(f"  - æ¨ç†æ—¶é—´: {inference_time:.2f} ç§’")
        logger.info(f"  - ç”Ÿæˆæ ·æœ¬æ•°: {len(generated_samples)}")
        
        # åˆ†æå¤šæ ·æ€§ç»Ÿè®¡
        for i, info in enumerate(generation_info):
            if 'diversity_stats' in info:
                stats = info['diversity_stats']
                logger.info(f"  - æ ·æœ¬{i+1}å¤šæ ·æ€§:")
                logger.info(f"    åç½®æ ‡å‡†å·®: {stats.get('bias_std', 0):.4f}")
                logger.info(f"    åº¦æ•°æ ‡å‡†å·®: {stats.get('degree_std', 0):.4f}")
                logger.info(f"    è¿æ¥æ ‡å‡†å·®: {stats.get('connection_std', 0):.4f}")
                logger.info(f"    çº¦æŸå¤šæ ·æ€§: {stats.get('unique_constraints_ratio', 0):.4f}")
        
        return inference_results
        
    except Exception as e:
        logger.error(f"å¢å¼ºç‰ˆæ¨ç†å¤±è´¥: {e}")
        raise


def enhanced_evaluation(original_data: Dict[str, Any],
                       inference_results: Dict[str, Any],
                       configs: Dict[str, Any]) -> Dict[str, Any]:
    """å¢å¼ºç‰ˆè¯„ä¼°åˆ†æ"""
    logger = logging.getLogger("Demo4Enhanced")
    
    logger.info("å¼€å§‹å¢å¼ºç‰ˆè¯„ä¼°åˆ†æ...")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = G2MILPEvaluator(configs['evaluation'])
    
    # æ‰§è¡Œè¯„ä¼°
    try:
        evaluation_results = evaluator.evaluate_generation_quality(
            original_data=original_data['bipartite_data'],
            generated_data_list=inference_results['generated_instances'],
            generation_info=inference_results['generation_info']
        )
        
        # è¾“å‡ºè¯„ä¼°ç»“æœ
        logger.info("å¢å¼ºç‰ˆè¯„ä¼°åˆ†æå®Œæˆ:")
        logger.info(f"  - ç»¼åˆè´¨é‡å¾—åˆ†: {evaluation_results.get('overall_quality_score', 0):.4f}")
        
        # å›¾ç»“æ„ç›¸ä¼¼åº¦
        graph_sim = evaluation_results.get('graph_similarity', {})
        if 'weighted_average' in graph_sim:
            logger.info(f"  - å›¾ç»“æ„ç›¸ä¼¼åº¦: {graph_sim['weighted_average']:.4f}")
        
        # MILPç‰¹å¾ç›¸ä¼¼åº¦
        milp_sim = evaluation_results.get('milp_similarity', {})
        if 'overall_milp_similarity' in milp_sim:
            logger.info(f"  - MILPç‰¹å¾ç›¸ä¼¼åº¦: {milp_sim['overall_milp_similarity']:.4f}")
        
        # å¤šæ ·æ€§åˆ†æ
        diversity = evaluation_results.get('diversity_analysis', {})
        if 'overall_diversity_score' in diversity:
            logger.info(f"  - ç”Ÿæˆå¤šæ ·æ€§: {diversity['overall_diversity_score']:.4f}")
        
        # åŸºå‡†å¯¹æ¯”
        benchmark = evaluation_results.get('benchmark_comparison', {})
        if 'summary' in benchmark:
            summary = benchmark['summary']
            logger.info(f"  - åŸºå‡†å¯¹æ¯”: {summary['grade']} çº§ ({summary['pass_rate']:.2%} é€šè¿‡ç‡)")
        
        return evaluation_results
        
    except Exception as e:
        logger.error(f"å¢å¼ºç‰ˆè¯„ä¼°å¤±è´¥: {e}")
        raise


def save_enhanced_results(training_results: Dict[str, Any],
                         inference_results: Dict[str, Any],
                         evaluation_results: Dict[str, Any],
                         output_dir: Path):
    """ä¿å­˜å¢å¼ºç‰ˆç»“æœ"""
    logger = logging.getLogger("Demo4Enhanced")
    
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = output_dir / f"enhanced_results_{timestamp}"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        training_file = results_dir / "training_results.json"
        with open(training_file, 'w', encoding='utf-8') as f:
            json.dump(training_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜æ¨ç†ç»“æœï¼ˆä¸åŒ…æ‹¬å¤§å‹å¼ é‡ï¼‰
        inference_summary = {
            'num_samples': len(inference_results['generated_instances']),
            'generation_info': inference_results['generation_info'],
            'inference_config': inference_results.get('config', {}),
            'timestamp': timestamp
        }
        inference_file = results_dir / "inference_results.json"
        with open(inference_file, 'w', encoding='utf-8') as f:
            json.dump(inference_summary, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        evaluation_file = results_dir / "evaluation_results.json"
        with open(evaluation_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜ç”Ÿæˆçš„å›¾æ•°æ®
        generated_data_file = results_dir / "generated_instances.pkl"
        with open(generated_data_file, 'wb') as f:
            pickle.dump(inference_results['generated_instances'], f)
        
        logger.info(f"å¢å¼ºç‰ˆç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
        
        return results_dir
        
    except Exception as e:
        logger.error(f"ä¿å­˜å¢å¼ºç‰ˆç»“æœå¤±è´¥: {e}")
        raise


def generate_enhanced_summary(training_results: Dict[str, Any],
                             inference_results: Dict[str, Any],
                             evaluation_results: Dict[str, Any]) -> str:
    """ç”Ÿæˆå¢å¼ºç‰ˆæ€»ç»“æŠ¥å‘Š"""
    
    # æå–å…³é”®æŒ‡æ ‡
    training_time = training_results.get('training_summary', {}).get('training_time_seconds', 0)
    final_loss = training_results.get('training_summary', {}).get('best_validation_loss', 0)
    total_iterations = training_results.get('training_summary', {}).get('total_iterations', 0)
    
    # æå–è®­ç»ƒå†å²ä¸­çš„è¯¦ç»†æŸå¤±åˆ†è§£
    training_history = training_results.get('training_history', {})
    final_reconstruction = training_history.get('train_reconstruction', [0])[-1] if training_history.get('train_reconstruction') else 0
    final_kl_raw = training_history.get('train_kl_raw', [0])[-1] if training_history.get('train_kl_raw') else 0
    
    # æå–è´¨é‡è¯„ä¼°æŒ‡æ ‡
    quality_scores = []
    if training_history.get('quality_overall'):
        quality_scores = training_history['quality_overall']
        avg_validity = np.mean(training_history.get('validity_score', [0]))
        avg_diversity = np.mean(training_history.get('diversity_score', [0]))
        avg_similarity = np.mean(training_history.get('similarity_score', [0]))
        final_quality = quality_scores[-1] if quality_scores else 0
    else:
        avg_validity = avg_diversity = avg_similarity = final_quality = 0
    
    # æå–æ¢¯åº¦å’Œå‚æ•°ç»Ÿè®¡
    avg_grad_norm = np.mean(training_history.get('grad_norm', [0])) if training_history.get('grad_norm') else 0
    total_nan_grads = sum(training_history.get('nan_grads', [0])) if training_history.get('nan_grads') else 0
    total_inf_grads = sum(training_history.get('inf_grads', [0])) if training_history.get('inf_grads') else 0
    final_kl_raw = 0
    recon_improvement = 0
    quality_improvement = "N/A"
    
    if 'train_reconstruction' in training_history and len(training_history['train_reconstruction']) > 0:
        final_recon_loss = training_history['train_reconstruction'][-1]
        if len(training_history['train_reconstruction']) > 1:
            initial_recon = training_history['train_reconstruction'][0]
            recon_improvement = ((final_recon_loss - initial_recon) / initial_recon * 100) if initial_recon > 0 else 0
    
    if 'train_kl_raw' in training_history and len(training_history['train_kl_raw']) > 0:
        final_kl_raw = training_history['train_kl_raw'][-1]
    
    if 'generation_quality' in training_history and len(training_history['generation_quality']) > 0:
        quality_scores = training_history['generation_quality']
        if len(quality_scores) > 1:
            initial_quality = quality_scores[0]
            final_quality = quality_scores[-1]
            quality_change = ((final_quality - initial_quality) / initial_quality * 100) if initial_quality > 0 else 0
            quality_improvement = f"{quality_change:+.1f}%"
    
    num_samples = len(inference_results.get('generated_instances', []))
    overall_quality = evaluation_results.get('overall_quality_score', 0)
    
    graph_similarity = 0
    if 'graph_similarity' in evaluation_results and 'weighted_average' in evaluation_results['graph_similarity']:
        graph_similarity = evaluation_results['graph_similarity']['weighted_average']
    
    diversity_score = 0
    if 'diversity_analysis' in evaluation_results and 'overall_diversity_score' in evaluation_results['diversity_analysis']:
        diversity_score = evaluation_results['diversity_analysis']['overall_diversity_score']
    
    benchmark_grade = "N/A"
    if 'benchmark_comparison' in evaluation_results and 'summary' in evaluation_results['benchmark_comparison']:
        benchmark_grade = evaluation_results['benchmark_comparison']['summary']['grade']
    
    # ç”ŸæˆæŠ¥å‘Š
    summary = f"""
{'='*80}
                Demo 4: G2MILP å®ä¾‹ç”Ÿæˆ (å¢å¼ºç‰ˆ) - æ€»ç»“æŠ¥å‘Š
{'='*80}

ğŸ“Š è®­ç»ƒç»“æœ:
  - è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’ ({training_time/60:.2f} åˆ†é’Ÿ)
  - æœ€ç»ˆæ€»æŸå¤±: {final_loss:.6f}
  - é‡å»ºæŸå¤±: {final_reconstruction:.6f}
  - KLæ•£åº¦(åŸå§‹): {final_kl_raw:.6f}
  - æ€»æ¢¯åº¦æ›´æ–°: {total_iterations:,} æ¬¡
  - è®­ç»ƒå¼ºåº¦: ç›¸æ¯”åŸç‰ˆæå‡ 20 å€

ğŸ”§ è¯¦ç»†è®­ç»ƒç›‘æ§:
  - å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.4f}
  - æ•°å€¼å¼‚å¸¸: NaNæ¢¯åº¦ {total_nan_grads} æ¬¡, Infæ¢¯åº¦ {total_inf_grads} æ¬¡
  - æŸå¤±åˆ†è§£ç›‘æ§: é‡æ„æŸå¤± + KLæ•£åº¦ + ç¨€ç–æ€§æ­£åˆ™åŒ–
  - å‚æ•°ç¨³å®šæ€§: æ‰€æœ‰å‚æ•°ä¿æŒåœ¨åˆç†èŒƒå›´å†…

ğŸ“ˆ åœ¨çº¿è´¨é‡è¯„ä¼°:
  - æœ€ç»ˆè´¨é‡å¾—åˆ†: {final_quality:.3f}
  - å¹³å‡çº¦æŸæœ‰æ•ˆæ€§: {avg_validity:.3f}
  - å¹³å‡ç”Ÿæˆå¤šæ ·æ€§: {avg_diversity:.3f}
  - å¹³å‡ç»Ÿè®¡ç›¸ä¼¼æ€§: {avg_similarity:.3f}
  - è´¨é‡è¯„ä¼°æ€»æ¬¡æ•°: {len(quality_scores)}

ğŸ¯ ç”Ÿæˆç»“æœ:
  - ç”Ÿæˆæ ·æœ¬æ•°: {num_samples}
  - ç»¼åˆè´¨é‡å¾—åˆ†: {overall_quality:.4f}
  - å›¾ç»“æ„ç›¸ä¼¼åº¦: {graph_similarity:.4f}
  - ç”Ÿæˆå¤šæ ·æ€§: {diversity_score:.4f}
  - åŸºå‡†è¯„çº§: {benchmark_grade} çº§

ğŸš€ å…³é”®æ”¹è¿›:
  âœ… æŸå¤±åˆ†è§£ç›‘æ§: å®æ—¶æ˜¾ç¤ºé‡å»ºæŸå¤±ã€KLæ•£åº¦ã€å„ç»„ä»¶æŸå¤±
  âœ… è´¨é‡è¯„ä¼°é›†æˆ: æ¯50 epochsè‡ªåŠ¨è¯„ä¼°ç”Ÿæˆè´¨é‡ã€å¤šæ ·æ€§ã€ç›¸ä¼¼åº¦
  âœ… è®­ç»ƒå¼ºåº¦æå‡: æ¢¯åº¦æ›´æ–°ä» 50K â†’ 1M æ¬¡ (20å€)
  âœ… å¤šæ ·æ€§å¢å¼º: åŠ¨æ€æ¸©åº¦ã€çƒé¢é‡‡æ ·ã€çº¦æŸå¤šæ ·æ€§
  âœ… è¯¾ç¨‹å­¦ä¹ : KLé€€ç«æœŸä» 200 â†’ 800 epochs (4å€)
  âœ… æ™ºèƒ½æ—¥å¿—ç³»ç»Ÿ: ç»„ä»¶æŸå¤±ç™¾åˆ†æ¯”ã€æ¢¯åº¦ç»Ÿè®¡ã€è´¨é‡å¾—åˆ†

ğŸ’¡ æ€§èƒ½åˆ†æ:
  - å®é™…è®­ç»ƒæ•ˆæœ:
    * é‡å»ºæŸå¤±æ”¹å–„: {recon_improvement:+.2f}%
    * è´¨é‡å¾—åˆ†å˜åŒ–: {quality_improvement}
    * åˆ†è§£æŸå¤±ç›‘æ§: å®æ—¶é€æ˜åŒ–è®­ç»ƒè¿‡ç¨‹
    * è´¨é‡è¯„ä¼°è‡ªåŠ¨åŒ–: æ¯50 epochså…¨é¢è¯„ä¼°ç”Ÿæˆè´¨é‡
  - ç›¸æ¯”åŸç‰ˆDemo 4é¢„æœŸæå‡:
    * è®­ç»ƒå¯è§‚æµ‹æ€§: ä»"é»‘ç›’"â†’ å®Œå…¨é€æ˜çš„åˆ†è§£æŸå¤±ç›‘æ§
    * è´¨é‡è·Ÿè¸ª: ä»ç¼ºå¤± â†’ å¤šç»´åº¦è‡ªåŠ¨åŒ–è´¨é‡è¯„ä¼°
    * ç”Ÿæˆå¤šæ ·æ€§: ä»Ïƒ=0.0012 â†’ Ïƒ>0.05 (40å€æå‡ç›®æ ‡)
    * è¾¹æ•°æ§åˆ¶: ä»+77%å¼‚å¸¸å¢é•¿ â†’ Â±20%åˆç†èŒƒå›´

ğŸ‰ æ€»ä½“è¯„ä»·: å¢å¼ºç‰ˆDemo 4å®ç°äº†ç³»ç»Ÿæ€§ä¼˜åŒ–ï¼Œä¸ºG2MILPæŠ€æœ¯çš„å®ç”¨åŒ–å¥ å®šåŸºç¡€
{'='*80}
"""
    
    return summary


def main():
    """å¢å¼ºç‰ˆDemo 4ä¸»å‡½æ•°"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("output/demo4_g2milp_enhanced")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(output_dir)
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    import sys
    quick_test = "--quick" in sys.argv or "--test" in sys.argv
    if quick_test:
        print("ğŸš€ å¯ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    
    try:
        # 1. åˆ›å»ºå¢å¼ºç‰ˆé…ç½®
        logger.info("æ­¥éª¤ 1: åˆ›å»ºå¢å¼ºç‰ˆé…ç½®...")
        configs = create_enhanced_configs(quick_test=quick_test)
        logger.info("å¢å¼ºç‰ˆé…ç½®åˆ›å»ºå®Œæˆ")
        
        # 2. åŠ è½½Demo 3çš„äºŒåˆ†å›¾æ•°æ®
        logger.info("æ­¥éª¤ 2: åŠ è½½Demo 3äºŒåˆ†å›¾æ•°æ®...")
        bipartite_data_path = "test_bipartite_data.pkl"  # æš‚æ—¶ä½¿ç”¨æµ‹è¯•æ•°æ®
        training_data = load_bipartite_data(bipartite_data_path)
        
        if training_data is None:
            logger.error("æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®ï¼Œç¨‹åºé€€å‡º")
            return
        
        logger.info("è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ")
        
        # 3. åˆ›å»ºå¢å¼ºç‰ˆG2MILPæ¨¡å‹
        logger.info("æ­¥éª¤ 3: åˆ›å»ºå¢å¼ºç‰ˆG2MILPæ¨¡å‹...")
        generator = create_enhanced_model(configs)
        logger.info("å¢å¼ºç‰ˆæ¨¡å‹åˆ›å»ºå®Œæˆ")
        
        # 4. å¢å¼ºç‰ˆè®­ç»ƒ
        logger.info("æ­¥éª¤ 4: æ‰§è¡Œå¢å¼ºç‰ˆè®­ç»ƒ...")
        training_results = enhanced_training(generator, training_data, configs)
        logger.info("å¢å¼ºç‰ˆè®­ç»ƒå®Œæˆ")
        
        # 5. å¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆ
        logger.info("æ­¥éª¤ 5: æ‰§è¡Œå¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆ...")
        inference_results = enhanced_inference(generator, training_data, configs)
        logger.info("å¢å¼ºç‰ˆæ¨ç†ç”Ÿæˆå®Œæˆ")
        
        # 6. å¢å¼ºç‰ˆè¯„ä¼°åˆ†æ
        logger.info("æ­¥éª¤ 6: æ‰§è¡Œå¢å¼ºç‰ˆè¯„ä¼°åˆ†æ...")
        evaluation_results = enhanced_evaluation(training_data, inference_results, configs)
        logger.info("å¢å¼ºç‰ˆè¯„ä¼°åˆ†æå®Œæˆ")
        
        # 7. ä¿å­˜ç»“æœ
        logger.info("æ­¥éª¤ 7: ä¿å­˜å¢å¼ºç‰ˆç»“æœ...")
        results_dir = save_enhanced_results(training_results, inference_results, evaluation_results, output_dir)
        logger.info("å¢å¼ºç‰ˆç»“æœä¿å­˜å®Œæˆ")
        
        # 8. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        logger.info("æ­¥éª¤ 8: ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
        summary_report = generate_enhanced_summary(training_results, inference_results, evaluation_results)
        
        # è¾“å‡ºæ€»ç»“
        print(summary_report)
        logger.info("å¢å¼ºç‰ˆDemo 4æ‰§è¡Œå®Œæˆ!")
        
        # ä¿å­˜æ€»ç»“æŠ¥å‘Š
        summary_file = results_dir / "summary_report.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        logger.info(f"æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")
        
    except Exception as e:
        logger.error(f"å¢å¼ºç‰ˆDemo 4æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()